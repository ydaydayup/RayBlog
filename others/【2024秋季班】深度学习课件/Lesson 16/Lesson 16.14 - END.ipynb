{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet\n",
    "(without LRN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/GoogleNet.PNG?versionId=CAEQFRiBgIDNw62XyhciIDA0ZDViOWM5YjYyNDRlZmJiMmMzNWVjOTZlNjk5YmMw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv + BN + ReLU -- basicconv\n",
    "#Inception\n",
    "#AUXclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels,**kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "                                 ,nn.BatchNorm2d(out_channels)\n",
    "                                 ,nn.ReLU(inplace=True))\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicConv2d(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicConv2d(2,10,kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self\n",
    "                 ,in_channels : int\n",
    "                 ,ch1x1 : int\n",
    "                 ,ch3x3red : int\n",
    "                 ,ch3x3 : int\n",
    "                 ,ch5x5red : int\n",
    "                 ,ch5x5 : int\n",
    "                 ,pool_proj : int\n",
    "                ):\n",
    "        super().__init__()\n",
    "        #1x1\n",
    "        self.branch1 = BasicConv2d(in_channels,ch1x1,kernel_size=1)\n",
    "        #1x1 + 3x3\n",
    "        self.branch2 = nn.Sequential(BasicConv2d(in_channels, ch3x3red, kernel_size=1)\n",
    "                                     ,BasicConv2d(ch3x3red, ch3x3, kernel_size=3,padding=1))\n",
    "        #1x1 + 5x5\n",
    "        self.branch3 = nn.Sequential(BasicConv2d(in_channels, ch5x5red, kernel_size=1)\n",
    "                                     ,BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2))\n",
    "        #pool + 1x1\n",
    "        self.branch4 = nn.Sequential(nn.MaxPool2d(kernel_size=3,stride=1, padding=1,ceil_mode=True)\n",
    "                                    ,BasicConv2d(in_channels,pool_proj,kernel_size=1))\n",
    "    def forward(self,x):\n",
    "        branch1 = self.branch1(x) #28x28,ch1x1\n",
    "        branch2 = self.branch2(x) #28x28,ch3x3\n",
    "        branch3 = self.branch3(x) #28x28,ch5x5\n",
    "        branch4 = self.branch4(x) #28x28,pool_proj\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return torch.cat(outputs, 1) #合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(28x28, ch1x1+ ch3x3 + ch5x5 + pool_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 9),)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2,9) #不能合并 (4,4&5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#横向不能合并(2 & 3)\n",
    "(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a,b],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#测试\n",
    "'''\n",
    "in_channels : int\n",
    ",ch1x1 : int\n",
    ",ch3x3red : int\n",
    ",ch3x3 : int\n",
    ",ch5x5red : int\n",
    ",ch5x5 : int\n",
    ",pool_proj : int\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "in3a = Inception(192,64,96,128,16,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(10,192,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 28, 28])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in3a(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/GoogleNet.PNG?versionId=CAEQFRiBgIDNw62XyhciIDA0ZDViOWM5YjYyNDRlZmJiMmMzNWVjOTZlNjk5YmMw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/辅助分类器.PNG?versionId=CAEQFRiBgIC7w62XyhciIGNkN2E5ZGRiMjlmZTQxYzY5YmY1ODA3YWI0YjYyNDYx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxClf(nn.Module):\n",
    "    def __init__(self,in_channels : int, num_classes : int, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_ = nn.Sequential(nn.AvgPool2d(kernel_size=5,stride=3)\n",
    "                                     ,BasicConv2d(in_channels,128, kernel_size=1))\n",
    "        self.clf_ = nn.Sequential(nn.Linear(4*4*128, 1024)\n",
    "                                 ,nn.ReLU(inplace=True)\n",
    "                                 ,nn.Dropout(0.7)\n",
    "                                 ,nn.Linear(1024,num_classes))\n",
    "    def forward(self,x):\n",
    "        x = self.feature_(x)\n",
    "        x = x.view(-1,4*4*128)\n",
    "        x = self.clf_(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuxClf(\n",
       "  (feature_): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "    (1): BasicConv2d(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (clf_): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.7, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4a后的辅助分类器\n",
    "AuxClf(512,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/GoogleNet.PNG?versionId=CAEQFRiBgIDNw62XyhciIDA0ZDViOWM5YjYyNDRlZmJiMmMzNWVjOTZlNjk5YmMw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(224 + 6 - 7)/2 + 1 = 112.5\n",
    "#pool (112 - 3)/2 + 1 = 55.5 向上取整之后得到56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self,num_classes: int = 1000, blocks = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, AuxClf]\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        aux_clf_block = blocks[2]\n",
    "        \n",
    "        #block1\n",
    "        self.conv1 = conv_block(3,64,kernel_size=7,stride=2,padding = 3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3,stride=2,ceil_mode = True)\n",
    "        \n",
    "        #block2\n",
    "        self.conv2 = conv_block(64,64,kernel_size=1)\n",
    "        self.conv3 = conv_block(64,192,kernel_size=3, padding = 1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3,stride=2,ceil_mode = True)\n",
    "        \n",
    "        #block3\n",
    "        self.inception3a = inception_block(192,64,96,128,16,32,32)\n",
    "        self.inception3b = inception_block(256,128,128,192,32,96,64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3,stride=2,ceil_mode = True)\n",
    "\n",
    "        \n",
    "        #block4 \n",
    "        self.inception4a = inception_block(480,192,96,208,16,48,64)\n",
    "        self.inception4b = inception_block(512,160,112,224,24,64,64)\n",
    "        self.inception4c = inception_block(512,128,128,256,24,64,64)\n",
    "        self.inception4d = inception_block(512,112,144,288,32,64,64)\n",
    "        self.inception4e = inception_block(528,256,150,320,32,128,128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3,stride=2,ceil_mode = True)\n",
    "        \n",
    "        #block5\n",
    "        self.inception5a = inception_block(832,256,160,320,32,128,128)\n",
    "        self.inception5b = inception_block(832,384,192,384,48,128,128)\n",
    "        \n",
    "        #clf\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1)) #我需要的输出的特征图尺寸是多少\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024,num_classes)\n",
    "        \n",
    "        #auxclf\n",
    "        self.aux1 = aux_clf_block(512, num_classes) #4a\n",
    "        self.aux2 = aux_clf_block(528, num_classes) #4d\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #block1\n",
    "        x = self.maxpool1(self.conv1(x))\n",
    "        \n",
    "        #block2\n",
    "        x = self.maxpool2(self.conv3(self.conv2(x)))\n",
    "        \n",
    "        #block3\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        #block4\n",
    "        x = self.inception4a(x)\n",
    "        aux1 = self.aux1(x)\n",
    "        \n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        aux2 = self.aux2(x)\n",
    "        \n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        #block5\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        #clf\n",
    "        x = self.avgpool(x) #在这个全局平均池化之后，特征图尺寸就变成了1x1\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, aux2, aux1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(10,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GoogLeNet(num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2, fc1, fc0 = net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000])\n",
      "torch.Size([10, 1000])\n",
      "torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "for i in [fc2, fc1, fc0]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─BasicConv2d: 1-1                       [10, 64, 112, 112]        9,536\n",
       "├─MaxPool2d: 1-2                         [10, 64, 56, 56]          --\n",
       "├─BasicConv2d: 1-3                       [10, 64, 56, 56]          4,224\n",
       "├─BasicConv2d: 1-4                       [10, 192, 56, 56]         110,976\n",
       "├─MaxPool2d: 1-5                         [10, 192, 28, 28]         --\n",
       "├─Inception: 1-6                         [10, 256, 28, 28]         164,064\n",
       "├─Inception: 1-7                         [10, 480, 28, 28]         389,376\n",
       "├─MaxPool2d: 1-8                         [10, 480, 14, 14]         --\n",
       "├─Inception: 1-9                         [10, 512, 14, 14]         376,800\n",
       "├─AuxClf: 1-10                           [10, 1000]                3,188,968\n",
       "├─Inception: 1-11                        [10, 512, 14, 14]         449,808\n",
       "├─Inception: 1-12                        [10, 512, 14, 14]         510,768\n",
       "├─Inception: 1-13                        [10, 528, 14, 14]         606,080\n",
       "├─AuxClf: 1-14                           [10, 1000]                3,191,016\n",
       "├─Inception: 1-15                        [10, 832, 14, 14]         835,276\n",
       "├─MaxPool2d: 1-16                        [10, 832, 7, 7]           --\n",
       "├─Inception: 1-17                        [10, 832, 7, 7]           1,044,480\n",
       "├─Inception: 1-18                        [10, 1024, 7, 7]          1,445,344\n",
       "├─AdaptiveAvgPool2d: 1-19                [10, 1024, 1, 1]          --\n",
       "├─Dropout: 1-20                          [10, 1024]                --\n",
       "├─Linear: 1-21                           [10, 1000]                1,025,000\n",
       "==========================================================================================\n",
       "Total params: 13,351,716\n",
       "Trainable params: 13,351,716\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 184.06\n",
       "==========================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 53.41\n",
       "Estimated Total Size (MB): 59.51\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net,(10,3,224,224),device=\"cpu\",depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/ResNet架构.PNG?versionId=CAEQFRiBgIDQw62XyhciIDUzZmRhZGJhNThjNTQ3NzhhZjAxOGFjYWM1MjYxZmMw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basicconv - conv2d + BN + ReLU (→ conv3x3, conv1x1)\n",
    "#Residual Unit, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入需要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Type, Union, List, Optional\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_, out_, stride=1, initialzero = False):\n",
    "    bn = nn.BatchNorm2d(out_)\n",
    "    #需要进行判断：要对BN进行0初始化吗？\n",
    "    #最后一层就初始化,不是最后一层就不改变gamma和beta\n",
    "    if initialzero == True:\n",
    "        nn.init.constant_(bn.weight, 0)\n",
    "    return nn.Sequential(nn.Conv2d(in_, out_\n",
    "                            , kernel_size=3,padding=1, stride = stride\n",
    "                            , bias = False)\n",
    "                         ,bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3x3(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_, out_, stride=1, initialzero = False):\n",
    "    bn = nn.BatchNorm2d(out_)\n",
    "    #需要进行判断：要对BN进行0初始化吗？\n",
    "    #最后一层就初始化,不是最后一层就不改变gamma和beta\n",
    "    if initialzero == True:\n",
    "        nn.init.constant_(bn.weight, 0)\n",
    "    return nn.Sequential(nn.Conv2d(in_, out_\n",
    "                            , kernel_size=1,padding=0, stride = stride\n",
    "                            , bias = False)\n",
    "                         ,bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1x1(2,10,1,True)[1].weight #请帮我执行0初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1x1(2,10,1)[1].weight #没有执行0初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    #这是残差单元类\n",
    "    #stride1是否等于2呢？如果等于2 - 特征图尺寸会发生变化\n",
    "    #需要在跳跃链接上增加1x1卷积层来调整特征图尺寸\n",
    "    #如果stride1等于1，则什么也不需要做\n",
    "    def __init__(self,out_: int\n",
    "                 ,stride1: int = 1 #定义该参数的类型，并且定义默认值\n",
    "                 ,in_ : Optional[int] = None\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stride1 = stride1\n",
    "        \n",
    "        #当特征图尺寸需要缩小时，卷积层的输出特征图数量out_等于输入特征图数量in_的2被\n",
    "        #当特征图尺寸不需要缩小时，out_ == in_\n",
    "        if stride1 !=1:\n",
    "            in_ = int(out_/2)\n",
    "        else:\n",
    "            in_ = out_\n",
    "        \n",
    "        #拟合部分，输出F(x)\n",
    "        self.fit_ = nn.Sequential(conv3x3(in_,out_,stride=stride1)\n",
    "                                 ,nn.ReLU(inplace=True)\n",
    "                                 ,conv3x3(out_,out_,initialzero=True)\n",
    "                                 )\n",
    "        \n",
    "        #跳跃链接，输出x(1x1卷积核之后的x)\n",
    "        self.skipconv = conv1x1(in_,out_,stride = stride1)\n",
    "        \n",
    "        #单独定义放在H(x)之后来使用的激活函数ReLU\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        fx = self.fit_(x) #拟合结果\n",
    "        if self.stride1 != 1:\n",
    "            x = self.skipconv(x) #跳跃链接\n",
    "        hx = self.relu(fx + x)\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResidualUnit(out_,stride1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(10,64,56,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3_x_18_0 = ResidualUnit(out_=128,stride1 = 2)\n",
    "#0号残差单元 - 需要特征图这班，特征图数量加倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3_x_18_0(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_x_18_0 = ResidualUnit(out_ = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 56, 56])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2_x_18_0(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/35.PNG?versionId=CAEQFRiBgID07vWryhciIDRhYTkwODA1MDU0YjRmNWQ5ZTk2NzAxMjdkYzE2MGZm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    #是需要将特征图尺寸缩小的场合吗？\n",
    "    #conv2_x - conv3_x - conv4_x - conv5_x 相互链接的时候\n",
    "    #每次都需要将特征图尺寸折半，同时卷积层上的middle_out = 1/2in_\n",
    "    def __init__(self, middle_out\n",
    "                 , stride1: int = 1\n",
    "                 , in_: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        out_ = 4 * middle_out\n",
    "        \n",
    "        #我希望使用选填参数in_来帮助我们区别，这个架构是不是在conv1的后面\n",
    "        #如果这个架构不是紧跟在conv1后，就不填写in_\n",
    "        #如果是跟在conv1后，就填写in_ = 64\n",
    "        if in_ == None:\n",
    "            if stride1 !=1: #缩小特征图的场合，即这个瓶颈结构是每个layers的第一个瓶颈结构\n",
    "                in_ = middle_out * 2\n",
    "                #不缩小特征图的场合，即这个瓶颈结构不是这个layers的第一个瓶颈结构\n",
    "                #而是跟在第一个瓶颈结构后的重复的结构\n",
    "            else:\n",
    "                in_ = middle_out * 4\n",
    "        \n",
    "        self.fit_ = nn.Sequential(conv1x1(in_,middle_out,stride=stride1)\n",
    "                                 ,nn.ReLU(inplace=True)\n",
    "                                 ,conv3x3(middle_out,middle_out)\n",
    "                                 ,nn.ReLU(inplace=True)\n",
    "                                 ,conv1x1(middle_out,out_,initialzero=True))\n",
    "        \n",
    "        self.skipconv = conv1x1(in_, out_, stride=stride1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        fx = self.fit_(x)\n",
    "        #跳跃链接\n",
    "        x = self.skipconv(x)\n",
    "        hx = self.relu(fx + x)\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = torch.ones(10,64,56,56) #conv2x的输入\n",
    "#假设，我是conv1后紧跟的第一个瓶颈结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_x_101_0 = Bottleneck(in_ = 64, middle_out=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 56, 56])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2_x_101_0(data1).shape #特征图尺寸不变，输出翻四倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不是conv1后紧跟的第一个瓶颈结构，但是需要缩小特征图尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= torch.ones(10,256,56,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3_x_101_0 = Bottleneck(middle_out=128,stride1=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 28, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3_x_101_0(data2).shape #输出翻两倍，特征图尺寸缩小一半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不是conv1后的第一个瓶颈结构，也不需要缩小特征图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = torch.ones(10,512,28,28)\n",
    "conv3_x_101_1 = Bottleneck(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 28, 28])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3_x_101_1(data3).shape #输出数量不变，特征图尺寸也不变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Bottleneck(middle_out, stride1, in_(optional))\n",
    "class ResidualUnit(out_, stride1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在单一的layers里的残差单元/瓶颈结构的数量使用num_blocks来表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34\n",
    "num_blocks_conv3x = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34layer - conv3_x\n",
    "ru0 = ResidualUnit(out_ = 128,stride1=2) #第一层有点不同，后面的全靠循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualUnit(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "ResidualUnit(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "ResidualUnit(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_blocks_conv3x - 1):\n",
    "    print(ResidualUnit(out_ = 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#350\n",
    "num_blocks_conv4_x = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4_x_50 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在列表中添加第0个瓶颈架构块\n",
    "conv4_x_50.append(Bottleneck(middle_out = 256,stride1 = 2))\n",
    "\n",
    "for i in range(num_blocks_conv4_x - 1):\n",
    "    conv4_x_50.append(Bottleneck(middle_out = 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv4_x_50) #包含了6个块，第一个块是包含步长=2的卷积层，剩下的块是重复结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34-conv2\n",
    "#50-conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34-conv2\n",
    "ru0 = ResidualUnit(out_ = 64) #不改变特征图尺寸\n",
    "ru1 = ResidualUnit(out_ = 64)\n",
    "ru2 = ResidualUnit(out_ = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50-conv2\n",
    "bt0 = Bottleneck(middle_out=64, in_ = 64) #也不需要改变特征图尺寸，需要填写参数in_，参数in_ = 64\n",
    "bt1 = Bottleneck(middle_out=64)\n",
    "bt2 = Bottleneck(middle_out=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个块\n",
    "# if 这个块是conv1之后的第一个块：\n",
    "#   需要一个独特的参数 in_ = 64\n",
    "# else:\n",
    "#   第一层就需要独特的参数 stride1 = 2\n",
    "\n",
    "# 剩下的块就使用for循环来跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "afterconv1 = True #你是conv1之后的第一个块，false = 你不是conv1之后的第一个块\n",
    "num_blocks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if afterconv1 == True:\n",
    "    layers.append(Bottleneck(middle_out=64, in_ = 64))\n",
    "else:\n",
    "    layers.append(Bottleneck(middle_out=128, stride1 = 2))\n",
    "\n",
    "for i in range(num_blocks-1):\n",
    "    layers.append(Bottleneck(middle_out=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if afterconv1 == True:\n",
    "    layers.append(ResidualUnit(out_=64, in_ = 64))\n",
    "else:\n",
    "    layers.append(ResidualUnit(out_=64, stride1 = 2))\n",
    "\n",
    "for i in range(num_blocks-1):\n",
    "    layers.append(ResidualUnit(out_=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#同时用于残差单元和瓶颈结构\n",
    "#帮助我们将一个layers内的全部块都打包在一个列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#专门用来生成ResNet的每一个layers的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool\n",
    "#int\n",
    "#float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(block: Type[Union[ResidualUnit, Bottleneck]]\n",
    "                ,middle_out: int\n",
    "                ,num_blocks: int\n",
    "                ,afterconv1: bool = False):\n",
    "    \n",
    "    layers = []\n",
    "    \n",
    "    if afterconv1 == True:\n",
    "        layers.append(block(middle_out, in_ = 64))\n",
    "    else:\n",
    "        layers.append(block(middle_out, stride1 = 2))\n",
    "    \n",
    "    for i in range(num_blocks-1):\n",
    "        layers.append(block(middle_out))\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_34_conv4_x = make_layers(ResidualUnit,\n",
    "                              256,\n",
    "                              6,\n",
    "                              False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (2): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (3): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (4): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (5): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(*layer_34_conv4_x) #python常见用法，星号解析列表/储存器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#测试 - 需要分别对残差块和瓶颈架构进行测试，并且需要对conv1后的首个架构，以及中间的架构进行测试<br>\n",
    "#注意检查：输入的数据结构是否正确，网络能否允许正确的数据结构输入，输入后产出的结构是否正确，包括特征图尺寸是否变化、特征图数量是否变化，以及一个layers中所包含的blocks数量是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34层网络，conv2_x，紧跟在conv1后的首个架构\n",
    "#不缩小特征图尺寸，每层的输出都是64，3个块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_x_34 = make_layers(ResidualUnit,\n",
    "           64,\n",
    "           3,\n",
    "           afterconv1 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashape = (10,64,56,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─ResidualUnit: 1-1                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-2                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-3                      [10, 64, 56, 56]          78,208\n",
       "==========================================================================================\n",
       "Total params: 234,624\n",
       "Trainable params: 234,624\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 291.59\n",
       "==========================================================================================\n",
       "Input size (MB): 8.03\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.94\n",
       "Estimated Total Size (MB): 8.97\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(conv2_x_34,datashape,depth=1,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashape = (10,64,56,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_x_101 = make_layers(Bottleneck,\n",
    "           64,\n",
    "           3,\n",
    "           afterconv1 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Bottleneck: 1-1                        [10, 256, 56, 56]         --\n",
       "|    └─Sequential: 2-1                   [10, 256, 56, 56]         --\n",
       "|    |    └─Sequential: 3-1              [10, 64, 56, 56]          4,224\n",
       "|    |    └─ReLU: 3-2                    [10, 64, 56, 56]          --\n",
       "|    |    └─Sequential: 3-3              [10, 64, 56, 56]          36,992\n",
       "|    |    └─ReLU: 3-4                    [10, 64, 56, 56]          --\n",
       "|    |    └─Sequential: 3-5              [10, 256, 56, 56]         16,896\n",
       "|    └─Sequential: 2-2                   [10, 256, 56, 56]         --\n",
       "|    |    └─Conv2d: 3-6                  [10, 256, 56, 56]         16,384\n",
       "|    |    └─BatchNorm2d: 3-7             [10, 256, 56, 56]         512\n",
       "|    └─ReLU: 2-3                         [10, 256, 56, 56]         --\n",
       "├─Bottleneck: 1-2                        [10, 256, 56, 56]         --\n",
       "|    └─Sequential: 2-4                   [10, 256, 56, 56]         --\n",
       "|    |    └─Sequential: 3-8              [10, 64, 56, 56]          16,512\n",
       "|    |    └─ReLU: 3-9                    [10, 64, 56, 56]          --\n",
       "|    |    └─Sequential: 3-10             [10, 64, 56, 56]          36,992\n",
       "|    |    └─ReLU: 3-11                   [10, 64, 56, 56]          --\n",
       "|    |    └─Sequential: 3-12             [10, 256, 56, 56]         16,896\n",
       "|    └─Sequential: 2-5                   [10, 256, 56, 56]         --\n",
       "|    |    └─Conv2d: 3-13                 [10, 256, 56, 56]         65,536\n",
       "|    |    └─BatchNorm2d: 3-14            [10, 256, 56, 56]         512\n",
       "|    └─ReLU: 2-6                         [10, 256, 56, 56]         --\n",
       "├─Bottleneck: 1-3                        [10, 256, 56, 56]         --\n",
       "|    └─Sequential: 2-7                   [10, 256, 56, 56]         --\n",
       "|    |    └─Sequential: 3-15             [10, 64, 56, 56]          16,512\n",
       "|    |    └─ReLU: 3-16                   [10, 64, 56, 56]          --\n",
       "|    |    └─Sequential: 3-17             [10, 64, 56, 56]          36,992\n",
       "|    |    └─ReLU: 3-18                   [10, 64, 56, 56]          --\n",
       "|    |    └─Sequential: 3-19             [10, 256, 56, 56]         16,896\n",
       "|    └─Sequential: 2-8                   [10, 256, 56, 56]         --\n",
       "|    |    └─Conv2d: 3-20                 [10, 256, 56, 56]         65,536\n",
       "|    |    └─BatchNorm2d: 3-21            [10, 256, 56, 56]         512\n",
       "|    └─ReLU: 2-9                         [10, 256, 56, 56]         --\n",
       "==========================================================================================\n",
       "Total params: 347,904\n",
       "Trainable params: 347,904\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.78\n",
       "==========================================================================================\n",
       "Input size (MB): 8.03\n",
       "Forward/backward pass size (MB): 963.38\n",
       "Params size (MB): 1.39\n",
       "Estimated Total Size (MB): 972.80\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(conv2_x_101,datashape,depth=3,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4_x_101 = make_layers(Bottleneck,256,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashape = (10,512,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Bottleneck: 1-1                        [10, 1024, 14, 14]        1,512,448\n",
       "├─Bottleneck: 1-2                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-3                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-4                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-5                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-6                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-7                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-8                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-9                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-10                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-11                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-12                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-13                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-14                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-15                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-16                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-17                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-18                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-19                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-20                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-21                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-22                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-23                       [10, 1024, 14, 14]        2,167,808\n",
       "==========================================================================================\n",
       "Total params: 49,204,224\n",
       "Trainable params: 49,204,224\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (T): 1.01\n",
       "==========================================================================================\n",
       "Input size (MB): 16.06\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 196.82\n",
       "Estimated Total Size (MB): 212.87\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(conv4_x_101, datashape, depth=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(224 +2p - 7)/2 = 112 #p = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers18 = [2,2,2,2] #列举了各层分别有多少个块的列表\n",
    "layer101 = [3,4,23,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block: Type[Union[ResidualUnit, Bottleneck]]\n",
    "                ,layers: List[int]\n",
    "                ,num_classes : int):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        block：要使用的用来加深深度的基本架构是？可以选择残差单元或瓶颈结构，两种都带有skip connection\n",
    "        layers：列表，每个层里具体有多少个块呢？可参考网络架构图。例如，34层的残差网络的layers = [3,4,6,3]\n",
    "        num_classes：真实标签含有多少个类别？\n",
    "        '''\n",
    "        \n",
    "        #layer1:卷积+池化的组合\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64\n",
    "                                              ,kernel_size=7,stride=2\n",
    "                                              ,padding=3,bias = False)\n",
    "                                   ,nn.BatchNorm2d(64)\n",
    "                                   ,nn.ReLU(inplace=True)\n",
    "                                   ,nn.MaxPool2d(kernel_size=3\n",
    "                                                 ,stride=2\n",
    "                                                 ,ceil_mode = True))\n",
    "        \n",
    "        #layer2 - layer5:残差块/瓶颈结构\n",
    "        self.layer2_x = make_layers(block,64,layers[0],afterconv1=True)\n",
    "        self.layer3_x = make_layers(block,128,layers[1])\n",
    "        self.layer4_x = make_layers(block,256,layers[2])\n",
    "        self.layer5_x = make_layers(block,512,layers[3])\n",
    "        \n",
    "        #全局平均池化\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        #分类\n",
    "        if block == ResidualUnit:\n",
    "            self.fc = nn.Linear(512,num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2048,num_classes)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x) #layer1，普通卷积+池化的输出\n",
    "        x = self.layer5_x(self.layer4_x(self.layer3_x(self.layer2_x(x))))\n",
    "        x = self.avgpool(x) #特征图尺寸1x1 (n_samples, fc, 1, 1)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/ResNet架构.PNG?versionId=CAEQFRiBgIDQw62XyhciIDUzZmRhZGJhNThjNTQ3NzhhZjAxOGFjYWM1MjYxZmMw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet(block,layers,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashape = (10,3,224,224) #ImageNet数据集的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "res34 = ResNet(ResidualUnit,[3,4,6,3],num_classes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "res101 = ResNet(Bottleneck,[3,4,23,3],num_classes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [10, 64, 56, 56]          --\n",
       "|    └─Conv2d: 2-1                       [10, 64, 112, 112]        9,408\n",
       "|    └─BatchNorm2d: 2-2                  [10, 64, 112, 112]        128\n",
       "|    └─ReLU: 2-3                         [10, 64, 112, 112]        --\n",
       "|    └─MaxPool2d: 2-4                    [10, 64, 56, 56]          --\n",
       "├─Sequential: 1-2                        [10, 64, 56, 56]          --\n",
       "|    └─ResidualUnit: 2-5                 [10, 64, 56, 56]          78,208\n",
       "|    └─ResidualUnit: 2-6                 [10, 64, 56, 56]          78,208\n",
       "|    └─ResidualUnit: 2-7                 [10, 64, 56, 56]          78,208\n",
       "├─Sequential: 1-3                        [10, 128, 28, 28]         --\n",
       "|    └─ResidualUnit: 2-8                 [10, 128, 28, 28]         230,144\n",
       "|    └─ResidualUnit: 2-9                 [10, 128, 28, 28]         312,064\n",
       "|    └─ResidualUnit: 2-10                [10, 128, 28, 28]         312,064\n",
       "|    └─ResidualUnit: 2-11                [10, 128, 28, 28]         312,064\n",
       "├─Sequential: 1-4                        [10, 256, 14, 14]         --\n",
       "|    └─ResidualUnit: 2-12                [10, 256, 14, 14]         919,040\n",
       "|    └─ResidualUnit: 2-13                [10, 256, 14, 14]         1,246,720\n",
       "|    └─ResidualUnit: 2-14                [10, 256, 14, 14]         1,246,720\n",
       "|    └─ResidualUnit: 2-15                [10, 256, 14, 14]         1,246,720\n",
       "|    └─ResidualUnit: 2-16                [10, 256, 14, 14]         1,246,720\n",
       "|    └─ResidualUnit: 2-17                [10, 256, 14, 14]         1,246,720\n",
       "├─Sequential: 1-5                        [10, 512, 7, 7]           --\n",
       "|    └─ResidualUnit: 2-18                [10, 512, 7, 7]           3,673,088\n",
       "|    └─ResidualUnit: 2-19                [10, 512, 7, 7]           4,983,808\n",
       "|    └─ResidualUnit: 2-20                [10, 512, 7, 7]           4,983,808\n",
       "├─AdaptiveAvgPool2d: 1-6                 [10, 512, 1, 1]           --\n",
       "├─Linear: 1-7                            [10, 1000]                513,000\n",
       "==========================================================================================\n",
       "Total params: 22,716,840\n",
       "Trainable params: 22,716,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 266.86\n",
       "==========================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 128.53\n",
       "Params size (MB): 90.87\n",
       "Estimated Total Size (MB): 225.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(res34,datashape,depth=2,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [10, 64, 56, 56]          --\n",
       "|    └─Conv2d: 2-1                       [10, 64, 112, 112]        9,408\n",
       "|    └─BatchNorm2d: 2-2                  [10, 64, 112, 112]        128\n",
       "|    └─ReLU: 2-3                         [10, 64, 112, 112]        --\n",
       "|    └─MaxPool2d: 2-4                    [10, 64, 56, 56]          --\n",
       "├─Sequential: 1-2                        [10, 256, 56, 56]         --\n",
       "|    └─Bottleneck: 2-5                   [10, 256, 56, 56]         75,008\n",
       "|    └─Bottleneck: 2-6                   [10, 256, 56, 56]         136,448\n",
       "|    └─Bottleneck: 2-7                   [10, 256, 56, 56]         136,448\n",
       "├─Sequential: 1-3                        [10, 512, 28, 28]         --\n",
       "|    └─Bottleneck: 2-8                   [10, 512, 28, 28]         379,392\n",
       "|    └─Bottleneck: 2-9                   [10, 512, 28, 28]         543,232\n",
       "|    └─Bottleneck: 2-10                  [10, 512, 28, 28]         543,232\n",
       "|    └─Bottleneck: 2-11                  [10, 512, 28, 28]         543,232\n",
       "├─Sequential: 1-4                        [10, 1024, 14, 14]        --\n",
       "|    └─Bottleneck: 2-12                  [10, 1024, 14, 14]        1,512,448\n",
       "|    └─Bottleneck: 2-13                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-14                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-15                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-16                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-17                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-18                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-19                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-20                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-21                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-22                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-23                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-24                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-25                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-26                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-27                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-28                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-29                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-30                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-31                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-32                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-33                  [10, 1024, 14, 14]        2,167,808\n",
       "|    └─Bottleneck: 2-34                  [10, 1024, 14, 14]        2,167,808\n",
       "├─Sequential: 1-5                        [10, 2048, 7, 7]          --\n",
       "|    └─Bottleneck: 2-35                  [10, 2048, 7, 7]          6,039,552\n",
       "|    └─Bottleneck: 2-36                  [10, 2048, 7, 7]          8,660,992\n",
       "|    └─Bottleneck: 2-37                  [10, 2048, 7, 7]          8,660,992\n",
       "├─AdaptiveAvgPool2d: 1-6                 [10, 2048, 1, 1]          --\n",
       "├─Linear: 1-7                            [10, 1000]                2,049,000\n",
       "==========================================================================================\n",
       "Total params: 76,981,288\n",
       "Trainable params: 76,981,288\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (T): 2.98\n",
       "==========================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 128.53\n",
       "Params size (MB): 307.93\n",
       "Estimated Total Size (MB): 442.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(res101,datashape,depth=2,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
