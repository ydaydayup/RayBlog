{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc011fb4-d4c6-4ff1-8fe2-a52645f464c0",
   "metadata": {},
   "source": [
    "**Transformer实战（上）**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab255b3e-df98-498b-b516-247f76815b24",
   "metadata": {},
   "source": [
    "# Transformer案例实战 之 机器翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb63be3-b2b7-4424-bbde-f3f2927cf507",
   "metadata": {},
   "source": [
    "机器翻译是一种使用计算机程序将一种语言的文字自动翻译成另一种语言的技术，在Transformer的应用中拥有重要的地位。这个任务涉及到从源语言中抽取意义，并将这些意义准确地表达在目标语言中，同时尽可能保持原文的风格和语境。现代的机器翻译系统通常采用Transformer为核心的深度学习技术，通过训练大量的双语数据集来学习语言之间的映射关系。这些系统能够理解和翻译复杂的句子结构，适应不同的语言特点，并处理词义多变的情况。机器翻译是自然语言处理领域中的一个重要而活跃的研究方向，广泛应用于全球化交流、多语种内容创作和国际贸易等领域。\n",
    "\n",
    "对于Transformer模型来说，机器翻译是一个典型的序列到序列（Seq2Seq）的有监督任务。这意味着模型需要接收一系列的输入（源语言的文本），并输出另一系列的文本（目标语言的翻译）。Transformer中的自注意力机制允许模型在翻译时能够关注到输入序列中的所有单词，帮助模型理解更复杂的语言结构和语境依赖，使得翻译更加准确和流畅；同时，在机器翻译任务重，**Transformer中的编码器用于理解输入文本，而解码器则用于生成翻译文本**。每一层都进一步提炼和传递信息，增强了模型在处理复杂文字时的能力。\n",
    "\n",
    "在今天的项目中，我们将完成一个英文到中文的翻译任务。通常来说，一个机器翻译任务需要覆盖至少如下的流程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80878984-318f-4175-a836-908dd89c922c",
   "metadata": {},
   "source": [
    "- 一、数据准备\n",
    "\n",
    "对于任何NLP任务来说，我们都需要对数据进行详细的处理、毕竟算法本身无法处理文字数据，而语言数据同时带有时序属性和文字属性，具有复杂的处理流程。\n",
    "\n",
    "1. **环境设置**：定义了工作目录和模型目录，用于存储数据和模型检查点。\n",
    "\n",
    "2. **数据加载**：准备英文和中文的句子文件，并确认这两个文件的行数相同，保证每行英文句子对应一行中文翻译。机器翻译模型通常需要规模较大、同时也足够精确的翻译数据来进行训练，这部分数据通常需要人为进行翻译来构建。当然现在在有人类审核的情况下，我们可以通过大语言模型来辅助我们生成一个个的翻译数据集；在经典的深度学习领域，也有一些常用的机器翻译数据集——\n",
    "\n",
    "> 中英数据集:\n",
    "> - LDC (Linguistic Data Consortium) 数据集：LDC提供了多种中英双语数据集，这些数据集广泛用于学术研究和商业应用。\n",
    "> - CWMT (China Workshop on Machine Translation)：CWMT是中国的机器翻译评测活动，提供了一些中英平行语料库，包括新闻、法律文件等多个领域。\n",
    "\n",
    "> 中法数据集:\n",
    "> - United Nations Parallel Corpus：虽然这个数据集涵盖多种语言，但其中也包含大量的中法对照材料，适合用于法律和国际关系领域的翻译训练。\n",
    "\n",
    "> 中日数据集:\n",
    "> - Tanaka Corpus：这是一个开源的中日双语语料库，主要包含日常用语，适合用于基础对话和日常交流的翻译训练。\n",
    "> - JEC Basic Sentence Data：这个数据集包含基础的中日句子对，适用于初学者和基础翻译模型的训练。\n",
    "\n",
    "> 中韩数据集:\n",
    "> - KAIST Korean-Chinese Parallel Corpus：由韩国科学技术院（KAIST）提供，包括韩语和中文的平行文本，适用于科技和教育领域的翻译。\n",
    "> - Naver Labs’ Multilingual Corpus：包含中韩以及其他语言对，适合用于多语言翻译系统的开发和测试。\n",
    "\n",
    "当然，也还有很多涉及其他语言之间互相翻译的经典数据集——\n",
    "\n",
    "> Europarl：包含欧洲议会的会议记录，涵盖多种欧洲语言。这个数据集非常适合训练政治和法律文本的翻译模型。\n",
    "\n",
    "> WMT (Workshop on Machine Translation)：每年都会发布新的数据集，用于机器翻译的国际比赛。这些数据集包含多种语言对，涉及新闻和其他类型的文本，非常适合用来评估和比较不同的翻译算法。\n",
    "\n",
    "> UN Parallel Corpus：联合国文件的平行语料库，包括六种官方语言：英语、法语、西班牙语、俄语、阿拉伯语和中文。这个数据集特别适合用来训练和测试法律和外交文本的翻译模型。\n",
    "\n",
    "> IWSLT (International Workshop on Spoken Language Translation)：主要关注口语翻译，包含TED演讲等的多语种数据，适合用来训练口语风格的翻译模型。\n",
    "\n",
    "> Tatoeba：一个包含简短句子的多语种平行语料库，适用于基础语言学习和测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa880940-64de-4ae4-b639-41597a1a2194",
   "metadata": {},
   "source": [
    "3. **分词**：在获得数据集之后，首先我们要进行分词。对于英文、法文这些天然就有空格来进行分词的语言来说，是否进行分词或许没有那么重要，但对于中文、日语这些没有天然分分隔的语言来说，分词是无法跳过的步骤。分词，又叫做Tokenizer，意思是将文字分成最小语义单元Token。你或许注意到过，基于Transformer的GPT、BERT等结构是“一个词一个词往外蹦”着进行预测的，其实就是一个token、一个token地进行预测的。因此，分词即是为了输入，也是为了输出。\n",
    "\n",
    "在深度学习和最近的NLP模型中，如BERT和GPT系列模型，采用了一种称为subword tokenization的方法，例如Byte-Pair Encoding (BPE) 或 SentencePiece。这些方法在某种程度上可以绕过传统分词的需求，通过将词汇分解成更小的、意义相关的片段（subwords），既保留了词汇的部分语义，也能有效处理未知或罕见词汇的问题。在本次的案例中，我们需要将采用`transformers`库的`AutoTokenizer`和`BertTokenizer`对英文进行分词，中文分词则通过字符级分割（一个字是一个Token）。\n",
    "\n",
    "4. **构建词典**：当我们将句子进行分词之后，下一步最好可以构建当前句子所构成的词典。词典不是一个必备流程，但是一个优化的流程，我们可以利用词典创造和embedding不一样的数据编码结果，丰富数据编码的形式，也可以直接在构建好的词典的基础上进行embedding。**在使用pytorch实现序列到序列（Seq2Seq）任务的时候，nn.Transformer中是不包括位置编码、掩码等encoder、decoder之前的结构的；因此我们首先要将数据传入embedding和位置编码结构，然后才会将数据传入Transformer，我们在构建词典后、真正获得的数据是与词典紧密相连的单词索引，而非直接放入Transoformer的数据**。我们通常会基于相同的词典将训练集、测试集进行索引，同时编码器和解码器都会基于相同的词典将单词索引映射为embedding后的向量，以确保输入和输出的一致性。在本次案例中，我们将利用`torchtext.vocab`的`build_vocab_from_iterator`函数，基于分词结果构建英文和中文的词典。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b245dc1-1c69-47aa-b5a9-ca9a86ca9f26",
   "metadata": {},
   "source": [
    "> - **经过词典所构建的数据（也就是单词索引）一般是什么样的？**\n",
    "\n",
    "源语言的单词索引序列通常是二维的数据结构。每个维度代表的含义如下：\n",
    "\n",
    "第一维代表批次中的句子数量（也就是batch size）。这允许模型同时处理多个句子，提高处理效率。\n",
    "\n",
    "第二维代表每个句子中的单词索引。句子长度可以是固定的（通过截断或填充来标准化），这样每个批次的数据形状才能保持一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4151f-66c5-4d46-af59-31f0b2356d7b",
   "metadata": {},
   "source": [
    "每个单词索引是一个整数，表示在词典中的位置。这里有一个简单的Markdown代码例子来表示源语言的单词索引序列。假设我们有以下中文句子，已经过分词和转换为索引：\n",
    "\n",
    "句子1: \"我 爱 北京 天安门\"\n",
    "句子2: \"北京 是 中国 的 首都\"\n",
    "\n",
    "假设词典中每个词对应的索引如下：\n",
    "\n",
    "我 -> 1, 爱 -> 2, 北京 -> 3, 天安门 -> 4, 是 -> 5, 中国 -> 6, 的 -> 7, 首都 -> 8\n",
    "\n",
    "如果将这些句子转换为单词索引序列，并考虑到填充（以保持统一长度，这里填充的索引为0），则可得到："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a6d06-c278-42f7-970e-752236d8b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "句子1索引: [1, 2, 3, 4]\n",
    "\n",
    "句子2索引: [3, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b4483-3368-4257-8f5a-b223c025c10a",
   "metadata": {},
   "source": [
    "为了在一个批次中处理，假设填充每个句子到最大长度5，则单词索引序列为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247ebeb-de26-4848-822c-4c14549453da",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1, 2, 3, 4, 0],  # 句子1填充后\n",
    "\n",
    " [3, 5, 6, 7, 8]]  # 句子2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf23288-e71a-41ec-a622-7301b094bc03",
   "metadata": {},
   "source": [
    "结构为（2,5），代表一共有2个句子，句子的最大长度为5。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b494d-82ee-495a-8687-8062b7d5bd38",
   "metadata": {},
   "source": [
    "- 二、模型构建\n",
    "\n",
    "在之前的课程中，我们详细复现过Transformer架构。在构建Transformer模型的过程中，首先要确定模型的极大核心结构，包括位置编码层、编码器层、解码器层、自注意力机制层、前馈网络等等；然后要确定模型的关键结构参数，如编解码器层数、每层的维度和头的数量，尤其是必须理清每个神经网络结构的输入和输出数据状态。\n",
    "\n",
    "1. **位置编码**：\n",
    "在Transformer模型中，位置编码用于向模型提供关于单词在句子中位置的信息，因为模型的自注意力层本身不包含处理序列顺序的机制。位置编码通过将位置信息编码为向量并与词向量相加，使得模型能够考虑到词语的顺序关系，从而理解语言的语法和句子结构。这样的设计允许Transformer有效地处理自然语言，并保持在不同位置的词语能够被区分和正确解释。我们定义了`PositionalEncoding`类为Transformer模型提供位置信息，这对于处理序列数据至关重要。\n",
    "\n",
    "2. **模型定义**：在我们的案例中，我们使用了一个基本的Transformer模型，命名为`TranslationModel`类，它包括源语言嵌入、目标语言嵌入、位置编码、Transformer层和最终的预测层。对于机器翻译这样seq2seq的任务来说，编码器的输入数据结构为源语言的单词索引序列，解码器的数据数据结构为目标语言的单词索引序列，在seq2seq过程中，一般Transformer对输入数据的处理过程是：\n",
    "> **文本转索引**：原始文本通过查找构建好的词典被转换为单词索引序列。这一步是将自然语言中的单词转换为模型可以理解的数值形式。<br><br>\n",
    "> **索引转嵌入**：这些单词索引接着被用来从嵌入层中获取对应的词向量。这一步是通过词嵌入将每个单词的索引映射到一个高维空间中，以便捕捉和表达词汇的语义和语法属性。<br><br>\n",
    "> **加入位置编码**：在获取词向量后，模型还会加入位置编码，以引入序列中各单词的位置信息，这对于帮助模型理解词语之间的关系非常重要。\n",
    "\n",
    "Transformer模型在执行机器翻译任务时，编码器负责进行源语言的信息提取，解码器负责输出翻译的内容，这个过程是有先后顺序之分的。回顾一下Transformer的原理，解码器在输出内容时，会需要编码器输出的原文的内容、同时也会需要掩码后正确标签的内容（如果你无法理解这个，回去看Transformer原理）、或者自己已经生成的内容（例如，在生成第n个词时，解码器会考虑到第1到n-1个已生成的词。当然，大部分时候，解码器会考虑的是正确的标签中的1到n-1个词）。因此解码器必须在编码器结束运行之后才能开始运行，这使得编码器会先输入数据、运行完毕后，解码器才会开始输入数据。\n",
    "\n",
    "在实际的代码实现中，这种先后顺序通常体现在数据流的处理上。首先，源语言的数据被送入编码器，编码器处理后的输出被存储起来。接着，这些编码器的输出连同解码器的初始输入（通常是一个起始符号）一起送入解码器。解码器基于这些输入逐步生成目标语言的输出。在某些实现中，这个过程可能在一个大的循环中反复执行，特别是在使用教师强制（teacher forcing）的训练过程中，解码器的每一步输出都可能作为下一步的输入。\n",
    "\n",
    "需要注意的是，由于Transformer模型是一个字、一个字地进行输入，因此也是一个字一个字地进行输出。为了判断当前句子中下一个最合适的字是什么，**无论是编码器还是解码器、输出的结构都是一个向量，通常表示为对词汇表的一个概率分布，即每个词/字作为句子中正确的下一个词/字的概率**。最终，这些概率分布被用来生成目标语言的文本，通常是通过选取每一步概率最高的单词。\n",
    "\n",
    "3. **损失函数**：在我们的案例中，我们使用`KLDivLoss`进行损失计算，适用于处理预测分布与实际分布的差异。KLDivLoss，即Kullback-Leibler散度损失，是一种在机器学习中常用于衡量两个概率分布差异的损失函数。它主要用于比较目标概率分布 Q 和预测概率分布 P 之间的相对熵，也就是这两个分布的差异程度。\n",
    "\n",
    "$$D_{KL}(P \\parallel Q) = \\sum_i Q(i) \\log \\left(\\frac{Q(i)}{P(i)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f567b7-d0be-4546-9fef-82f67afefa32",
   "metadata": {},
   "source": [
    "其中：\n",
    "- P是模型预测的概率分布，通常是对数概率形式。\n",
    "- Q是目标或真实概率分布。\n",
    "- i索引覆盖所有可能的事件。\n",
    "- Q(i)和P(i)分别是在分布 Q 和 P 中第 i 个事件的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a904c1a-538c-42cc-8504-60707e977cc0",
   "metadata": {},
   "source": [
    "如果 P 是模型输出的对数概率形式，公式可以调整为：\n",
    "\n",
    "$$D_{KL}(P \\parallel Q) = \\sum_i Q(i) (\\log Q(i) - P(i))$$\n",
    "\n",
    "在机器翻译任务中使用`KLDivLoss`作为损失函数可以非常有效，主要原因在于机器翻译中的输出通常被表示为词汇表上的概率分布。每一个输出词汇的概率反映了在给定的上下文中该词被选择作为翻译的可能性。`KLDivLoss`可以衡量模型生成的概率分布与目标概率分布之间的距离，它允许模型学习到更细致的概率差异，尤其是当目标分布在某些词汇上具有高度确定性时，KLDivLoss 可以强调这些词汇的重要性，从而指导模型更准确地预测目标语言中每个词的概率。\n",
    "\n",
    "除了 KLDivLoss，机器翻译过程中还可以使用其他几种损失函数：\n",
    "> - 交叉熵损失（Cross Entropy Loss）：这是机器翻译中最常用的损失函数之一。它测量的是预测概率分布与实际概率分布之间的差异，特别适用于分类问题，其中输出是一个离散的类别变量（在机器翻译中即词汇表中的单词）。<br><br>\n",
    "> - 困惑度（Perplexity）：虽然困惑度本身不是一个损失函数，但它是评价语言模型性能的一种标准，常常用来衡量模型在机器翻译任务中的表现。低困惑度意味着模型对数据的预测更加准确。<br><br>\n",
    "> - 标签平滑（Label Smoothing）：这是一种正则化技术，通常与交叉熵损失结合使用。通过给非目标标签分配一个小的非零概率，它可以帮助改善模型的泛化能力，减少模型对某些频繁标签的过度自信。<br><br>\n",
    "> - L2损失：虽然不常见，但在一些特定情况下，如模型输出与目标输出之间的误差是连续的，也可以考虑使用L2损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ff3dc-b5da-4698-bc33-86ebdfd4d242",
   "metadata": {},
   "source": [
    "- 三、训练过程\n",
    "\n",
    "相比起数据和架构定义的过程，机器翻译的训练过程对我们来说相对简单和熟悉。我们需要定义Dataloader等结构，并且需要保持每个batch内的数据长度一致（需要对短的数据进行填充，对长的数据进行裁剪）。除此之外，我们需要定义优化器、进行epoch上的循环等过程。这个流程与其他深度学习算法并无区别，因此对大家来说相对容易。\n",
    "\n",
    "1. **数据加载器**：在PyTorch定义`DataLoader`用于批量加载数据，以及`collate_fn`函数对数据进行适当处理，确保批次内的数据长度一致。\n",
    "2. **优化器**：我们选择Adam优化器进行参数优化。\n",
    "3. **训练循环**：进行模型训练，包括前向传播、损失计算、梯度计算和参数更新。同时设置定期保存模型的逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821a9c7-b698-4ccc-86f7-aef62ac2d55a",
   "metadata": {},
   "source": [
    "- 四、推理和翻译\n",
    "1. **翻译函数**：对于Transformer来说，输出的结构是一个个的概率分布，因此我们还需要将该概率分布转化为具体的文字。因此我们需要定义`translate`函数，将训练好的模型打包、接受一个英文句子作为输入，逐词生成其中文翻译。\n",
    "2. **逐步预测**：从`<bos>`开始，逐步通过模型预测下一个词，直到生成`<eos>`或达到最大句子长度。\n",
    "\n",
    "接下来就让我们一步步来完成这个案例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001afc9c-b13f-4ea6-a46a-4e2e8b86e0e1",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c37a49-5446-4995-85cb-9283fbab86d4",
   "metadata": {},
   "source": [
    "### 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413eecca-0654-4874-ae09-a149de3f0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入所需的库\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# hugging face的分词器，github地址：https://github.com/huggingface/tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "# 用于构建词典\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.functional import pad, log_softmax\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dbec00-be5e-491c-acd8-1e64e6f0a68a",
   "metadata": {},
   "source": [
    "在这里，对你来说可能需要全新安装的库有torchtext，tensorboard以及tqdm。你可以使用下面的代码来进行安装。需要注意的是，torchtext需要与你的pytorch版本进行匹配后才能安装成功，匹配表如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9d6732-ef2d-489a-b3ea-d0d33b6d0752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1eead-10dd-4beb-b5f9-e22ee26f8b98",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/Case/01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d49024-01fe-4023-a7b3-87fc0728448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.17.0\n",
      "  Downloading torchtext-0.17.0-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.17.0) (4.62.3)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.17.0) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.17.0) (2.2.0+cu121)\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.17.0) (1.22.4)\n",
      "Collecting torchdata==0.7.1 (from torchtext==0.17.0)\n",
      "  Downloading torchdata-0.7.1-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.2.0->torchtext==0.17.0) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.2.0->torchtext==0.17.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.2.0->torchtext==0.17.0) (1.9)\n",
      "Requirement already satisfied: networkx in d:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.2.0->torchtext==0.17.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.2.0->torchtext==0.17.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.2.0->torchtext==0.17.0) (2024.2.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in d:\\programdata\\anaconda3\\lib\\site-packages (from torchdata==0.7.1->torchtext==0.17.0) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.0) (2024.2.2)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torchtext==0.17.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.0->torchtext==0.17.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.0->torchtext==0.17.0) (1.3.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (d:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (d:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading torchtext-0.17.0-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 1.9/1.9 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading torchdata-0.7.1-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 1.3/1.3 MB 650.2 kB/s eta 0:00:00\n",
      "Installing collected packages: torchdata, torchtext\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.2.0\n",
      "    Uninstalling torchtext-0.2.0:\n",
      "      Successfully uninstalled torchtext-0.2.0\n",
      "Successfully installed torchdata-0.7.1 torchtext-0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb80e33-ea7c-455a-8582-ecc8a27c0250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.62.2-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (1.22.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (4.25.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (58.0.4)\n",
      "Requirement already satisfied: six>1.9 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (2.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.6.0)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 5.5/5.5 MB 982.4 kB/s eta 0:00:00\n",
      "Downloading grpcio-1.62.2-cp39-cp39-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 3.8/3.8 MB 644.8 kB/s eta 0:00:00\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "   -------------------------------------- 105.4/105.4 kB 674.8 kB/s eta 0:00:00\n",
      "Installing collected packages: grpcio, markdown, tensorboard\n",
      "Successfully installed grpcio-1.62.2 markdown-3.6 tensorboard-2.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (d:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (d:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69dc42e-1d18-4742-b503-a2ccb36453a8",
   "metadata": {},
   "source": [
    "如我们的流程所示，我们首先进行各种环境设置——"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8cff0c2-5b23-41f4-920e-dffc3cb22f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工作目录、为Transformer放数据、缓存文件的目录\n",
    "work_dir = Path(\"./dataset\")\n",
    "# 训练好的模型会放在该目录下，注意隔一段时间就要对模型进行保存，这是深度学习训练的基本\n",
    "model_dir = Path(r\"D:\\pythonwork\\2024DL\\model\")\n",
    "# 上次运行到的地方，如果是第一次运行，为None，如果中途暂停了，下次运行时，指定目前最新的模型即可。\n",
    "model_checkpoint = None # 'model_10000.pt'\n",
    "\n",
    "# 如果工作目录不存在，则创建一个\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "\n",
    "# 如果模型目录不存在，则创建一个\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# 英文句子的文件路径/数据集路径，\n",
    "en_filepath = './dataset/train.en'\n",
    "# 中文句子的文件路径/数据集路径\n",
    "zh_filepath = './dataset/train.zh'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f732a-2f49-4f5c-a77b-b205dffdd07d",
   "metadata": {},
   "source": [
    "- 查看数据与数据的属性 之 查看数据本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07885d6d-c020-4e97-8124-0a5aa3591eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentences (first 5 lines):\n",
      "A pair of red - crowned cranes have staked out their nesting territory\n",
      "A pair of crows had come to nest on our roof as if they had come for Lhamo.\n",
      "A couple of boys driving around in daddy's car.\n",
      "A pair of nines? You pushed in with a pair of nines?\n",
      "Fighting two against one is never ideal,\n"
     ]
    }
   ],
   "source": [
    "# 查看一下文件中的数据\n",
    "# 打开并读取英文文件\n",
    "with open(en_filepath, 'r', encoding='utf-8') as file:\n",
    "    # 读取所有行\n",
    "    english_sentences = file.readlines()\n",
    "    # 打印前5行，假设我们只想看前几行\n",
    "    print(\"English sentences (first 5 lines):\")\n",
    "    for line in english_sentences[:5]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155c19bf-2986-46ce-aacb-d73bd1c73162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chinese sentences (first 5 lines):\n",
      "一对丹顶鹤正监视着它们的筑巢领地\n",
      "一对乌鸦飞到我们屋顶上的巢里，它们好像专门为拉木而来的。\n",
      "一对乖乖仔开着老爸的车子。\n",
      "一对九？一对九你就全下注了？\n",
      "一对二总不是好事，\n"
     ]
    }
   ],
   "source": [
    "# 打开并读取中文文件\n",
    "with open(zh_filepath, 'r', encoding='utf-8') as file:\n",
    "    # 读取所有行\n",
    "    chinese_sentences = file.readlines()\n",
    "    # 打印前5行，同样只看前几行\n",
    "    print(\"\\nChinese sentences (first 5 lines):\")\n",
    "    for line in chinese_sentences[:5]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db257763-cbb2-4d93-9d82-67c10007a11f",
   "metadata": {},
   "source": [
    "- 查看数据与数据的属性 之 确保两种语言的样本量一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2650d3-285a-436e-a526-4b32406e59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个获取文件行数的方法\n",
    "# 在开始训练之前，一定要确保数据集中两种语言的句子是一一对应的\n",
    "def get_row_count(filepath):\n",
    "    count = 0\n",
    "    for _ in open(filepath, encoding='utf-8'):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# 英文句子数量\n",
    "en_row_count = get_row_count(en_filepath)\n",
    "# 中文句子数量\n",
    "zh_row_count = get_row_count(zh_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5fbc4b-40d4-4869-8fe7-c18664195553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_row_count #一共有1000w个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9be59a-c6ce-4985-b689-b61c87995764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证，如果句子数量不一致，则主动报错\n",
    "assert en_row_count == zh_row_count, \"英文和中文文件行数不一致！\"\n",
    "# 句子数量，主要用于后面显示进度。\n",
    "row_count = en_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d865220-a275-4222-9e0f-e94c3a2eb66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_row_count == zh_row_count #Assert是一种让代码主动发起报错的经典Python用法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da305de-79db-43c4-bebd-6e8914782f73",
   "metadata": {},
   "source": [
    "- 查看数据与数据的属性 之 确定每个句子的长度，方便进行裁剪和填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c20bb519-01ff-4664-bac1-dd4a0f59801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文句子 平均长度 = 54.03, 最大长度 = 571, 方差 = 1693.09\n",
      "中文句子: 平均长度 = 17.21, 最大长度 = 205, 方差 = 165.29\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sample_file_statistics(filepath, sample_fraction=0.01):\n",
    "    # 初始化统计变量\n",
    "    total_length = 0\n",
    "    length_square_sum = 0\n",
    "    max_length = 0\n",
    "    count = 0\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if random.random() < sample_fraction:\n",
    "                # 计算行长度\n",
    "                length = len(line.strip())\n",
    "                # 累加总长度\n",
    "                total_length += length\n",
    "                # 累加长度的平方，用于方差计算\n",
    "                length_square_sum += length ** 2\n",
    "                # 更新最大长度\n",
    "                max_length = max(max_length, length)\n",
    "                # 计数\n",
    "                count += 1\n",
    "\n",
    "    # 计算平均长度和方差\n",
    "    if count > 0:\n",
    "        average_length = total_length / count\n",
    "        variance = (length_square_sum - (total_length ** 2) / count) / count\n",
    "    else:\n",
    "        average_length = 0\n",
    "        variance = 0\n",
    "\n",
    "    return average_length, max_length, variance\n",
    "\n",
    "# 抽样并计算英文和中文句子的平均长度、最大长度和方差\n",
    "en_stats = sample_file_statistics(en_filepath)\n",
    "zh_stats = sample_file_statistics(zh_filepath)\n",
    "\n",
    "print(f\"英文句子 平均长度 = {en_stats[0]:.2f}, 最大长度 = {en_stats[1]}, 方差 = {en_stats[2]:.2f}\")\n",
    "print(f\"中文句子: 平均长度 = {zh_stats[0]:.2f}, 最大长度 = {zh_stats[1]}, 方差 = {zh_stats[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a06e78e-b9f2-4a4e-abc5-3fcfab7035ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量为： 10000000\n",
      "句子最大长度为： 72\n",
      "batch_size: 64\n",
      "每5000步保存一次模型\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 定义句子最大长度，如果句子不够这个长度，则填充，若超出该长度，则裁剪\n",
    "# 我们的数据集中方差超级大，所以一般会设置超出平均长度一些\n",
    "# 如果你需要更强大的模型，你应该顺应最大长度\n",
    "max_length = 72\n",
    "print(\"句子数量为：\", en_row_count)\n",
    "print(\"句子最大长度为：\", max_length)\n",
    "\n",
    "# 定义英文和中文词典，都为Vocab类对象，后面会对其初始化\n",
    "en_vocab = None\n",
    "zh_vocab = None\n",
    "\n",
    "# 定义batch_size，由于是训练文本，占用内存较小，可以适当大一些\n",
    "batch_size = 64\n",
    "# epochs数量，不用太大，因为句子数量较多，甚至可以设置为1\n",
    "# 越大的模型、收到epochs的影响越小，在许多大语言模型的训练环境中，我们都是使用epoch=1的情况\n",
    "epochs = 1\n",
    "# 多少步保存一次模型，防止程序崩溃导致模型丢失。\n",
    "save_after_step = 5000\n",
    "\n",
    "# 是否使用缓存，由于文件较大，初始化动作较慢，所以将初始化好的文件持久化\n",
    "use_cache = True\n",
    "\n",
    "# 定义训练设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"batch_size:\", batch_size)\n",
    "print(\"每{}步保存一次模型\".format(save_after_step))\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3295670-d7ab-437e-b121-2b3512eedab4",
   "metadata": {},
   "source": [
    "### 分词与词典构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ad148-42a4-4bdb-81fd-366d53ada63d",
   "metadata": {},
   "source": [
    "我们要从huggingface库中直接调出相应的分词器来用，分词器是自然语言处理中用来将原始文本转换为模型可以理解的格式的工具。它通常执行以下任务：\n",
    "\n",
    "- 分词（Tokenization）：将连续的文本字符串分割成离散的单元（tokens），例如单词、子词或符号。\n",
    "  \n",
    "- 添加特殊标记：例如，为句子添加开始 [CLS] 和结束 [SEP] 标记，这在某些模型如BERT中是必需的。\n",
    "  \n",
    "- 生成注意力掩码：告诉模型哪些部分是真实数据，哪些部分是填充。\n",
    "  \n",
    "- 转换为ID：将每个token转换为词汇表中的唯一ID。\n",
    "\n",
    "在这里我们使用的 AutoTokenizer 是 huggingface的transformers 库中的一个工具，可以自动根据给定的预训练模型名称加载相应的分词器。这样做的好处是你不需要知道背后具体的分词器细节，只需要提供模型的名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55b388-c6cd-4d3f-890f-b43f6c0b6b58",
   "metadata": {},
   "source": [
    "注意，huggingface的使用需要全局代理魔法。没有魔法或魔法不合格，则会报`MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443),Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))`错误。如果尝试调用太多次，也会报上述错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ee00a9-5ed5-490a-ad33-0f8cf00e8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在 Ping huggingface.co [157.240.7.8] 具有 32 字节的数据:\n",
      "请求超时。\n",
      "请求超时。\n",
      "请求超时。\n",
      "请求超时。\n",
      "\n",
      "157.240.7.8 的 Ping 统计信息:\n",
      "    数据包: 已发送 = 4，已接收 = 0，丢失 = 4 (100% 丢失)，\n"
     ]
    }
   ],
   "source": [
    "!ping huggingface.co #使用这段代码来验证你是否能连得上huggingface，如果你的代码显示下面的内容，证明你无法连接huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e5875-aef2-473b-bfc6-deb43172c7ad",
   "metadata": {},
   "source": [
    "如果你无法连接huggingface，我的建议是你直接访问huggingface.co的网页端，并在网页端将你所需的模型放到本地。你可以参考这篇文章，或者直接使用镜像网站hf-mirror:\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/689290456\n",
    "\n",
    "> https://hf-mirror.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "286b1f07-9780-4ef4-ae3b-bc5f2ac77618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000017F49F4FA30>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 23ebcfa7-57c0-4a0f-a925-dc5909b1f69b)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000180FDB8DC60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 40961f3b-4d34-4831-a93e-83ed13278223)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# 加载基础的分词器模型，使用的是基础的bert模型。`uncased`意思是不区分大小写\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def en_tokenizer(line):\n",
    "    \"\"\"\n",
    "    定义英文分词器，后续也要使用\n",
    "    :param line: 一句英文句子，例如\"I'm learning Deep learning.\"\n",
    "    :return: subword分词后的记过，例如：['i', \"'\", 'm', 'learning', 'deep', 'learning', '.']\n",
    "    \"\"\"\n",
    "    # 使用bert进行分词，并获取tokens。add_special_tokens是指不要在结果中增加‘<bos>’和`<eos>`等特殊字符\n",
    "    return tokenizer.encode(line, add_special_tokens=False).tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ced2bc-c98b-4ba5-bf5f-18dce42f1164",
   "metadata": {},
   "source": [
    "即便导入成功，在多次访问后依然会出现maxretry的错误 ↑ 因此最好是将模型下载到本地后，直接从本地进行调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6b634bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000017F49F4F1C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 3b939734-2337-4b97-b73c-e8b230da758a)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def en_tokenizer(line):\n",
    "    return tokenizer.convert_ids_to_tokens(tokenizer.encode(line,  add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05abed1b-fb8a-48d2-8404-43b9597d150a",
   "metadata": {},
   "source": [
    "看一下英语的tokenizer分词的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b5a3a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'a', 'english', 'token', '##izer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(en_tokenizer(\"I'm a English tokenizer.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643052a7-1266-4486-9033-43287cef3432",
   "metadata": {},
   "source": [
    "接下来，我们需要根据分好的词语来建立词典。在这里需要注意的是，在定义分词函数时，我们需要使用yield关键字来帮助我们节约内存。yield是一个非常有用的关键字，它可以允许你在创建函数时将函数设置成类似迭代器的形式，这样的函数在每次被调用时不需要一次加载整个数据集到内存中，可以不断地访问大型数据集中的片段数据。这对于处理大数据文件特别有用，因为它可以显著减少内存使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c54e1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_en_tokens():\n",
    "    \"\"\"\n",
    "    每次yield一个分词后的英文句子，之所以yield方式是为了节省内存。\n",
    "    如果先分好词再构造词典，那么将会有大量文本驻留内存，造成内存溢出。\n",
    "    \"\"\"\n",
    "    file = open(en_filepath, encoding='utf-8')\n",
    "    print(\"-------开始构建英文词典-----------\")\n",
    "    for line in tqdm(file, desc=\"构建英文词典\", total=row_count):\n",
    "        yield en_tokenizer(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed4d28-9249-48bf-abe3-7a4f11bcddd9",
   "metadata": {},
   "source": [
    "接下来我们结合缓存机制、构造英文词典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1d99005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定英文词典缓存文件路径\n",
    "en_vocab_file = work_dir / \"vocab_en.pt\"\n",
    "# 如果使用缓存，且缓存文件存在，则加载缓存文件\n",
    "if use_cache and os.path.exists(en_vocab_file):\n",
    "    en_vocab = torch.load(en_vocab_file, map_location=\"cpu\")\n",
    "# 否则就从0开始构造词典\n",
    "else:\n",
    "    # 构造词典\n",
    "    en_vocab = build_vocab_from_iterator(\n",
    "        # 传入一个可迭代的token列表。例如[['i', 'am', ...], ['machine', 'learning', ...], ...]\n",
    "        yield_en_tokens(),\n",
    "        # 最小频率为2，即一个单词最少出现两次才会被收录到词典\n",
    "        min_freq=2,\n",
    "        # 在词典的最开始加上这些特殊token\n",
    "        specials=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"],\n",
    "    )\n",
    "    # 设置词典的默认index，后面文本转index时，如果找不到，就会用该index填充\n",
    "    en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "    # 保存缓存文件\n",
    "    if use_cache:\n",
    "        torch.save(en_vocab, en_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9e5de19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文词典大小: 27584\n",
      "{0: '<s>', 1: '</s>', 2: '<pad>', 3: '<unk>', 4: '.', 5: ',', 6: 'the', 7: \"'\", 8: 'i', 9: 'you'}\n"
     ]
    }
   ],
   "source": [
    "# 打印一下看一下效果，用前10个词为例\n",
    "print(\"英文词典大小:\", len(en_vocab))\n",
    "print(dict((i, en_vocab.lookup_token(i)) for i in range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc187f-1231-4b9c-8e81-9a96641a74ea",
   "metadata": {},
   "source": [
    "接下来是中文词典的构造："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9e8f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_tokenizer(line):\n",
    "    \"\"\"\n",
    "    定义中文分词器\n",
    "    :param line: 中文句子，例如：机器学习\n",
    "    :return: 分词结果，例如['机','器','学','习']\n",
    "    \"\"\"\n",
    "    return list(line.strip().replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "def yield_zh_tokens():\n",
    "    file = open(zh_filepath, encoding='utf-8')\n",
    "    for line in tqdm(file, desc=\"构建中文词典\", total=row_count):\n",
    "        yield zh_tokenizer(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9027b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_vocab_file = work_dir / \"vocab_zh.pt\"\n",
    "if use_cache and os.path.exists(zh_vocab_file):\n",
    "    zh_vocab = torch.load(zh_vocab_file, map_location=\"cpu\")\n",
    "else:\n",
    "    zh_vocab = build_vocab_from_iterator(\n",
    "        yield_zh_tokens(),\n",
    "        min_freq=1,\n",
    "        specials=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"],\n",
    "    )\n",
    "    zh_vocab.set_default_index(zh_vocab[\"<unk>\"])\n",
    "    torch.save(zh_vocab, zh_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "753cf4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文词典大小: 8280\n",
      "{0: '<s>', 1: '</s>', 2: '<pad>', 3: '<unk>', 4: '。', 5: '的', 6: '，', 7: '我', 8: '你', 9: '是'}\n"
     ]
    }
   ],
   "source": [
    "# 打印看一下效果，用前10个词为例\n",
    "print(\"中文词典大小:\", len(zh_vocab))\n",
    "print(dict((i, zh_vocab.lookup_token(i)) for i in range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981f61b-f79c-4670-9351-0d2964a917c0",
   "metadata": {},
   "source": [
    "如此我们的词典就构造完成了。接下来我们将上述所有流程打包在一个继承自Dataset类的pytorch数据读取类中，帮助我们将数据以tensor的形式呈现给pytorch算法："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604edc0-9118-48dc-baad-5426487039cd",
   "metadata": {},
   "source": [
    "### 数据批量加载为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b432c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 加载英文tokens\n",
    "        self.en_tokens = self.load_tokens(en_filepath, en_tokenizer, en_vocab, \"构建英文tokens\", 'en')\n",
    "        # 加载中文tokens\n",
    "        self.zh_tokens = self.load_tokens(zh_filepath, zh_tokenizer, zh_vocab, \"构建中文tokens\", 'zh')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.en_tokens[index], self.zh_tokens[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return row_count\n",
    "\n",
    "    def load_tokens(self, file, tokenizer, vocab, desc, lang):\n",
    "        \"\"\"\n",
    "        加载tokens，即将文本句子们转换成index们。\n",
    "        :param file: 文件路径，例如\"./dataset/train.en\"\n",
    "        :param tokenizer: 分词器，例如en_tokenizer函数\n",
    "        :param vocab: 词典, Vocab类对象。例如 en_vocab\n",
    "        :param desc: 用于进度显示的描述，例如：构建英文tokens\n",
    "        :param lang: 语言。用于构造缓存文件时进行区分。例如：’en‘\n",
    "        :return: 返回构造好的tokens。例如：[[6, 8, 93, 12, ..], [62, 891, ...], ...]\n",
    "        \"\"\"\n",
    "\n",
    "        # 定义缓存文件存储路径\n",
    "        cache_file = work_dir / \"tokens_list.{}.pt\".format(lang)\n",
    "        # 如果使用缓存，且缓存文件存在，则直接加载\n",
    "        if use_cache and os.path.exists(cache_file):\n",
    "            print(f\"正在加载缓存文件{cache_file}, 请稍后...\")\n",
    "            return torch.load(cache_file, map_location=\"cpu\")\n",
    "\n",
    "        # 从0开始构建，定义tokens_list用于存储结果\n",
    "        tokens_list = []\n",
    "        # 打开文件\n",
    "        with open(file, encoding='utf-8') as file:\n",
    "            # 逐行读取\n",
    "            for line in tqdm(file, desc=desc, total=row_count):\n",
    "                # 进行分词\n",
    "                tokens = tokenizer(line)\n",
    "                # 将文本分词结果通过词典转成index\n",
    "                tokens = vocab(tokens)\n",
    "                # append到结果中\n",
    "                tokens_list.append(tokens)\n",
    "        # 保存缓存文件\n",
    "        if use_cache:\n",
    "            torch.save(tokens_list, cache_file)\n",
    "\n",
    "        return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8da6f471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载缓存文件dataset\\tokens_list.en.pt, 请稍后...\n",
      "正在加载缓存文件dataset\\tokens_list.zh.pt, 请稍后...\n"
     ]
    }
   ],
   "source": [
    "dataset = TranslationDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02b2e8-0f2e-4c30-864f-d0995f43a9be",
   "metadata": {},
   "source": [
    "查看编码后的句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18d8064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([11, 2730, 12, 554, 19, 17210, 18077, 27, 3078, 203, 57, 102, 18832, 3653], [12, 40, 1173, 1084, 3169, 164, 693, 397, 84, 100, 14, 5, 1218, 2397, 535, 67])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030208a7-b5c1-4bac-9d7f-f709c8497e7e",
   "metadata": {},
   "source": [
    "现在，我们要定义一个函数，将短句子进行填充、长句子进行裁剪："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b2fc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将dataset的数据进一步处理，并组成一个batch。\n",
    "    :param batch: 一个batch的数据，例如：\n",
    "                  [([6, 8, 93, 12, ..], [62, 891, ...]),\n",
    "                  ....\n",
    "                  ...]\n",
    "    :return: 填充后的且等长的数据，包括src, tgt, tgt_y, n_tokens\n",
    "             其中src为原句子，即要被翻译的句子\n",
    "             tgt为目标句子：翻译后的句子，但不包含最后一个token\n",
    "             tgt_y为label：翻译后的句子，但不包含第一个token，即<bos>\n",
    "             n_tokens：tgt_y中的token数，<pad>不计算在内。\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义'<bos>'的index，在词典中为0，所以这里也是0\n",
    "    bs_id = torch.tensor([0])\n",
    "    # 定义'<eos>'的index\n",
    "    eos_id = torch.tensor([1])\n",
    "    # 定义<pad>的index\n",
    "    pad_id = 2\n",
    "\n",
    "    # 用于存储处理后的src和tgt\n",
    "    src_list, tgt_list = [], []\n",
    "\n",
    "    # 循环遍历句子对儿\n",
    "    for (_src, _tgt) in batch:\n",
    "        \"\"\"\n",
    "        _src: 英语句子，例如：`I love you`对应的index\n",
    "        _tgt: 中文句子，例如：`我 爱 你`对应的index\n",
    "        \"\"\"\n",
    "\n",
    "        processed_src = torch.cat(\n",
    "            # 将<bos>，句子index和<eos>拼到一块\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    _src,\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    _tgt,\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        将长度不足的句子进行填充到max_padding的长度的，然后增添到list中\n",
    "\n",
    "        pad：假设processed_src为[0, 1136, 2468, 1349, 1]\n",
    "             第二个参数为: (0, 72-5)\n",
    "             第三个参数为：2\n",
    "        则pad的意思表示，给processed_src左边填充0个2，右边填充67个2。\n",
    "        最终结果为：[0, 1136, 2468, 1349, 1, 2, 2, 2, ..., 2]\n",
    "        \"\"\"\n",
    "        src_list.append(\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (0, max_length - len(processed_src),),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_length - len(processed_tgt),),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 将多个src句子堆叠到一起\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "\n",
    "    # tgt_y是目标句子去掉第一个token，即去掉<bos>\n",
    "    tgt_y = tgt[:, 1:]\n",
    "    # tgt是目标句子去掉最后一个token\n",
    "    tgt = tgt[:, :-1]\n",
    "\n",
    "    # 计算本次batch要预测的token数\n",
    "    n_tokens = (tgt_y != 2).sum()\n",
    "\n",
    "    # 返回batch后的结果\n",
    "    return src, tgt, tgt_y, n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaef0f0-3d08-464e-a914-d3fd4b6b50ae",
   "metadata": {},
   "source": [
    "在Dataloader中将我们的数据集进行打包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa822c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4284873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt, tgt_y, n_tokens = next(iter(train_loader))\n",
    "src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad030b5c-f471-4cad-bd96-b7838f5d2e8a",
   "metadata": {},
   "source": [
    "最终全部裁剪到72个token以下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1c0ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.size: torch.Size([64, 72])\n",
      "tgt.size: torch.Size([64, 71])\n",
      "tgt_y.size: torch.Size([64, 71])\n",
      "n_tokens: tensor(1266)\n"
     ]
    }
   ],
   "source": [
    "print(\"src.size:\", src.size())\n",
    "print(\"tgt.size:\", tgt.size())\n",
    "print(\"tgt_y.size:\", tgt_y.size())\n",
    "print(\"n_tokens:\", n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9eb2e-53d7-464e-aa64-a0cf916f250a",
   "metadata": {},
   "source": [
    "正如我们所说，此时transformer的数据输入结构为二维的单词索引，每个batch有64个句子，每个句子最长包含72个单词的索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f36bb-310b-4a3e-9083-e2095e2c4b26",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad45e0d-fd38-46ee-a592-a09748783a11",
   "metadata": {},
   "source": [
    "### 位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174ad28-94aa-4274-85c5-0d2928429d29",
   "metadata": {},
   "source": [
    "首先我们来进行位置编码、注意力机制、编码器、解码器等结构的构建。这些内容都与我们在之前课程中、复现transformer时的结构相一致，因此在这里我们就不进行详细的解读了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "373ba1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"进行位置编码.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ab315-2067-4226-8e32-154ccb30207d",
   "metadata": {},
   "source": [
    "### Embedding与掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c24e81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, src_vocab, tgt_vocab, dropout=0.1):\n",
    "        super(TranslationModel, self).__init__()\n",
    "\n",
    "        # 定义原句子的embedding\n",
    "        # embedding的维度被设置成了超参数\n",
    "        # 在本次案例中我们使用的是256\n",
    "        self.src_embedding = nn.Embedding(len(src_vocab), d_model, padding_idx=2)\n",
    "        # 定义目标句子的embedding\n",
    "        self.tgt_embedding = nn.Embedding(len(tgt_vocab), d_model, padding_idx=2)\n",
    "        # 定义posintional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_len=max_length)\n",
    "        # 定义Transformer\n",
    "        self.transformer = nn.Transformer(d_model, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # 定义最后的预测层，这里并没有定义Softmax，而是把他放在了模型外。\n",
    "        self.predictor = nn.Linear(d_model, len(tgt_vocab))\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        进行前向传递，输出为Decoder的输出。注意，这里并没有使用self.predictor进行预测，\n",
    "        因为训练和推理行为不太一样，所以放在了模型外面。\n",
    "        :param src: 原batch后的句子，例如[[0, 12, 34, .., 1, 2, 2, ...], ...]\n",
    "        :param tgt: 目标batch后的句子，例如[[0, 74, 56, .., 1, 2, 2, ...], ...]\n",
    "        :return: Transformer的输出，或者说是TransformerDecoder的输出。\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        生成tgt_mask，即阶梯型的mask，例如：\n",
    "        [[0., -inf, -inf, -inf, -inf],\n",
    "        [0., 0., -inf, -inf, -inf],\n",
    "        [0., 0., 0., -inf, -inf],\n",
    "        [0., 0., 0., 0., -inf],\n",
    "        [0., 0., 0., 0., 0.]]\n",
    "        tgt.size()[-1]为目标句子的长度。\n",
    "        \"\"\"\n",
    "        # 对目标句子，要掩盖住embedding矩阵的上半部分（代表从过去向未来询问的部分）\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size()[-1]).to(device)\n",
    "        # 但除此之外，我们还需要对所有填充的部分进行掩码，减少对模型的噪音干扰\n",
    "        # 掩盖住原句子中<pad>的部分，例如[[False,False,False,..., True,True,...], ...]\n",
    "        src_key_padding_mask = TranslationModel.get_key_padding_mask(src)\n",
    "        # 掩盖住目标句子中<pad>的部分\n",
    "        tgt_key_padding_mask = TranslationModel.get_key_padding_mask(tgt)\n",
    "\n",
    "        # 对src和tgt进行编码\n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        # 给src和tgt的token增加位置信息\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        # 经过这一步之后，数据的结构变成了三维\n",
    "        #(batch_size, sentence_len, embedding_dimension)\n",
    "\n",
    "        # 将准备好的数据送给nn.transformer\n",
    "        # 在这里我们是一并将源数据与目标数据给到了transformer模型\n",
    "        # 但transformer模型实际上是先试用src原始数据\n",
    "        # 再试用tgt目标数据的\n",
    "        # 同时两个数据的掩码也各不相同\n",
    "        out = self.transformer(src, tgt,\n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        \"\"\"\n",
    "        这里直接返回transformer的结果。因为训练和推理时的行为不一样，\n",
    "        所以在该模型外再进行线性层的预测。\n",
    "        \"\"\"\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(tokens):\n",
    "        \"\"\"\n",
    "        用于key_padding_mask\n",
    "        \"\"\"\n",
    "        return tokens == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da04d0-6684-4006-ae26-1ca26d77a93a",
   "metadata": {},
   "source": [
    "规定训练次数到达设定的次数时，就保存一次模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "151e88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint:\n",
    "    model = torch.load(model_dir / model_checkpoint)\n",
    "else:\n",
    "    model = TranslationModel(256, en_vocab, zh_vocab)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e21e3-dc06-46d0-89c0-665ccdf0be8c",
   "metadata": {},
   "source": [
    "检验一下模型能够运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4485a9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 71, 256])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src, tgt).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6a51833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 71, 256])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size([64, 71, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b1e395f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1086, -0.9489,  0.0671,  ...,  0.2900, -1.0769, -0.9015],\n",
       "         [ 0.7002,  0.2061, -0.3974,  ...,  0.7878, -0.9466, -0.8742],\n",
       "         [ 0.4416,  0.5567,  0.8869,  ...,  0.0506, -1.1705, -0.9455],\n",
       "         ...,\n",
       "         [ 0.5505,  0.4742,  0.7696,  ...,  0.2593, -0.0192, -0.9401],\n",
       "         [ 0.5870,  0.0738,  0.4658,  ...,  1.6159, -0.9348, -0.8466],\n",
       "         [ 0.5404,  0.3085,  0.8862,  ...,  1.2423, -0.3007, -0.4248]],\n",
       "\n",
       "        [[ 0.2161, -0.4012,  0.1673,  ...,  0.9661,  0.0261, -0.4047],\n",
       "         [-1.0799,  0.6539,  0.4570,  ...,  0.7826, -0.3653, -1.0629],\n",
       "         [ 0.6776, -1.7098,  0.1928,  ...,  1.0279, -0.2207, -0.3342],\n",
       "         ...,\n",
       "         [ 0.6773,  0.4844, -0.0996,  ...,  0.7487, -0.1460, -1.2397],\n",
       "         [ 1.0414, -1.3704, -0.5414,  ...,  0.5545, -0.5459,  0.0381],\n",
       "         [ 1.0267, -0.6749,  0.1335,  ...,  0.1397, -0.0418, -0.6256]],\n",
       "\n",
       "        [[ 0.8706,  0.0169, -0.6459,  ...,  1.9233,  0.4750, -0.3505],\n",
       "         [ 1.0289,  0.0775, -1.1608,  ...,  1.2717,  0.2613, -0.4597],\n",
       "         [ 1.7475, -0.3570, -0.5864,  ...,  1.0383,  0.7088, -0.8422],\n",
       "         ...,\n",
       "         [ 0.6325,  0.4709,  0.5958,  ...,  0.7954,  0.2128, -0.8401],\n",
       "         [ 0.9142,  0.5673,  0.9782,  ...,  0.1511, -0.1131, -1.0533],\n",
       "         [ 0.9694,  0.3791, -0.5197,  ...,  0.8706, -0.2191, -1.2361]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2720, -0.7666,  0.1536,  ..., -0.0145,  0.2754, -0.6387],\n",
       "         [ 0.5678,  0.4500, -0.0699,  ...,  1.1462,  0.5657, -0.6240],\n",
       "         [ 0.7934,  0.0249,  0.2748,  ...,  0.4361,  0.3150, -0.0210],\n",
       "         ...,\n",
       "         [ 0.0933,  0.4605,  0.5466,  ...,  0.8373,  0.7892, -0.7398],\n",
       "         [ 1.2976, -0.0630,  0.3042,  ...,  1.1575,  0.4116, -0.3710],\n",
       "         [ 1.4494, -0.6141,  0.6225,  ...,  1.0591,  0.7751, -0.5170]],\n",
       "\n",
       "        [[-0.2467,  0.1550,  0.0941,  ...,  0.6422, -0.3403, -1.4785],\n",
       "         [ 0.4353,  0.0613, -0.7353,  ...,  1.2844,  0.3305, -0.7852],\n",
       "         [ 0.3296,  1.7642,  0.4052,  ...,  0.5342, -0.1257, -0.7447],\n",
       "         ...,\n",
       "         [ 0.9273,  0.9930, -0.6464,  ...,  0.7007, -0.2699, -0.4881],\n",
       "         [ 0.7690,  0.3785,  0.4118,  ...,  0.8664,  0.0403, -0.9651],\n",
       "         [ 0.7299,  1.2450,  0.4256,  ..., -0.2308,  0.1771, -0.4723]],\n",
       "\n",
       "        [[ 0.0755, -0.3569,  0.2109,  ...,  0.0079, -0.0021,  0.1097],\n",
       "         [ 0.8029, -0.6030, -0.2336,  ...,  0.7850,  0.4108, -1.3566],\n",
       "         [ 0.7045, -0.5874, -0.2391,  ...,  0.4028, -0.8907, -0.8779],\n",
       "         ...,\n",
       "         [ 0.6908,  0.0547,  0.1762,  ...,  0.0487, -0.2660, -1.4126],\n",
       "         [ 1.1736,  0.1800,  0.2583,  ..., -0.3762,  0.4114, -1.8634],\n",
       "         [ 0.7969, -0.2912,  0.4809,  ..., -1.0315, -0.3380, -1.5632]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6307b2-a6d9-4a2a-86e7-4e57f1601d24",
   "metadata": {},
   "source": [
    "## 实际训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb3398-9fdd-4bfc-9f69-14241b58af3e",
   "metadata": {},
   "source": [
    "### 优化器与损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c13c65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61a4d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TranslationLoss, self).__init__()\n",
    "        # 使用KLDivLoss，不需要知道里面的具体细节。\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = 2\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        损失函数的前向传递\n",
    "        :param x: 将Decoder的输出再经过predictor线性层之后的输出。\n",
    "                  也就是Linear后、Softmax前的状态\n",
    "        :param target: tgt_y。也就是label，例如[[1, 34, 15, ...], ...]\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        由于KLDivLoss的input需要对softmax做log，所以使用log_softmax。\n",
    "        等价于：log(softmax(x))\n",
    "        \"\"\"\n",
    "        x = log_softmax(x, dim=-1)\n",
    "\n",
    "        \"\"\"\n",
    "        构造Label的分布，也就是将[[1, 34, 15, ...]] 转化为:\n",
    "        [[[0, 1, 0, ..., 0],\n",
    "          [0, ..., 1, ..,0],\n",
    "          ...]],\n",
    "        ...]\n",
    "        \"\"\"\n",
    "        # 首先按照x的Shape构造出一个全是0的Tensor\n",
    "        true_dist = torch.zeros(x.size()).to(device)\n",
    "        # 将对应index的部分填充为1\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), 1)\n",
    "        # 找出<pad>部分，对于<pad>标签，全部填充为0，没有1，避免其参与损失计算。\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "        # 计算损失\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78b5e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = TranslationLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c276ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs/transformer_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c82f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf38af-6782-4551-8fd5-1f75b23e0622",
   "metadata": {},
   "source": [
    "### 实际训练流程与代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc0bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1:   1%|▏         | 2313/156250 [3:34:59<243:49:26,  5.70s/it, loss=4.24]"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "if model_checkpoint:\n",
    "    step = int('model_10000.pt'.replace(\"model_\", \"\").replace(\".pt\", \"\"))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for index, data in enumerate(train_loader):\n",
    "        # 生成数据\n",
    "        src, tgt, tgt_y, n_tokens = data\n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 进行transformer的计算\n",
    "        out = model(src, tgt)\n",
    "        # 将结果送给最后的线性层进行预测\n",
    "        out = model.predictor(out)\n",
    "\n",
    "        \"\"\"\n",
    "        计算损失。由于训练时我们的是对所有的输出都进行预测，所以需要对out进行reshape一下。\n",
    "                我们的out的Shape为(batch_size, 词数, 词典大小)，view之后变为：\n",
    "                (batch_size*词数, 词典大小)。\n",
    "                而在这些预测结果中，我们只需要对非<pad>部分进行，所以需要进行正则化。也就是\n",
    "                除以n_tokens。\n",
    "        \"\"\"\n",
    "        loss = criteria(out.contiguous().view(-1, out.size(-1)), tgt_y.contiguous().view(-1)) / n_tokens\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        loop.update(1)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        del src\n",
    "        del tgt\n",
    "        del tgt_y\n",
    "\n",
    "        if step != 0 and step % save_after_step == 0:\n",
    "            torch.save(model, model_dir / f\"model_{step}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe93ef-e85d-44d8-aa0a-e9419a9a2ade",
   "metadata": {},
   "source": [
    "还记得吗？我们设置了epoch为1。运行一个epoch就需要3个半小时的时间，你可以尝试着在GPU上运行大约3~5个epoch看看能否有更好的效果↑ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb61f5a-bcd2-488d-9137-f9b2b90499bf",
   "metadata": {},
   "source": [
    "## 推理与测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb07dbb-8977-4d9d-bf73-e90167563e0b",
   "metadata": {},
   "source": [
    "我们直接使用从外部保存的模型，有我们训练好的5500模型，你也可以使用你自己训练好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e19f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_5000.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a17d28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcb07c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(src: str):\n",
    "    \"\"\"\n",
    "    :param src: 英文句子，例如 \"I like machine learning.\"\n",
    "    :return: 翻译后的句子，例如：”我喜欢机器学习“\n",
    "    \"\"\"\n",
    "\n",
    "    # 将与原句子分词后，通过词典转为index，然后增加<bos>和<eos>\n",
    "    src = torch.tensor([0] + en_vocab(en_tokenizer(src)) + [1]).unsqueeze(0).to(device)\n",
    "    # 首次tgt为<bos>\n",
    "    tgt = torch.tensor([[0]]).to(device)\n",
    "    # 一个一个词预测，直到预测为<eos>，或者达到句子最大长度\n",
    "    for i in range(max_length):\n",
    "        # 进行transformer计算\n",
    "        out = model(src, tgt)\n",
    "        # 预测结果，因为只需要看最后一个词，所以取`out[:, -1]`\n",
    "        predict = model.predictor(out[:, -1])\n",
    "        # 找出最大值的index\n",
    "        y = torch.argmax(predict, dim=1)\n",
    "        # 和之前的预测结果拼接到一起\n",
    "        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)\n",
    "        # 如果为<eos>，说明预测结束，跳出循环\n",
    "        if y == 1:\n",
    "            break\n",
    "    # 将预测tokens拼起来\n",
    "    tgt = ''.join(zh_vocab.lookup_tokens(tgt.squeeze().tolist())).replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1524ccb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'好，这项目结束了。让我们看看看这个是多好的。'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Alright, this project is finished. Let's see how good this is.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed981d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好吗？'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a9dcec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天是一个美好的日子！'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Today is a nice day!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96e9b648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我有很多朋友。'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I have many friends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16c00074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你有什么问题吗？'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Do you have any problems?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed87d1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我能说什么？我可以说什么？祝你今天愉你好一天！'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"What can I say? Wish you a nice day!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a203fe0-d4fa-40a8-a60c-d52a76250158",
   "metadata": {},
   "source": [
    "你也可以从数据集中分割出训练和测试集（例如分割出100w测试集），将测试集用于单独的测试来验证模型的效果。在机器翻译的过程中，数据准备是比模型训练更为关键的流程，当你在你的数据集上进行了完备的准备，你的训练流程也会更加流畅。你可以尝试更换数据集、在你自己的数据集上进行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac947f-cf36-4855-bdbb-5765f38ed9ec",
   "metadata": {},
   "source": [
    "# Transformer案例实战之 股价预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de64f0-8d32-40a4-97dd-4711747a13a8",
   "metadata": {},
   "source": [
    "股价预测是利用计算机程序通过分析历史和实时的市场数据来预测未来股票价格的技术，它是时序领域中最常见应用之一，也是因其巨大潜力而备受关注的领域。这个任务涉及到从历史股价数据中提取趋势，并将这些趋势准确地映射到未来的预测中，同时尽可能地捕捉市场动态和投资者情绪的变化。在使用深度学习技术进行股价预测的各种实践当中，Transformer被认为是最为强大的实践模型，Transofrmer通过训练大量的股市交易数据来学习价格波动的复杂模式、ta能够理解和预测股票价格的复杂波动，适应不同的市场条件，并处理非线性的市场因素。股价预测是金融技术领域中的一个重要而活跃的研究方向，广泛应用于投资决策、风险管理和财务规划等领域。\n",
    "\n",
    "对于Transformer模型来说，股价预测是一个典型的时间序列分析任务。这意味着模型需要接收一系列的输入（如历史股价和交易量的数据），并输出另一系列的数据（未来的股价预测）。Transformer中的自注意力机制允许模型在预测时能够关注到输入序列中的所有数据点，帮助模型理解更复杂的市场趋势和周期性波动，使得预测更加准确和具有前瞻性；同时，在股价预测任务中，Transformer中的编码器用于分析输入的市场数据，而解码器则用于生成未来价格的预测。每一层都进一步提炼和传递信息，增强了模型在处理复杂市场数据时的能力。\n",
    "\n",
    "在之前的课程中，我们使用LSTM完成过高度完整、考虑到方方面面的股价预测流程。**与其他Transformer的任务一样，对股价预测来说，最重要的流程是处理数据**。在高质量的数据上、架构则会决定当前预测的上限，因此我们将复用在LSTM流程中所使用的数据处理流程、除了架构之外的细节也都可以参考LSTM的股价预测案例本身。当然，当Transformer在执行股价预测时，会存在许多与LSTM不同的地方，包括对未来信息进行掩码、需要同时使用过去的信息和未来的信息等等。在本次案例中，我们将具体使用代码来实现这些不同之处。\n",
    "\n",
    "今天的项目中，我们将完成一个基于历史数据的股价预测任务。通常来说，一个股价预测任务需要覆盖至少如下的流程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4797c96-0676-4c22-8ff6-20afcdfda005",
   "metadata": {},
   "source": [
    "- 一、数据处理\n",
    "\n",
    "1. **加载数据**：股票数据的加载可以使用一个很复杂的工作，对于单量数据集来说、只要从CSV文件中读取股票的收盘价数据、并要求该数据按照时间顺序进行排列即可；但对于多变量数据集、甚至面板数据来说，在导入数据后则要确定数据是如何排列的、按照时间顺序排列、与按照股票顺序排列后再按时间顺序排列，其实大有不同。\n",
    "\n",
    "2. **股票数据的预处理**：在任意股票数据上，我们都可以使用下面的这些预处理手段——\n",
    "> `MinMaxScaler归一化`：使用`MinMaxScaler`归一化，使数据值在-1到1之间，以便于神经网络处理。在之前的课程中我们就提到过，归一化对股票数据而言是一个对模型结果提升巨大的预处理流程。即便不进行任何特征工程，只要能够对数据进行归一化，对数据预测也有很大的帮助。当然，归一化具体怎么做也会很大程度影响模型的表现，我们可以针对每个股票进行精确的、属于单支股票自己的归一化，也可以就对全部数据进行笼统的简单归一化，有时候简单归一化反而能够达到抗过拟合的效果，需要根据实际的需求进行判断。另外，是否需要对标签本身（也就是预测的股价本身）进行归一化也是需要讨论的命题。通常来说，为了保证预测损失的可解释性，我们不会对标签本身也进行归一化，但从神经网络的预测角度来看，我们可以对股票数据的标签进行归一化，如果你在预测的不是股价本身、而是某种趋势、某种比例，那你完全可以对股价本身也进行归一化来加速Transformer的运行。<br><br>\n",
    "> `调价因子`：在许多股票数据集中，都会有`Dividend`、市场监管、`AdjustmentFactor`等特征，这些特征对于股票价格有一定的影像，因此我们需要根据金融业务、依据这些特征对股票价格进行调整，等到调整后的价格。<br><br>\n",
    "> `数据分割`：数据被分为训练集和测试集。通常，训练集用于模型学习，而测试集用于评估模型的泛化能力。注意，对于股票数据来说，我们分割的是完全的时间序列数据，因此只能够按照“过去是训练集、未来是测试集”的方式进行分割。当然，如果我们使用的是多变量的数据、甚至是截面的数据，那我们就需要先将每支股票进行分割后、再在每支股票内部进行“过去是训练集、未来是测试集”的分割了。这种方式可以帮助我们更好地预测每一支股票。当然，在股票数量特别多的情况下，对每一支股票进行精准预测可能会造成模型过拟合、或者训练混乱，因此我们也可能将数据更粗糙地分割（例如，按照股票所在的sector进行分割、按照股票所在的行业进行分割等等）。<br><br>\n",
    "> 在进行预处理的时候，预处理的顺序有很大的学问。我们应该先对数据进行处理再进行数据集分割，还是先分割后再进行数据预处理？最标准的方式是后者，这可以避免测试集的信息泄露到训练集中。如果先做数据处理再进行分割，可能导致模型的表现虚高。\n",
    " \n",
    "3. **创建序列/滑窗**：时间序列数据往往是按照二维表方式进行排列的，但Transformer等系列NLP算法要求的数据输入格式为三维数据。对于文字数据，通常我们使用embedding的手段来将文字二维表转化为三维的embedding数据集，但对于时间序列数据，我们需要进行滑窗、手动创建三维数据集。在之前的LSTM课程中我们详细讲解了对单变量数据集、多变量数据集以及面板数据集进行滑窗的具体手段，同时我们提到了基于特征进行滑窗、包括标签进行滑窗等滑窗方法。按照预测的步数，还可以将预测分为多步预测、递归预测等实现手段。对Transformer而言，我们在进行时间序列预测时，可以采用和LSTM相类似的滑窗方式，但具体的来说——\n",
    "> 如果我们使用的是完整的**带有编码器和解码器双重结构的Transformer**，那我们可能不需要选择带标签的滑窗，因为解码器本身就要求真实标签作为输入的一部分，我们没有必要再特地将真实标签包括在编码器的输入数据中。如果我们选择了带标签的滑窗，我们可能需要对编码器的数据进行掩码。<br><br>\n",
    "> 如果我们使用的是**Encoder-only（只有编码器中有多头注意力机制）的Transformer**，那我们可以选择和LSTM一样的、带标签的滑窗方式来提升模型的效果。\n",
    "\n",
    "由于在LSTM的案例中我们已经非常详细地解读过各类时间序列数据、以及各类滑窗方式的详细讲解，因此在这里不再赘述。如果你对滑窗方式没有足够的了解，请回看LSTM课程的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce1dde-abcd-4f91-a6fa-2dd07695a8fc",
   "metadata": {},
   "source": [
    "- 二、模型定义\n",
    "\n",
    "在传统的Transformer架构中，编码器和解码器的概念是从机器翻译的角度来定义的，其中编码器处理输入序列，而解码器生成输出序列。对于股价预测这样的时间序列任务，应用Transformer可能需要一些调整，因为股价预测不完全符合典型的序列到序列模型（Seq2Seq）的框架。在这里我们可能需要讨论，**时间序列预测任务是更像分类/回归任务，还是更像生成式任务**？这个答案可能因任务而异，但对于股票场景来说，**股票预测不是一次性对未来进行超长时间段的预测，相反，股票预测是通过滑窗的方式，不断利用更靠近未来的历史信息去预测固定时间段外的未来的任务**，这是说，我们有两种预测类型：\n",
    "\n",
    "> 1. 利用过去120天的数据预测未来300天的任务\n",
    "\n",
    "> 2. 将过去120天数据划分为30天一个窗用1-30天的窗去预测31-37天，用2-31天的窗去预测32-38天，这样的任务\n",
    "\n",
    "很显然，文本生成任务时我们使用的是第一种预测类型，我们需要根据过去少量的信息预测大量的未来，通常预测序列会非常长。但在进行股票预测时，大多数时候我们需要预测短时间内变幻莫测的股票市场，因此股票预测场景下的多步预测往往时间区间不会很长，一般是5-7天，最长差不多是30天。相比起一次性要生成几百字的生成式算法、我们更倾向于认为股票预测是回归任务。\n",
    "\n",
    "当我们把股票预测当做时回归任务来看待时，我们可以使用“encoder-only”的结构，即只使用encoder来进行预测，在decoder的部分我们使用线性层来帮助我们进行输出。如此，decoder部分复杂的掩码工作、teacher forcing工作等等都无需出现在股票预测的场景中。当然，如果你非常想要使用完整的encoder-decoder结构的transformer，你也可以参考机器翻译案例中的代码来进行修改。\n",
    "\n",
    "1. **位置编码（`PositionalEncoding`）**：虽然Transformer对数据是一个、一个进行预测，但并不代表训练的时候这些数据是一个个进入网络的；和LSTM、RNN等线性的架构不同，Transformer的会将时间序列上所有的点混合在一起打包成QKV矩阵，并同时计算每一个时间串所对应的未来，因此进入Transformer的时间序列数据也需要被添加位置信息，来帮助Transformer认知到原始数据的顺序。幸运的是，这里进行位置编码的方式和文字数据是一模一样的，我们就不再进行赘述。\n",
    "\n",
    "2. **Transformer模型（`TransAm`）**：在这里我们使用了encoder-only的Transformer，包括位置编码层、多层Transformer编码器和一个使用线性层填充的decoder作为输出层。这里的decoder因为没有包含多头注意力机制，因此不能够算是使用了经典的encoder架构。本质我们使用的还是encoder-only。\n",
    "\n",
    "需要注意的是，在本次的例子中我们进行的是单步预测（也就是每次预测只输出未来的1个样本的预测），这是为了让Transformer代码本身看起来不要变得更加复杂。如果你希望进行多步预测，则需要修改参数pred_len，同时别忘记在训练结束后、要对训练出的结果进行去重、才能获得最终的预测结果。当然，如果你使用的是encoder-decoder双重架构，那你可能滑窗方式与现在课程中所设置的有所区别（例如，你没有采用逐步输出的方式，而是直接把超长序列放入encoder，然后输出未来的超长序列），这种情况下你可能无需设置pred_len，而是采用与机器翻译更相似的方法来进行预测。如果你使用encoder-decoder结构，别忘记对序列中未来的信息进行掩码哦。\n",
    "\n",
    "5. **权重初始化**：选择合适的初始化方法对于模型的学习效率和最终性能非常关键。通过初始化偏置为 0 和权重为一个较小的均匀分布，可以帮助避免训练初期的梯度消失或爆炸问题，使得模型训练过程更加稳定。这种初始化策略尤其在处理复杂的模型如深层神经网络时非常重要。\n",
    "\n",
    "在这里，我们只对decoder模型使用了均匀分布初始化，因为编码器部分我们使用了PyTorch中的TransformerEncoderLayer测过，这个层默认已经包括了权重初始化，通常是Glorot（也称为Xavier）初始化或其他类型，因此无需再进行初始化了。同样的，如果你使用的是完整的encoder-decoder结构，那你可能也无需再进行初始化了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce7ff9-533b-4298-8542-e365a7883996",
   "metadata": {},
   "source": [
    "- 三、训练过程\n",
    "\n",
    "股票数据的训练过程也相对单纯，这一次我们使用了batch循环和epoch循环分割的写法，我们将batch循环打包在了一个函数当中，将epoch循环暴露在外，这样分开的目的是为了让训练代码更加简洁、也是为大家提供了一套全新的神经网络训练代码。\n",
    "\n",
    "1. **定义损失函数和优化器**：使用均方误差损失函数（MSE）和AdamW优化器。\n",
    "2. **学习率调整**：使用学习率调度器来调整学习率。\n",
    "3. **训练循环**：在每个epoch中，模型在训练数据上进行迭代，计算损失，进行反向传播和参数更新。\n",
    "\n",
    "- 四、评估和可视化\n",
    "1. **评估函数**：定义了一个评估函数来计算模型在测试集上的总损失。\n",
    "2. **可视化结果**：如果你是对单支股票进行预测、那在特定的epoch后，可以使用`plot_and_loss`函数绘制模型预测的结果和实际值，以可视化模型的性能。在我们的例子中，我们是对抽选的多支股票进行预测，因此就没有进行绘图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd55e2-b1e5-4353-9644-73d27868cd42",
   "metadata": {},
   "source": [
    "接下来就让我们一步步来完成这个案例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5be38-d988-4e96-8505-27f07084f4b2",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0cb9ece1-b308-41d1-a3f8-756510205abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from decimal import ROUND_HALF_UP, Decimal\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import pad, log_softmax\n",
    "\n",
    "# typing 模块提供了一些类型，用于类型提示\n",
    "from typing import Union,List,Tuple,Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5a487-644f-4e60-afdd-dfa6eae6c541",
   "metadata": {},
   "source": [
    "### 环境设置与数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6fd6df63-e147-4a6d-89c8-711a629e7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工作目录、为Transformer放数据、缓存文件的目录\n",
    "work_dir = Path(r\"D:\\pythonwork\\2024DL\")\n",
    "# 训练好的模型会放在该目录下，注意隔一段时间就要对模型进行保存，这是深度学习训练的基本\n",
    "model_dir = Path(r\"D:\\pythonwork\\2024DL\\model\")\n",
    "# 上次运行到的地方，如果是第一次运行，为None，如果中途暂停了，下次运行时，指定目前最新的模型即可。\n",
    "model_checkpoint = None # 'model_10000.pt'\n",
    "\n",
    "# 如果工作目录不存在，则创建一个\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "\n",
    "# 如果模型目录不存在，则创建一个\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# 股价数据本身的导入路径\n",
    "stock_price_filepath = './stock_prices.csv'\n",
    "# 股价列表的导入路径\n",
    "stock_list_filepath = './stock_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d50627db-70fa-4ac9-a706-df5e8b127b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据导入\n",
    "stock= pd.read_csv(stock_price_filepath)\n",
    "stock_list = pd.read_csv(stock_list_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5c9782f0-6437-45f4-a105-70fc5ee5aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看数据的具体情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "91686537-a58b-4cf7-89a8-a12e7d179716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjustmentFactor</th>\n",
       "      <th>ExpectedDividend</th>\n",
       "      <th>SupervisionFlag</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170104_1301</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1301</td>\n",
       "      <td>2734.0</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>2742.0</td>\n",
       "      <td>31400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170104_1332</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1332</td>\n",
       "      <td>568.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2798500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.012324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170104_1333</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1333</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>270800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170104_1376</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1376</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>11300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.011053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170104_1377</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1377</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>150800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RowId        Date  SecuritiesCode    Open    High     Low   Close  \\\n",
       "0  20170104_1301  2017-01-04            1301  2734.0  2755.0  2730.0  2742.0   \n",
       "1  20170104_1332  2017-01-04            1332   568.0   576.0   563.0   571.0   \n",
       "2  20170104_1333  2017-01-04            1333  3150.0  3210.0  3140.0  3210.0   \n",
       "3  20170104_1376  2017-01-04            1376  1510.0  1550.0  1510.0  1550.0   \n",
       "4  20170104_1377  2017-01-04            1377  3270.0  3350.0  3270.0  3330.0   \n",
       "\n",
       "    Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag    Target  \n",
       "0    31400               1.0               NaN            False  0.000730  \n",
       "1  2798500               1.0               NaN            False  0.012324  \n",
       "2   270800               1.0               NaN            False  0.006154  \n",
       "3    11300               1.0               NaN            False  0.011053  \n",
       "4   150800               1.0               NaN            False  0.003026  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8cde8f48-a539-44df-9c80-8b087aa11f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>EffectiveDate</th>\n",
       "      <th>Name</th>\n",
       "      <th>Section/Products</th>\n",
       "      <th>NewMarketSegment</th>\n",
       "      <th>33SectorCode</th>\n",
       "      <th>33SectorName</th>\n",
       "      <th>17SectorCode</th>\n",
       "      <th>17SectorName</th>\n",
       "      <th>NewIndexSeriesSizeCode</th>\n",
       "      <th>NewIndexSeriesSize</th>\n",
       "      <th>TradeDate</th>\n",
       "      <th>Close</th>\n",
       "      <th>IssuedShares</th>\n",
       "      <th>MarketCapitalization</th>\n",
       "      <th>Universe0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1301</td>\n",
       "      <td>20211230</td>\n",
       "      <td>KYOKUYO CO.,LTD.</td>\n",
       "      <td>First Section (Domestic)</td>\n",
       "      <td>Prime Market</td>\n",
       "      <td>50</td>\n",
       "      <td>Fishery, Agriculture and Forestry</td>\n",
       "      <td>1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>7</td>\n",
       "      <td>TOPIX Small 2</td>\n",
       "      <td>20211230.0</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>1.092828e+07</td>\n",
       "      <td>3.365911e+10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1305</td>\n",
       "      <td>20211230</td>\n",
       "      <td>Daiwa ETF-TOPIX</td>\n",
       "      <td>ETFs/ ETNs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>20211230.0</td>\n",
       "      <td>2097.0</td>\n",
       "      <td>3.634636e+09</td>\n",
       "      <td>7.621831e+12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1306</td>\n",
       "      <td>20211230</td>\n",
       "      <td>NEXT FUNDS TOPIX Exchange Traded Fund</td>\n",
       "      <td>ETFs/ ETNs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>20211230.0</td>\n",
       "      <td>2073.5</td>\n",
       "      <td>7.917718e+09</td>\n",
       "      <td>1.641739e+13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1308</td>\n",
       "      <td>20211230</td>\n",
       "      <td>Nikko Exchange Traded Index Fund TOPIX</td>\n",
       "      <td>ETFs/ ETNs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>20211230.0</td>\n",
       "      <td>2053.0</td>\n",
       "      <td>3.736943e+09</td>\n",
       "      <td>7.671945e+12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1309</td>\n",
       "      <td>20211230</td>\n",
       "      <td>NEXT FUNDS ChinaAMC SSE50 Index Exchange Trade...</td>\n",
       "      <td>ETFs/ ETNs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>20211230.0</td>\n",
       "      <td>44280.0</td>\n",
       "      <td>7.263200e+04</td>\n",
       "      <td>3.216145e+09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SecuritiesCode  EffectiveDate  \\\n",
       "0            1301       20211230   \n",
       "1            1305       20211230   \n",
       "2            1306       20211230   \n",
       "3            1308       20211230   \n",
       "4            1309       20211230   \n",
       "\n",
       "                                                Name  \\\n",
       "0                                   KYOKUYO CO.,LTD.   \n",
       "1                                    Daiwa ETF-TOPIX   \n",
       "2              NEXT FUNDS TOPIX Exchange Traded Fund   \n",
       "3             Nikko Exchange Traded Index Fund TOPIX   \n",
       "4  NEXT FUNDS ChinaAMC SSE50 Index Exchange Trade...   \n",
       "\n",
       "           Section/Products NewMarketSegment 33SectorCode  \\\n",
       "0  First Section (Domestic)     Prime Market           50   \n",
       "1                ETFs/ ETNs              NaN            -   \n",
       "2                ETFs/ ETNs              NaN            -   \n",
       "3                ETFs/ ETNs              NaN            -   \n",
       "4                ETFs/ ETNs              NaN            -   \n",
       "\n",
       "                        33SectorName 17SectorCode 17SectorName  \\\n",
       "0  Fishery, Agriculture and Forestry            1       FOODS    \n",
       "1                                  -            -            -   \n",
       "2                                  -            -            -   \n",
       "3                                  -            -            -   \n",
       "4                                  -            -            -   \n",
       "\n",
       "  NewIndexSeriesSizeCode NewIndexSeriesSize   TradeDate    Close  \\\n",
       "0                      7      TOPIX Small 2  20211230.0   3080.0   \n",
       "1                      -                  -  20211230.0   2097.0   \n",
       "2                      -                  -  20211230.0   2073.5   \n",
       "3                      -                  -  20211230.0   2053.0   \n",
       "4                      -                  -  20211230.0  44280.0   \n",
       "\n",
       "   IssuedShares  MarketCapitalization  Universe0  \n",
       "0  1.092828e+07          3.365911e+10       True  \n",
       "1  3.634636e+09          7.621831e+12      False  \n",
       "2  7.917718e+09          1.641739e+13      False  \n",
       "3  3.736943e+09          7.671945e+12      False  \n",
       "4  7.263200e+04          3.216145e+09      False  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "cc4e1c4d-575e-4449-adc0-64093af69b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了效率我们抽取其中的10支股票\n",
    "#你可以更换随机数种子，当你更换后，你选出的5支股票可能与我不一样\n",
    "\n",
    "# 从 SecuritiesCode 中随机选择5个不同的股票代码\n",
    "selected_codes = stock['SecuritiesCode'].drop_duplicates().sample(n=5,random_state=1412)\n",
    "\n",
    "# 根据选中的股票代码筛选出所有对应的数据行\n",
    "stock = stock[stock['SecuritiesCode'].isin(selected_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "63717446-a384-442f-b0d1-0fa43301cd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1429, 1884, 6902, 7282, 8360], dtype=int64)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock[\"SecuritiesCode\"].unique() #1429, 6902, 1884, 8360, 7282 确保和之前取出的一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3423d9ec-46cd-4713-b0cf-28f19e8c53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock.sort_values(\"SecuritiesCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "940a54e6-9a3f-40ae-8e4f-4fa7b4925835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#恢复索引\n",
    "stock.index = range(stock.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d0bfc709-f2ed-4f2e-97d8-add50c88d9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjustmentFactor</th>\n",
       "      <th>ExpectedDividend</th>\n",
       "      <th>SupervisionFlag</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170104_1429</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1429</td>\n",
       "      <td>418.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>24900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210729_1429</td>\n",
       "      <td>2021-07-29</td>\n",
       "      <td>1429</td>\n",
       "      <td>534.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>22600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.016729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20181004_1429</td>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>1429</td>\n",
       "      <td>402.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>53900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.012853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200630_1429</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>1429</td>\n",
       "      <td>600.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20171106_1429</td>\n",
       "      <td>2017-11-06</td>\n",
       "      <td>1429</td>\n",
       "      <td>509.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>31100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RowId        Date  SecuritiesCode   Open   High    Low  Close  \\\n",
       "0  20170104_1429  2017-01-04            1429  418.0  420.0  418.0  418.0   \n",
       "1  20210729_1429  2021-07-29            1429  534.0  538.0  533.0  538.0   \n",
       "2  20181004_1429  2018-10-04            1429  402.0  402.0  391.0  398.0   \n",
       "3  20200630_1429  2020-06-30            1429  600.0  603.0  587.0  591.0   \n",
       "4  20171106_1429  2017-11-06            1429  509.0  512.0  503.0  504.0   \n",
       "\n",
       "   Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag    Target  \n",
       "0   24900               1.0               NaN            False  0.007160  \n",
       "1   22600               1.0               NaN            False  0.016729  \n",
       "2   53900               1.0               NaN            False -0.012853  \n",
       "3   60000               1.0               NaN            False  0.013514  \n",
       "4   31100               1.0               NaN            False  0.013972  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130a06d-1b6b-4c9d-b0f2-e7269ad2782d",
   "metadata": {},
   "source": [
    "### 股票数据的数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d2c3e108-a33c-4fea-9aa9-c9f11b31dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#预处理函数\n",
    "def preprocess_stock_data(stock_data):\n",
    "    \"\"\"\n",
    "    对股票数据进行预处理，包括重命名列、重新排序列、填补缺失值、删除具有大量缺失值的行以及重置索引。\n",
    "    \n",
    "    :param stock_data: DataFrame类型，需要预处理的股票数据。\n",
    "    :return: 预处理后的股票数据。\n",
    "    \"\"\"\n",
    "    # 创建数据的副本以避免修改原始数据\n",
    "    processed_data = stock_data.copy()\n",
    "    \n",
    "    # 将Target列重命名为Sharpe Ratio\n",
    "    processed_data.rename(columns={'Target': 'Sharpe Ratio'}, inplace=True)\n",
    "    \n",
    "    # 填补ExpectedDividend列的缺失值\n",
    "    processed_data[\"ExpectedDividend\"] = processed_data[\"ExpectedDividend\"].fillna(0)\n",
    "    \n",
    "    # 删除具有大量缺失值的行\n",
    "    processed_data.dropna(inplace=True)\n",
    "    \n",
    "    # 重置索引\n",
    "    processed_data.index = range(processed_data.shape[0])\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1edc725b-fe3e-4e25-9c27-b4d601cd60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分割函数\n",
    "def split_data_by_column(data, column_name, train_ratio=0.67):\n",
    "    \"\"\"\n",
    "    按照指定列的值分割数据集为训练集和测试集。\n",
    "    \n",
    "    :param data: DataFrame类型，需要分割的数据集。\n",
    "    :param column_name: str类型，用作分割依据的列名。\n",
    "    :param train_ratio: float类型，训练集所占比例，默认值为0.67。\n",
    "    :return: 分割后的训练集和测试集。\n",
    "    \"\"\"\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    for value in data[column_name].unique():\n",
    "        subset = data[data[column_name] == value]\n",
    "        train_size = int(len(subset) * train_ratio)\n",
    "        train = pd.concat([train, subset[:train_size]])\n",
    "        test = pd.concat([test, subset[train_size:]])\n",
    "    \n",
    "    #由于我们是按照指定列将train和test分开\n",
    "    #因此现在train和test的索引被切断了\n",
    "    #需要为train、test恢复索引\n",
    "    \n",
    "    train.index = range(train.shape[0])\n",
    "    test.index = range(test.shape[0])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d4d009f3-a65b-49c8-a37c-0269bac3e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#精密的归一化处理\n",
    "def normalize_data_bycode(train, test, columns_to_normalize):\n",
    "    \"\"\"\n",
    "    对指定列的数据进行归一化处理。\n",
    "    \n",
    "    :param train_data: DataFrame类型，训练数据集。\n",
    "    :param test_data: DataFrame类型，测试数据集。\n",
    "    :param columns_to_normalize: list类型，需要归一化的列名列表。\n",
    "    :return: 归一化后的训练数据集和测试数据集。\n",
    "    \"\"\"\n",
    "    \n",
    "    for sec_code in train['SecuritiesCode'].unique():\n",
    "        \n",
    "        train.loc[train['SecuritiesCode'] == sec_code, columns_to_normalize] = train.loc[train['SecuritiesCode'] == sec_code, columns_to_normalize].astype(float)\n",
    "        test.loc[test['SecuritiesCode'] == sec_code, columns_to_normalize]  = test.loc[test['SecuritiesCode'] == sec_code, columns_to_normalize].astype(float)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        # 训练集归一化\n",
    "        scaler.fit(train.loc[train['SecuritiesCode'] == sec_code, columns_to_normalize])\n",
    "        train.loc[train['SecuritiesCode'] == sec_code, columns_to_normalize] = scaler.transform(train.loc[train['SecuritiesCode'] == sec_code, columns_to_normalize])\n",
    "        \n",
    "        # 测试集归一化，注意这里使用与训练集相同的scaler\n",
    "        if sec_code in test['SecuritiesCode'].values:  # 确保测试集中存在该股票代码\n",
    "            test.loc[test['SecuritiesCode'] == sec_code, columns_to_normalize] = scaler.transform(test.loc[test['SecuritiesCode'] == sec_code, columns_to_normalize])\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "#粗糙的归一化处理\n",
    "def normalize_data_byall(train, test, columns_to_normalize):\n",
    "    scaler = MinMaxScaler()\n",
    "        \n",
    "    # 训练集归一化\n",
    "    scaler.fit(train.loc[:,columns_to_normalize])\n",
    "    train.loc[:, columns_to_normalize] = scaler.transform(train.loc[:, columns_to_normalize])\n",
    "        \n",
    "    # 测试集归一化，注意这里使用与训练集相同的scaler\n",
    "    test.loc[:, columns_to_normalize] = scaler.transform(test.loc[:, columns_to_normalize])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0ac797e0-013c-436f-bf2c-4feef021d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义基于AdjustmentFactor的调价函数\n",
    "def adjust_price(price):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "    price (pd.DataFrame) : 包含股票价格信息的 pd.DataFrame。\n",
    "\n",
    "    返回：\n",
    "    price DataFrame (pd.DataFrame): 含有新生成的 AdjustedClose 的股票价格DataFrame\n",
    "\n",
    "    函数功能：\n",
    "    该函数将输入的原始股价数据进行处理，生成带有调整后收盘价(AdjustedClose)的新DataFrame。\n",
    "    AdjustedClose的计算方式为原收盘价与调整因子(AdjustmentFactor)的累计乘积，结果保留一位小数。\n",
    "    若计算后的AdjustedClose为0，则替换为空值(np.nan)，并对此列数据进行向前填充，以保持数据完整性。\n",
    "    \"\"\"\n",
    "    # 将 Date 列转换为 datetime 格式\n",
    "    # 因为我们在前面查看数据类型的时候发现Date为object类型\n",
    "    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    def generate_adjusted_close(df):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            df (pd.DataFrame)  : 单个 SecuritiesCode 的股票价格\n",
    "        返回:\n",
    "            df (pd.DataFrame): 单个 SecuritiesCode 的带有 AdjustedClose 的股票价格\n",
    "        \"\"\"\n",
    "        # 排序数据以生成 CumulativeAdjustmentFactor\n",
    "        df = df.sort_values(\"Date\", ascending=False)\n",
    "        # 生成 CumulativeAdjustmentFactor\n",
    "        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()#累乘\n",
    "        # 生成 AdjustedClose\n",
    "        df.loc[:, \"AdjustedClose\"] = (\n",
    "            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n",
    "        ).map(lambda x: float(\n",
    "            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n",
    "        ))\n",
    "        # 反转顺序\n",
    "        df = df.sort_values(\"Date\")\n",
    "        # 填充 AdjustedClose，将 0 替换为 np.nan\n",
    "        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n",
    "        # 向前填充 AdjustedClose\n",
    "        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n",
    "        return df\n",
    "    \n",
    "    # 生成 AdjustedClose\n",
    "    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n",
    "    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "91818b5e-2131-4950-bbf7-3057b7829fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#完成除了移动标签列和归一化之外的全部预处理\n",
    "\n",
    "#数据预处理\n",
    "data_ = preprocess_stock_data(stock)\n",
    "# 行业可能会对股票有影响，所以将行业也加入特征进行建模\n",
    "# 将17SectorName进行labelencoder\n",
    "stock_list['17SectorName'] = LabelEncoder().fit_transform(stock_list['17SectorName'])\n",
    "# 并入主表\n",
    "data_ = data_.merge(stock_list[['SecuritiesCode','17SectorName']],on='SecuritiesCode')\n",
    "# 调整股价\n",
    "data_ = adjust_price(data_)\n",
    "#将close列移动到最后一列\n",
    "close_col = data_.pop('Close')\n",
    "data_.loc[:,'Close'] = close_col\n",
    "#将所有除了时间、id之外的数据转换成浮点数\n",
    "for column in data_.columns[3:]:\n",
    "    data_[column] = data_[column].astype(float)\n",
    "#分割数据\n",
    "train, test = split_data_by_column(data_,\"SecuritiesCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "bee520a2-d383-43ee-ab7f-44f43016946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#粗糙的数据归一化\n",
    "unnormalized_close_train = train.loc[:,\"Close\"].copy()\n",
    "unnormalized_close_test = test.loc[:,\"Close\"].copy()\n",
    "\n",
    "train, test = normalize_data_byall(train,test\n",
    "                            ,columns_to_normalize = train.columns[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "510f7b8f-33ce-4f3a-bbb8-66d1fa233d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjustmentFactor</th>\n",
       "      <th>ExpectedDividend</th>\n",
       "      <th>SupervisionFlag</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "      <th>17SectorName</th>\n",
       "      <th>CumulativeAdjustmentFactor</th>\n",
       "      <th>AdjustedClose</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170104_1429</td>\n",
       "      <td>2017-01-04 00:00:00</td>\n",
       "      <td>1429</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012586</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392311</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.012652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170105_1429</td>\n",
       "      <td>2017-01-05 00:00:00</td>\n",
       "      <td>1429</td>\n",
       "      <td>0.013149</td>\n",
       "      <td>0.012586</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012783</td>\n",
       "      <td>0.012783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170106_1429</td>\n",
       "      <td>2017-01-06 00:00:00</td>\n",
       "      <td>1429</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>0.012845</td>\n",
       "      <td>0.013375</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013179</td>\n",
       "      <td>0.013179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170110_1429</td>\n",
       "      <td>2017-01-10 00:00:00</td>\n",
       "      <td>1429</td>\n",
       "      <td>0.013280</td>\n",
       "      <td>0.013105</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012783</td>\n",
       "      <td>0.012783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170111_1429</td>\n",
       "      <td>2017-01-11 00:00:00</td>\n",
       "      <td>1429</td>\n",
       "      <td>0.013019</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.013105</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>0.012915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RowId                 Date  SecuritiesCode      Open      High  \\\n",
       "0  20170104_1429  2017-01-04 00:00:00            1429  0.012889  0.012586   \n",
       "1  20170105_1429  2017-01-05 00:00:00            1429  0.013149  0.012586   \n",
       "2  20170106_1429  2017-01-06 00:00:00            1429  0.012759  0.012845   \n",
       "3  20170110_1429  2017-01-10 00:00:00            1429  0.013280  0.013105   \n",
       "4  20170111_1429  2017-01-11 00:00:00            1429  0.013019  0.012716   \n",
       "\n",
       "        Low    Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag  \\\n",
       "0  0.013510  0.003915               0.0               0.0              0.0   \n",
       "1  0.013240  0.005083               0.0               0.0              0.0   \n",
       "2  0.013375  0.006731               0.0               0.0              0.0   \n",
       "3  0.013510  0.011693               0.0               0.0              0.0   \n",
       "4  0.013105  0.005804               0.0               0.0              0.0   \n",
       "\n",
       "   Sharpe Ratio  17SectorName  CumulativeAdjustmentFactor  AdjustedClose  \\\n",
       "0      0.392311           1.0                         0.0       0.012652   \n",
       "1      0.343628           1.0                         0.0       0.012783   \n",
       "2      0.376025           1.0                         0.0       0.013179   \n",
       "3      0.327266           1.0                         0.0       0.012783   \n",
       "4      0.384325           1.0                         0.0       0.012915   \n",
       "\n",
       "      Close  \n",
       "0  0.012652  \n",
       "1  0.012783  \n",
       "2  0.013179  \n",
       "3  0.012783  \n",
       "4  0.012915  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() #确认是按股票顺序排列、且每支股票内部是时间顺序，才能开始滑窗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "97c5ec66-42c0-4231-bcdc-2a386f29dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意我们不要归一化后的target\n",
    "# 将保存的Close列放回DataFrame中\n",
    "\n",
    "close_col = train.pop(\"Close\")\n",
    "train.loc[:,\"Close\"] = unnormalized_close_train\n",
    "\n",
    "close_col = test.pop(\"Close\")\n",
    "test.loc[:,\"Close\"] = unnormalized_close_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467fd47-ed16-4185-8074-883e4320def2",
   "metadata": {},
   "source": [
    "### 关注截面的、基于特征+标签的滑窗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "020311e8-a06f-4686-8ffd-6875c7437ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#关注截面的滑窗\n",
    "\n",
    "def create_multivariate_dataset_4(dataset, window_size, pred_len):\n",
    "    \"\"\"\n",
    "    将多变量时间序列转变为能够用于训练和预测的数据，确保每个窗口内的Securities Code唯一\n",
    "    \n",
    "    参数:\n",
    "        dataset: DataFrame，其中包含特征和标签，特征从索引3开始，最后一列是标签\n",
    "        window_size: 滑窗的窗口大小\n",
    "    \"\"\"\n",
    "    X, y, y_indices = [], [], []\n",
    "    for i in range(len(dataset) - window_size - pred_len + 1):\n",
    "        # 检查窗口内的Securities Code是否唯一\n",
    "        securities_code = dataset.iloc[i:i + window_size, 2]\n",
    "        if len(securities_code.unique()) == 1:  # 如果Securities Code在窗口内唯一\n",
    "            # 选取从第4列到最后一列的特征和标签\n",
    "            feature_and_label = dataset.iloc[i:i + window_size, 3:].copy().values\n",
    "            # 长度pred_len的标签作为目标\n",
    "            target = dataset.iloc[(i + window_size):(i + window_size + pred_len), -1]\n",
    "            \n",
    "            # 记录本窗口中要预测的标签的时间点\n",
    "            target_indices = list(range(i + window_size, i + window_size + pred_len))\n",
    "\n",
    "            X.append(feature_and_label)\n",
    "            y.append(target)\n",
    "            #将每个标签的索引添加到y_indices列表中\n",
    "            y_indices.extend(target_indices)\n",
    "            \n",
    "    X = torch.FloatTensor(np.array(X, dtype=np.float32))\n",
    "    y = torch.FloatTensor(np.array(y, dtype=np.float32))            \n",
    "\n",
    "    return X, y, y_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "310dfb85-4489-47b0-9017-3d0274d6980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置参数\n",
    "input_size = train.shape[1]-3  #输入特征的维度\n",
    "n_head = 6 #多头注意力机制的头数\n",
    "epochs = 1000 #迭代epoch\n",
    "learning_rate = 0.01 #学习率\n",
    "window_size = 30 #窗口大小\n",
    "pred_len = 1 #多步预测的步数，对于Transformer而言直接执行单步预测也没有问题\n",
    "num_layers = 1 #编码器/解码器的层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ba4bfe95-1efd-4427-8bf7-e31b6875ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据进行滑窗\n",
    "X_train_final, y_train_final, y_train_indices = create_multivariate_dataset_4(train, window_size, pred_len)\n",
    "X_test_final, y_test_final, y_test_indices = create_multivariate_dataset_4(test, window_size, pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3fe182fc-5e2c-4cd9-bf20-c2f0df50193b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3874, 30, 12])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7f84b20c-e67f-43ab-ab72-a1ed17061d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2889e-02, 1.2586e-02, 1.3510e-02,  ..., 0.0000e+00,\n",
       "          1.2652e-02, 4.1800e+02],\n",
       "         [1.3149e-02, 1.2586e-02, 1.3240e-02,  ..., 0.0000e+00,\n",
       "          1.2783e-02, 4.1900e+02],\n",
       "         [1.2759e-02, 1.2845e-02, 1.3375e-02,  ..., 0.0000e+00,\n",
       "          1.3179e-02, 4.2200e+02],\n",
       "         ...,\n",
       "         [1.3019e-02, 1.2586e-02, 1.1889e-02,  ..., 0.0000e+00,\n",
       "          1.1597e-02, 4.1000e+02],\n",
       "         [1.2238e-02, 1.2326e-02, 1.2834e-02,  ..., 0.0000e+00,\n",
       "          1.2520e-02, 4.1700e+02],\n",
       "         [1.2368e-02, 1.2197e-02, 1.2294e-02,  ..., 0.0000e+00,\n",
       "          1.1729e-02, 4.1100e+02]],\n",
       "\n",
       "        [[1.3149e-02, 1.2586e-02, 1.3240e-02,  ..., 0.0000e+00,\n",
       "          1.2783e-02, 4.1900e+02],\n",
       "         [1.2759e-02, 1.2845e-02, 1.3375e-02,  ..., 0.0000e+00,\n",
       "          1.3179e-02, 4.2200e+02],\n",
       "         [1.3280e-02, 1.3105e-02, 1.3510e-02,  ..., 0.0000e+00,\n",
       "          1.2783e-02, 4.1900e+02],\n",
       "         ...,\n",
       "         [1.2238e-02, 1.2326e-02, 1.2834e-02,  ..., 0.0000e+00,\n",
       "          1.2520e-02, 4.1700e+02],\n",
       "         [1.2368e-02, 1.2197e-02, 1.2294e-02,  ..., 0.0000e+00,\n",
       "          1.1729e-02, 4.1100e+02],\n",
       "         [1.1978e-02, 1.1678e-02, 1.1889e-02,  ..., 0.0000e+00,\n",
       "          1.1070e-02, 4.0600e+02]],\n",
       "\n",
       "        [[1.2759e-02, 1.2845e-02, 1.3375e-02,  ..., 0.0000e+00,\n",
       "          1.3179e-02, 4.2200e+02],\n",
       "         [1.3280e-02, 1.3105e-02, 1.3510e-02,  ..., 0.0000e+00,\n",
       "          1.2783e-02, 4.1900e+02],\n",
       "         [1.3019e-02, 1.2716e-02, 1.3105e-02,  ..., 0.0000e+00,\n",
       "          1.2915e-02, 4.2000e+02],\n",
       "         ...,\n",
       "         [1.2368e-02, 1.2197e-02, 1.2294e-02,  ..., 0.0000e+00,\n",
       "          1.1729e-02, 4.1100e+02],\n",
       "         [1.1978e-02, 1.1678e-02, 1.1889e-02,  ..., 0.0000e+00,\n",
       "          1.1070e-02, 4.0600e+02],\n",
       "         [1.1717e-02, 1.1548e-02, 1.1889e-02,  ..., 0.0000e+00,\n",
       "          1.1861e-02, 4.1200e+02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[6.8220e-02, 6.7860e-02, 6.6469e-02,  ..., 0.0000e+00,\n",
       "          6.4971e-02, 8.1500e+02],\n",
       "         [6.2622e-02, 6.4098e-02, 6.4442e-02,  ..., 0.0000e+00,\n",
       "          6.3917e-02, 8.0700e+02],\n",
       "         [6.4575e-02, 6.3838e-02, 6.4712e-02,  ..., 0.0000e+00,\n",
       "          6.2994e-02, 8.0000e+02],\n",
       "         ...,\n",
       "         [5.1035e-02, 5.4496e-02, 5.1743e-02,  ..., 0.0000e+00,\n",
       "          5.5351e-02, 7.4200e+02],\n",
       "         [5.4680e-02, 5.4107e-02, 5.4175e-02,  ..., 0.0000e+00,\n",
       "          5.3242e-02, 7.2600e+02],\n",
       "         [5.2988e-02, 5.3198e-02, 5.3634e-02,  ..., 0.0000e+00,\n",
       "          5.3901e-02, 7.3100e+02]],\n",
       "\n",
       "        [[6.2622e-02, 6.4098e-02, 6.4442e-02,  ..., 0.0000e+00,\n",
       "          6.3917e-02, 8.0700e+02],\n",
       "         [6.4575e-02, 6.3838e-02, 6.4712e-02,  ..., 0.0000e+00,\n",
       "          6.2994e-02, 8.0000e+02],\n",
       "         [6.0669e-02, 5.9946e-02, 5.9714e-02,  ..., 0.0000e+00,\n",
       "          5.7855e-02, 7.6100e+02],\n",
       "         ...,\n",
       "         [5.4680e-02, 5.4107e-02, 5.4175e-02,  ..., 0.0000e+00,\n",
       "          5.3242e-02, 7.2600e+02],\n",
       "         [5.2988e-02, 5.3198e-02, 5.3634e-02,  ..., 0.0000e+00,\n",
       "          5.3901e-02, 7.3100e+02],\n",
       "         [5.3248e-02, 5.2550e-02, 5.0932e-02,  ..., 0.0000e+00,\n",
       "          4.9684e-02, 6.9900e+02]],\n",
       "\n",
       "        [[6.4575e-02, 6.3838e-02, 6.4712e-02,  ..., 0.0000e+00,\n",
       "          6.2994e-02, 8.0000e+02],\n",
       "         [6.0669e-02, 5.9946e-02, 5.9714e-02,  ..., 0.0000e+00,\n",
       "          5.7855e-02, 7.6100e+02],\n",
       "         [5.3639e-02, 5.3717e-02, 5.1067e-02,  ..., 0.0000e+00,\n",
       "          5.0738e-02, 7.0700e+02],\n",
       "         ...,\n",
       "         [5.2988e-02, 5.3198e-02, 5.3634e-02,  ..., 0.0000e+00,\n",
       "          5.3901e-02, 7.3100e+02],\n",
       "         [5.3248e-02, 5.2550e-02, 5.0932e-02,  ..., 0.0000e+00,\n",
       "          4.9684e-02, 6.9900e+02],\n",
       "         [4.8561e-02, 5.3717e-02, 5.0122e-02,  ..., 0.0000e+00,\n",
       "          5.4428e-02, 7.3500e+02]]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "51574d17-b13f-429e-a9c8-614cc3e81393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[406.],\n",
       "        [412.],\n",
       "        [414.],\n",
       "        ...,\n",
       "        [699.],\n",
       "        [735.],\n",
       "        [733.]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621bbe8-a782-4167-8052-c715a100eed9",
   "metadata": {},
   "source": [
    "## 模型构建与模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053edad7-419b-460a-9e36-f09aa7b4ab4b",
   "metadata": {},
   "source": [
    "### 位置编码函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df7edd-3867-4418-8292-fe6389c71c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed_value=1):\n",
    "    random.seed(seed_value)  # Python内置的随机库\n",
    "    np.random.seed(seed_value)  # Numpy库\n",
    "    torch.manual_seed(seed_value)  # 为CPU设置种子\n",
    "    torch.cuda.manual_seed(seed_value)  # 为当前GPU设置种子\n",
    "    torch.cuda.manual_seed_all(seed_value)  # 为所有GPU设置种子\n",
    "    torch.backends.cudnn.deterministic = True  # 确保每次返回的卷积算法将是确定的\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(2)\n",
    "\n",
    "# 定义了学习率调度\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n",
    "\n",
    "# 定义位置编码模块\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # 初始化函数，设置模型参数\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建位置编码张量，全零初始化\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 生成位置序列\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算分母项，用于调整正弦和余弦函数的频率\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 使用正弦函数生成位置编码的偶数部分\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 使用余弦函数生成位置编码的奇数部分\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 修改pe形状并固定维度顺序\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # 注册位置编码为模型的缓冲区\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    # 前向传播函数，将位置编码添加到输入张量上\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c494b-eb11-4fbb-ba54-ce9f90ccf5c8",
   "metadata": {},
   "source": [
    "### Encoder-Only的Transformer结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d95cd-4906-402f-9a6f-a3efcb1e1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Transformer模型\n",
    "class TransAm(nn.Module):\n",
    "    # 初始化函数，设置模型参数\n",
    "    def __init__(self, feature_size=250, pred_len=1, n_head = 1, num_layers=1, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None  # 源数据掩码，用于遮蔽未来信息\n",
    "        # 实例化位置编码模块，注意此时因为数据本来就是浮点数，所以无需再次embedding\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        # 创建Transformer编码层\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=n_head, dropout=dropout)\n",
    "        # 根据编码层构建Transformer编码器\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        # 初始化解码器，将Transformer输出转换为预测长度\n",
    "        self.decoder = nn.Linear(feature_size, pred_len)\n",
    "        # 初始化模型权重\n",
    "        self.init_weights()\n",
    "\n",
    "    # 初始化权重函数，用于设置解码器的初始权重\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, src):\n",
    "        # 检查并更新掩码\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "        # 应用位置编码\n",
    "        src = self.pos_encoder(src)\n",
    "        # 通过编码器处理数据\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        # 选择最后一个时间步的输出进行解码\n",
    "        output = output[:, -1, :]\n",
    "        # 通过解码器生成最终预测\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    # 生成后续掩码，用于遮蔽未来信息\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "caf5af36-8fc8-41a9-91fe-aebb84afe693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#设置GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#实例化模型、进行训练\n",
    "model = TransAm(input_size, pred_len, n_head, num_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate) #定义优化器\n",
    "criterion = nn.MSELoss() #定义损失函数\n",
    "train_loader = data.DataLoader(data.TensorDataset(X_train_final, y_train_final)\n",
    "                         #每个表单内部是保持时间顺序的即可，表单与表单之间可以shuffle\n",
    "                         , shuffle=True\n",
    "                         , batch_size=32\n",
    "                         , drop_last = True) #将数据分批次\n",
    "test_loader = data.DataLoader(data.TensorDataset(X_test_final, y_test_final)\n",
    "                         , shuffle=True\n",
    "                         , batch_size=32\n",
    "                         , drop_last = True) #将数据分批次\n",
    "\n",
    "# 初始化早停参数\n",
    "early_stopping_patience = 3  # 设置容忍的epoch数，即在这么多epoch后如果没有改进就停止\n",
    "early_stopping_counter = 0  # 用于跟踪没有改进的epoch数\n",
    "best_train_rmse = float('inf')  # 初始化最佳的训练RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec84e41-a6bc-4cc9-93e6-7c9e3643bf95",
   "metadata": {},
   "source": [
    "## 训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222651bd-87f1-4a88-ab64-45321ed49a1e",
   "metadata": {},
   "source": [
    "### train_batch函数与epoch分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "477f9a2e-3a75-4b3c-a5f4-113a12337c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(trainloader):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    \n",
    "    for X_batch, y_batch in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch.to(device))\n",
    "        loss = criterion(y_pred, y_batch.to(device))\n",
    "        loss.backward()\n",
    "        #梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(eval_model, testloader):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in testloader:\n",
    "            y_pred = eval_model(X_batch.to(device))\n",
    "            total_loss += len(data[0]) * criterion(y_pred, y_batch.to(device)).cpu().item()\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8d8523da-3f4f-4fb3-a7c8-424cbb3a24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source, epoch):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in testloader:\n",
    "            y_pred = eval_model(X_batch.to(device))\n",
    "            total_loss += len(data[0]) * criterion(y_pred, y_batch.to(device)).cpu().item()\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "    plt.plot(test_result, color=\"red\")\n",
    "    plt.plot(truth, color=\"blue\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('transformer-epoch%d.png' % epoch)\n",
    "    plt.close()\n",
    "\n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "36573201-6855-4d37-aae5-ec6b046e4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: train RMSE 463.2390, test RMSE 1203.4922\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  0.84s | valid loss 1203.49219\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: train RMSE 642.2194, test RMSE 1043.2932\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time:  0.90s | valid loss 1043.29321\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 150: train RMSE 1590.6501, test RMSE 1939.3271\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time:  0.85s | valid loss 1939.32715\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 200: train RMSE 1403.1254, test RMSE 1548.5898\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 200 | time:  0.84s | valid loss 1548.58984\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 250: train RMSE 1140.5841, test RMSE 1369.9457\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 250 | time:  0.83s | valid loss 1369.94568\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 300: train RMSE 919.8604, test RMSE 921.8638\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 300 | time:  0.82s | valid loss 921.86383\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 350: train RMSE 1872.7001, test RMSE 2083.7078\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 350 | time:  0.86s | valid loss 2083.70776\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 400: train RMSE 1488.4404, test RMSE 1033.9636\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 400 | time:  0.83s | valid loss 1033.96362\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 450: train RMSE 1838.0354, test RMSE 2337.1755\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 450 | time:  0.81s | valid loss 2337.17554\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 500: train RMSE 664.8637, test RMSE 1288.0487\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 500 | time:  0.84s | valid loss 1288.04871\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 550: train RMSE 1473.1107, test RMSE 1178.8754\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 550 | time:  0.83s | valid loss 1178.87537\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 600: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 600 | time:  0.82s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 650: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 650 | time:  0.85s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 700: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 700 | time:  0.82s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 750: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 750 | time:  0.86s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 800: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 800 | time:  0.84s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 850: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 850 | time:  0.84s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 900: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 900 | time:  0.82s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 950: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 950 | time:  0.82s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 1000: train RMSE nan, test RMSE nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 1000 | time:  0.83s | valid loss   nan\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_batch(train_loader)\n",
    "    #验证与打印\n",
    "    if epoch % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_train_final.to(device)).cpu()\n",
    "            train_rmse = np.sqrt(loss_fn(y_pred, y_train_final))\n",
    "            y_pred = model(X_test_final.to(device)).cpu()\n",
    "            test_rmse = np.sqrt(loss_fn(y_pred, y_test_final))\n",
    "        print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
    "\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f}'.format(epoch, (\n",
    "                time.time() - epoch_start_time), test_rmse))\n",
    "        print('-' * 89)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9baa7-001f-4bd1-9248-585a99602140",
   "metadata": {},
   "source": [
    "在进行大约550个迭代后，我们达到了1178.87537这个损失水准，在同样的数据集下，这是LSTM需要经过特征工程、以及迭代至少1400个epochs后才能达到的水平。同时，Transformer的运行效率也相当不错，进行50个epochs往往值需要1秒不到的时间。显然，Transformer的学习能力十分强悍。不过，在550个epochs后，出现了梯度消失的现象，导致损失变为了nan。在这次的代码中，我们没有设置提前停止，因此迭代一直进行了下去。限于有限的时间，我们无法在代码结果上为你穷尽最好的可能性，但你可以尝试以下手段来对transformer继续进行改善：\n",
    "\n",
    "- **继续调整学习率调度**\n",
    "我们的案例中使用了学习率调度schechuler，但效果并不是非常显著，你可以尝试继续调整参数来改变当前的状况。在使用学习率调度时，你需要注意——\n",
    "> 温和启动：初期使用较小的学习率，逐渐增大，可以帮助模型在训练初期稳定下来，防止梯度爆炸。<br><br>\n",
    "> 动态调整：训练过程中根据验证集的表现调整学习率，比如在损失停止下降时降低学习率。\n",
    "\n",
    "- **增加模型的正则化**\n",
    "> 权重衰减：通过对模型的参数使用L2正则化来防止过拟合，有时也能帮助控制梯度爆炸。<br><br>\n",
    "> Dropout：增加或调整Transformer中的dropout比率，可以在训练过程中随机“丢弃”部分的输出，增强模型的泛化能力。\n",
    "\n",
    "- **修改模型架构或参数**\n",
    "调整Transformer的层数、头的数量或者模型维度，找到最适合你数据的配置。\n",
    "\n",
    "- **加强梯度裁剪**\n",
    "我们现在的梯度裁剪策略比较温和，你可以使用更小的阈值来加强梯度裁剪的强度。\n",
    "\n",
    "- **继续优化数据、例如尝试对数据完成特征工程**\n",
    "  \n",
    "在股价预测等金融时间序列分析任务中，原始数据往往含有丰富但未充分利用的信息。进行有效的特征工程可以帮助模型更好地理解和利用这些信息，从而提高预测的准确性和鲁棒性。你可以利用LSTM课程中所构建的一系列特征工程方法来尝试对数据进行有效的特征工程构建，获悉可以进一步降低Transformer预测的初始损失，并将损失降低到LSTM等算法无法达到的程度。\n",
    "\n",
    "- **使用完整的、带有decoder结构的Transformer**\n",
    "  \n",
    "使用编码器来处理历史输入数据，然后通过解码器逐步生成未来每个时间点的预测，将多步预测的步数扩大、或者直接这种结构特别适用于需要预测未来一系列值的情况。\n",
    "\n",
    "如果使用完整的encoder-decoder结构，就可以在解码器中实现自回归机制，即每一步生成的输出可以作为下一步的输入，这是目前为止最简单的、实现递归预测的方法，这有助于模型在生成每个后续预测时利用所有之前的预测。如果使用techer forceing机制，让模型在每个后续预测时都利用真实的标签，那Transformer结构的预测准确程度还会大幅提升。只要你认真学习了之前的Transformer原理以及这堂案例课，相信将当前的结构修改为encoder-decoder结构对你来说会比较容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4294534-1845-40fc-8f37-a9a17731e69a",
   "metadata": {},
   "source": [
    "到这里，我们对股票案例的讲解就结束了，虽然还留有许多可以去尝试的空间，但当前的案例已经比较完整。你可以继续在此基础上进行尝试和探索，如果你获得了更好的结果，也欢迎在课程中与我进行分享。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a705d8-c16d-44ed-97ba-575f23704c70",
   "metadata": {},
   "source": [
    "**Transformer实战（下）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e54d4-9a5a-4fa7-867e-329c5a4b88c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a0ad2-177c-47c6-a9b2-72b171d5efcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
