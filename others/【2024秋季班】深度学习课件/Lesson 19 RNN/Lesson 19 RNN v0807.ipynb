{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39160ec8-e413-4182-9e0e-3577ed8f7f2a",
   "metadata": {},
   "source": [
    "# 1 欢迎来到NLP的世界！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b20d7-d526-4240-b44a-bcab7873bfdd",
   "metadata": {},
   "source": [
    "![](https://www.searchenginejournal.com/wp-content/uploads/2020/08/an-introduction-to-natural-language-processing-with-python-for-seos-5f3519eeb8368.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56266d8-0f9d-47c2-aa18-988de72e27ea",
   "metadata": {},
   "source": [
    "**目录**\n",
    "\n",
    "1 欢迎来到NLP的世界！<br>\n",
    "&emsp;&emsp;1.1 是智能的象征，也是通往智能之路<br>\n",
    "&emsp;&emsp;1.2 大模型引发行业剧变<br>\n",
    "&emsp;&emsp;1.3 NLP领域的危险与机遇<br><br>\n",
    "2 自然语言领域中的数据<br>\n",
    "&emsp;&emsp; 2.1 深度学习中的时间序列数据<br>\n",
    "&emsp;&emsp; 2.2 深度学习中的文字序列数据<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 2.2.1 分词操作<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 2.2.2 词、字与token<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 2.2.3 编码<br><br>\n",
    "3 循环神经网络<br>\n",
    "&emsp;&emsp; 3.1 RNN的基本架构与数据流<br>\n",
    "&emsp;&emsp; 3.2 RNN的效率问题与权值共享<br>\n",
    "&emsp;&emsp; 3.3 RNN的输入与输出结构<br>\n",
    "&emsp;&emsp; 3.4 在PyTorch中实现循环神经网络<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 3.4.1 单层循环神经网络<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 3.4.2 深层循环神经网络<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 3.4.3 双向循环神经网络<br>\n",
    "&emsp;&emsp; 3.5 RNN的反向传播与缺陷<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 3.5.1 RNN反向传播的数学过程<br>\n",
    "&emsp;&emsp;&emsp;&emsp; 3.5.2 RNN的反向传播带来的问题<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28607327-73cc-4be4-a1ca-03d56e4e9ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 是智能的象征，也是通向智能之路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a1c74-72f3-482e-9b9e-2ddb2dfbc78f",
   "metadata": {},
   "source": [
    "无论你是经验丰富的深度算法工程师，还是仅仅简单了解过人工智能这一概念的算法入门者，你一定都听说过大名鼎鼎的“图灵测试”。图灵测试是计算机科学家阿兰·图灵在1950年提出的概念，这一测试可以衡量机器是否能够表现出与人类相当的智能。具体来说，图灵测试让一个人类与另一个不可视房间中存在的机器**进行对话**，如果该人类无法判断出与他/她对话的对象是机器还是真人，那该机器就被认为“通过图灵测试”，即被认可“拥有人工智能”。\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.licdn.com/dms/image/D4D12AQHjcIMCb7sTfQ/article-cover_image-shrink_600_2000/0/1679513310072?e=2147483647&v=beta&t=NsZ2UMVjgOgZRwZvG3tItLS5Xs9-pMbZYIbUeLYDWYo\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a516968-46d0-4b65-9e7d-8a9c996ec03b",
   "metadata": {},
   "source": [
    "自图灵测试被提出70多年以来，它一直是深度学习研究者们津津乐道的主题之一，众多模型都被投入进行不严格的“图灵测试”，而人们也以“模型是否与真人足够相似”、“模型生成的文字能否与假乱真”来衡量模型的优异程度。不公平的是，当人工智能在非语言领域表现卓越、甚至远远超出人类平均水平时，人们往往习以为常——例如，广泛应用的人脸识别技术其实并不出圈，大部分普通人心中所想的“人工智能”与人脸识别机器大相径庭。然而，当人工智能的交流能力接近人类水平、甚至还不能超越人类水平时（例如：ChatGPT的爆火），社会人就已经感觉到了巨大的威胁，国家紧急出台人工智能应用标准、学者与企业家发起“暂停研发”的呼吁、全球开启AI军备竞赛等技术大事件接踵而至，一时之间热闹非凡。\n",
    "\n",
    "事实上，在研究者们追求人工智能实现的路径上，我们有三种不同的智能层次：\n",
    "\n",
    "- **运算智能**：让计算机拥有快速计算和记忆存储能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce20a5a-6134-4611-9225-6c5cac6d8a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "> 硬件加速器：例如 GPU（图形处理单元）、TPU（张量处理单元）、ASICs（应用特定集成电路）等。\n",
    "> \n",
    "> 并行计算：多核处理器、分布式系统、超线程技术等。\n",
    "> \n",
    "> 高效算法：如 FFT（快速傅里叶变换）、Strassen算法（快速矩阵乘法）等。\n",
    "> \n",
    "> 内存和存储技术：如 SSD、RAM、以及新型存储技术如 3D XPoint。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40542cf-0f4f-4a1f-bca9-9e08ce2d8991",
   "metadata": {},
   "source": [
    "- **感知智能**：让计算机系统具备感知外部环境的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10ba61-0dcc-43c8-a10b-58fd5e763494",
   "metadata": {
    "tags": []
   },
   "source": [
    "> 计算机视觉：包括以卷积神经网络（CNN）和图像处理在内的一系列内容，应用于图像识别、目标检测、图像分割等。\n",
    "> \n",
    "> 语音识别：技术包括递归神经网络（RNN）、长短时记忆网络（LSTM）、声谱图等。\n",
    "> \n",
    "> 触觉技术：例如电容触摸屏、压力感应器等。\n",
    "> \n",
    "> 其它传感器技术：如雷达、激光雷达（LiDAR）、红外线传感器、摄像头、麦克风、气味检测传感器等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afcd5b-5af4-41c4-83b2-bd163c9f170a",
   "metadata": {},
   "source": [
    "- **认知智能**：让计算机系统具备类似于人类认知和思维能力的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ef291-c7fb-4c29-b64d-ed755a8a824e",
   "metadata": {
    "tags": []
   },
   "source": [
    "> 自然语言处理：如 RNN、transformer、BERT、GPT架构、语义分析、情感分析等。\n",
    "> \n",
    "> 增强学习：技术包括 Q-learning、Deep Q Networks (DQN)、蒙特卡洛树搜索（MCTS）等。\n",
    "> \n",
    "> 知识图谱：结合大量数据，构建对象之间的关系，支持更复杂的查询和推理。\n",
    "> \n",
    "> 逻辑推理和符号计算：如专家系统、规则引擎、SAT solvers 等。\n",
    "> \n",
    "> 模拟人类思维的框架和算法：例如认知架构（如 SOAR 和 ACT-R）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d202885-cd3f-4668-b384-8313d4aa1c6d",
   "metadata": {},
   "source": [
    "无论是图灵测试的设计方式，还是GPT爆火引发的AI浪潮都说明——在人工智能发展的过程当中，深度学习学者们、甚至整个人类社会都无意识地达成了一种高度的共识：**认知智能是智能的终极体现，人机同频的交流是智能被实现的象征**，无论一个人工智能算法有多强大的能力。只要它不能普适性地理解人类、不能让人类理解、不能与人类顺畅交流，它终归是无法融入人类和商业社会的（残酷的是，一个真人也是一样）。人工智能的终极评判标准，就是人机同频交流。\n",
    "\n",
    "在“人机同频交流”的大目标下，自然语言处理这一领域的关键性不言而喻。人类90%的信息获取与交流都依赖于语言，人类所有的逻辑、情感、知识、智慧、甚至社会的构建、文明的传承依赖于对语言的理解和表达。因此，计算机想要具备“看人类所看，想人类所想，与人类同频”的能力，就必须理解人类所使用的自然语言，而自然语言处理（Natural Langurage Process）正是研究**如何让计算机认知人类语言、理解人类语言、生成人类语言、甚至依赖这些语言与人进行交流、完成特定语言任务**的关键学科。豪不夸张的说，人工智能能否真正“智能”，很大程度上都依赖于自然语言处理领域的发展。也正因如此，在当今的机器学习世界，**自然语言处理有着极其重要的学术和工业地位**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333f101-61ea-4cd1-a09b-5f616bc7973e",
   "metadata": {},
   "source": [
    "- **持续繁荣的学术界**\n",
    "\n",
    "在过去10年中，计算机视觉技术逐渐成熟、对抗式技术停滞不前，深度学习领域的重大发展和成就都离不开自然语言技术的推动——从Word2Vec、LSTM到Transformer结构，再到BERT、GPT-3和GPT-4等模型，人工智能的每次出圈都离不开NLP，图灵奖得主、深度学习之父Geoffrey Hinton甚至直言“深度学习的下一个大的进展应该是让神经网络真正理解文字的含义”，人人都在关注NLP领域的发展。\n",
    "\n",
    "在过去5年中，NLP经典会议ACL和NAACL中被接受论文数量和比率都逐年增高，得益于语言与其他信息承载形式可以很好的结合，计算机视觉领域、强化学习、对抗式学习、自动驾驶等领域也都受到NLP的影响、纷纷出现借鉴NLP架构的精彩论文——能够在图像领域大杀四方的非卷积架构ViT（Vision Transformer）就是最典型的代表，而23年3月，谷歌大脑发表论文《LEAST-TO-MOST PROMPTING》，验证了大模型+恰当的提示工程可在自动驾驶领域的高难导航数据集SCAN上达到99%的预测精度，而在这之前SCAN数据集上的平均预测精度大约只有50%左右；同时，在2023年3月发布的\"机器学习/深度学习领域年度百佳论文\"列表中，专注NLP或需要NLP技术支持的论文占据了榜单的2/3，涉及生成式语言模型技术、预训练技术、大语言模型技术、语音技术、图文模型技术等各个方向，NLP无愧于人工智能研究的王者领域。\n",
    "\n",
    "- **工业界方兴未艾，招聘市场再度火热**\n",
    "\n",
    "在工业界，NLP技术已被投入到各类实用技术当中——搜索引擎、推荐系统、语音助手、聊天机器人、自动摘要、情感分析等都离不开NLP技术，在我们的PC或移动设备上，几乎每一个涉及到文本或语音的产品或服务都在使用NLP。大模型诞生后，这种现象更为明显，在2023年世界人工智能大会上，整个软件展区几乎全被大语言模型相关的软件和APP覆盖，几乎所有前沿科技企业、互联网企业都在尝试研究探索自己的大模型产品。\n",
    "\n",
    "受到大模型影响，NLP方向招聘市场也开始逐渐回暖，23年春招、23年夏季NLP岗位数量明显多于去年，且逐渐增加了各式各样大模型相关岗位：\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e829b-dee2-494e-a0a7-6520921c72a6",
   "metadata": {},
   "source": [
    "## 1.2 大模型引发行业剧变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37796330-2cec-479e-94aa-1509e179a2fb",
   "metadata": {},
   "source": [
    "随着对NLP的研究不断深入，我们见证了NLP在过去几十年中的巨大进展和突破。从早期的规则驱动方法到统计模型的兴起，再到如今神经网络和深度学习的崛起，NLP领域一直在不断演变和创新。这些发展为我们理解、分析和处理人类语言提供了强大的工具和框架，也为我们未来指出一条明路，让我们一起来看看NLP现在处于什么样的发展阶段，我们应该如何把握住时代的红利、规避时代风险。\n",
    "\n",
    "从2011年第三次人工智能革命开启，自然语言处理领域已经经历了三大发展阶段：\n",
    "\n",
    "1. 探索阶段：2011～2015（前Transformer时代）\n",
    "\n",
    "在AlphaGo和卷积网络掀起第三次人工智能革命之前，NLP领域主要依赖人工规则和知识库构建非常精细的“规则类语言模型”，当人工智能浪潮来临后，NLP转向使用统计学模型、深度学习模型和大规模语料库。**在这个阶段，NLP领域的重要目标是“研发语言模型、找出能够处理语言数据的算法”**。因此在这个阶段，NLP领域学者们一直在尝试一些重要的技术和算法，如隐马尔可夫模型（HMM）、条件随机场（CRF）和支持向量机（SVM）。同时，这个阶段也见证了循环神经网络RNN和长短期记忆网络LSTM等神经网络模型的出现和发展。\n",
    "\n",
    "2. 提升阶段：2015～2020（Transformer时代）\n",
    "\n",
    "RNN和LSTM是非常有效的语言模型，但是和在视觉领域大放光彩的卷积网络比起来，RNN对语言的处理能力只能达到“小规模数据上勉强够用”的程度。2015年谷歌将自注意力机制发扬光大、提出了Transformer架构，在未来的几年中，基于transformer的BERT、GPT等语言模型相继诞生，因此**这个阶段NLP领域的重要目标是“大幅提升语言模型在自然语言理解和生成方面的能力”**。这是自然语言处理理论发展最辉煌的时代之一。此外，这个阶段中语言模型已经能够很好地完成NLP领域方面的各个任务，因此工业界也实现了不少语言模型的应用，比如搜索引擎、推荐系统、自动翻译、智能助手等。\n",
    "\n",
    "3. 应用阶段：2020-至今（大模型时代）\n",
    "\n",
    "2020年秋天、GPT3.0所写的小软文在社交媒体上爆火，这个总参数量超出1750w、每运行1s就要消耗100w美元的大语言模型（Large Language Models，LLMs）为NLP领域开启了一个全新的阶段。在这一阶段，大规模预训练模型的出现改变了NLP的研究和应用方式，它充分利用了大规模未标注数据的信息，使得模型具备了更强的语言理解能力和泛化能力。基于预训练+微调模式诞生的大模型在许多NLP任务上取得了前所未有好成绩，在模型精度、模型泛化能力、复杂任务处理能力方面都展示出了难以超越的高水准，这吸引了大量资本的注意、同时也催生了NLP领域全新的发展方向与研究方向。现阶段NLP领域的核心目标主要集中在模型研发&成本降低&模型技术变现三大方向上：\n",
    "\n",
    "> 1) 如何研发、训练自己的大模型？\n",
    "\n",
    "虽然GPT系列大模型的原理并未开源，但GPT的成功无疑为“如何提升语言模型表现”指出了一条明路。在GPT的启发下，海内外各大科技企业正在研发基于BERT、基于GPT或基于Transformer其他组合方式的大模型，国内一线大模型ChatGLM系列就是基于BERT和GPT的融合理念开发的中文大模型。同时，大模型研发和训练技术、如生物反馈式强化学习（RLFH）、近端策略优化（PPO）、奖励权重策略（Reward-based Weighting）、DeepSpeed训练引擎等发展迅速，势不可挡。虽然现在已不是NLP理论发展的高峰，但毫无疑问，大模型算法研发与训练依然是NLP最前沿的研究方向之一。\n",
    "\n",
    "> 2) 如何降低大模型应用门槛与应用成本？\n",
    "\n",
    "大模型吞吃大量语料、训练成本极高，要将大模型应用到具体商业场景、还需进一步研究和训练。因此降低大模型应用成本的预训练、微调、大规模语料库构建等技术正蓬勃发展！自2020年以来已诞生十余种可行的微调方法和自动语料生成方法，如有监督微调（SFT）、低阶自适应微调方法LoRA、提示词前缀微调方法Prefix Tuning、轻量级Prefix微调Prompt Tuning、百倍效率提升的微调方法P-Tuning V2、以及自适应预算分配微调方法AdaLoRA等。这些方法催生了GPT4.0和大量语言方面落地应用，已经大大改变了NLP的研究和应用格局。\n",
    "\n",
    "> 3) 如何化技术为产品，实现大语言模型的商业应用？\n",
    "\n",
    "大语言模型在变现方面有两大优势：首先，大语言模型的性能十分强大、足以很好地支持各类NLP方面服务；其次，大语言模型使用自然语言与消费者交互，可以大幅降低新产品的使用门槛，还可以与图像、语音等领域强势联动、形成多模态的产品。基于这两点变现优势，自动翻译、智能助手、文本分析、情感分析等经典NLP任务都有了实用且价格低廉的APP产品，人们在日常生活工作中更是有无限的机会接触到各类基于大模型技术的NLP应用，家庭物联网、语音指令等技术更是已经走入千家万户，一些谐星的领域，如AI算命、AI佛祖、AI心理咨询师等也相继诞生……\n",
    "\n",
    "同时，随着大模型应用门槛和使用门槛都逐步降低，大量的大模型产品不断涌现——ChatGPT、跨语言代码编译工具Cursor、Github官方代码编写工具CopilotX、一键生成PPT内容的Gamma AI，office全家桶中配置的Copilot、Photoshop中配置的fill features，广泛生成图像的MidJourney和Stable Diffusion……这些应用不仅改变了商业的运营方式，也极大地影响了人们的生活和工作。同时，大模型APP研发范式LangChain也受到了大规模追捧，LangChain正在逐步构建基于大模型研发变现产品的行业规范，很快整个人工智能领域都将迎来大规模变现的时代。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6a90d-66a8-4cb7-b420-7e89ee17a7ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 NLP领域的危险与机遇\n",
    "\n",
    "\n",
    "微软联合创始人比尔·盖茨说“像ChatGPT这样的AI聊天机器人将变得与个人电脑或互联网同样重要”，英伟达总裁黄仁勋说“ChatGPT是AI领域iPhone，是更伟大事物的开始”。GPT的诞生在社会上引起巨大的轰动，这是因为它代表了大模型技术和预训练模型在自然语言处理领域的重要突破。它不仅提升了人机交互的能力，还为智能助手、虚拟智能人物和其他领域的创新应用打开了新的可能性。随着大模型的进一步发展和应用，我们有理由期待GPT以及类似的技术在未来带来更多令人惊叹的创新和进步。\n",
    "\n",
    "\n",
    "> 整体行业欣欣向荣，但行业结构会发生变化<br><br>\n",
    "> 核心技术发生转移、论文发表难度会上升<br><br>\n",
    "> 不会爆发针对NLPers的大规模失业潮（毕竟大部分NLPers在理论大爆发的年代做的也不是理论研究工作）<br><br>\n",
    "> NLPers面临转型、掌握大模型技术的NLPers会成为抢手人才<br><br>\n",
    "> 怀抱决心入行，现在是好时机，不过技术重点会发生改变<br><br>\n",
    "> 根据选择的方向，现在可以略微轻算法、重应用，但算法原理依然要学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba72bb1-7e94-44b9-a284-b4e354adad59",
   "metadata": {},
   "source": [
    "如果不跟进技术变革，个人也面临被淘汰的危机。现代社会的快速发展和技术创新意味着技能和知识的更新换代速度越来越快。如果个人停留在过时的技能和知识上，将很难适应变化的需求和就业市场的竞争。技术变革带来了新的工作方式、工具和需求。例如，随着人工智能、自动化和数字化的发展，许多传统的工作岗位可能会被自动化或被新的技术取代。如果个人没有及时学习和掌握新的技能，他们可能会发现自己在就业市场上面临竞争的不利位置。另外，技术变革也创造了新的机遇和职业领域。例如，大数据分析、人工智能开发、虚拟现实等新兴领域正迅速发展，对具备相关技能和知识的人才需求量大。如果个人能够抓住这些机遇，不断学习和更新自己的技能，就能在新兴领域找到有竞争力的职业发展机会。因此，紧跟技术变革并不断提升自己的技能和知识是非常重要的。持续学习、适应新技术、关注行业趋势以及不断发展自己的专业能力，可以帮助个人在快速变化的社会中保持竞争力，并更好地应对技术变革可能带来的淘汰危机。\n",
    "\n",
    "因此，作为立志投身NLP领域的人们，我们应该抓住这个机会，勇往直前。NLP领域不仅具有广阔的前景，而且对于个人成长和职业发展也带来了巨大的机遇。我们可以**参与到大模型的研发和优化**中，探索更高效的模型结构和训练方法；我们可以**开发创新的NLP应用**，为人们提供更智能、高效的语言交互体验；我们可以**参与到多模态、具身和人类行为模拟等前沿研究**中，推动NLP技术的进一步发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b94444-f6d2-41e2-9577-1dc1f09d02c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 自然语言领域中的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad37ec0-d610-4b46-b70c-90030697ce59",
   "metadata": {},
   "source": [
    "在深度学习的世界中，某一领域的架构/算法往往是根据该领域中特定的数据状态设计出来的。例如，为了处理带有空间信息的图像数据，算法工程师们使用了能够处理空间信息的卷积操作来创造卷积神经网络；又比如，为了将充满噪音的数据转变成干净的数据，算法工程师们创造了能够吞吃噪音、输出纯净数据的自动编码器结构。因此，在了解每个领域的算法架构之前，我们最好先学习当前领域的数据特点和数据结构，在自然语言处理领域也是如此。\n",
    "\n",
    "自然语言领域的核心数据是**序列数据**，这是一种在样本与样本之间存在特定顺序、且这种特定顺序不能被轻易修改的数据。这是什么意思呢？在机器学习和普通深度神经网络的领域中我们所使用的数据是二维表。如下所示，在普通的二维表中，样本与样本之间是相互独立的，一个样本及其特征对应了唯一的标签，因此无论我们先训练1号样本、还是先训练7号样本、还是只训练数据集中的一部分样本，都不会从本质上改变数据的含义、许多时候也不会改变算法对数据的理解和学习结果。\n",
    "\n",
    "![01](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/01.png)\n",
    "\n",
    "但序列数据则不然，对序列数据来说，一旦**调换样本顺序**或**样本发生缺失**，数据的含义就会发生巨大变化。最典型的序列数据有以下几种类型：\n",
    "\n",
    "1. **文本数据（Text Data）**：文本数据中的样本的“特定顺序”是语义的顺序，也就是词与词、句子与句子、段落与段落之间的顺序。在语义环境中，词语顺序的变化或词语的缺失可能会彻底改变语义，例如——\n",
    "> **改变顺序**：事半功倍和事倍功半；曾国藩战太平天国时非常著名的典故：他将“屡战屡败”修改为“屡败屡战”，前者给人绝望，后者给人希望。<br><br>\n",
    "> **样本缺失**（对文本来说特指上下文缺失）：小猫睡在毛毯上，因为它很____。当我们在横线上填上不同的词时，句子的含义会发生变化。\n",
    "\n",
    "2. **时间序列数据（Time Series Data）**：时间序列数据中的“特定顺序”就是时间顺序，时序数据中的每个样本就是每个时间点，在不同时间点上存在着不同的标签取值，且这些标签取值常常用于描述某个变量随时间变化的趋势，因此样本之间的顺序不能随意改变。例如，股票价格、气温记录和心电图等数据，一旦改变样本顺序，就会破坏当前趋势，影响对未来时间下的标签的预测。\n",
    "\n",
    "3. **音频数据（Audio Data）**：音频数据大部分时候是文本数据的声音信号，此时音频数据中的“特定顺序”也是语义的顺序；当然，音频数据中的顺序也可能是音符的顺序，试想你将一首歌的旋律全部打乱再重新播放，那整首歌的旋律和听感就会完全丧失。\n",
    "\n",
    "4. **视频数据（Video Data）**：你知道动画是由一张张原画构成的吗？视频数据本质就是由一帧帧图像构成的，因此视频数据是图像按照特定顺序排列后构成的数据。和音频数据类似，如果将动画或电影中的画面顺序打乱再重新播放，那没有任何人能够理解视频的内容。\n",
    "\n",
    "类似的数据还有很多，例如DNA序列数据，从医学角度来说DNA测序的顺序不能被打乱，否则就会违背医学常识。除此之外，符号序列数据也是常见的序列数据，密码学、自动编码学、甚至自动编程的算法都对数据本身的逻辑有严格的要求。很明显，**在处理序列数据时，我们不仅要让算法理解每一个样本，还需要让算法学习到样本与样本之间的联系**。今天，这些能够学习到样本之间联系的算法们构成了自然语言处理架构群。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be30b6-4685-408d-ad9d-eefcb55818d1",
   "metadata": {},
   "source": [
    "## 2.1 深度学习中的时间序列数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1079f4e-05cc-4abb-86f5-f86d9736ad11",
   "metadata": {},
   "source": [
    "序列数据的概念很容易理解，但奇妙的是，现实中的序列数据可以是二、三、四、五任意维度，只要给原始的数据加上“时间顺序”或“位置顺序”，任意数据都可以化身为序列数据。在这里，我们展现几种常见的序列数据：\n",
    "\n",
    "> 二维时间序列\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/02.png)\n",
    "\n",
    "时间序列中，样本与样本\n",
    "之间的顺序是时间顺序，因此每个样本是一个时间点，时间顺序也就是time_step这一维度上的顺序。**这种顺序在自然语言处理领域叫做“时间步”（time_step）**，它代表了当前时间序列的长度，因此也被称为序列长度（sequence_length）。对时间序列而言，时间步的顺序这正是我们要求算法必须去学习的顺序。在时序数据中，时间点可以是任意时间单位（分钟、小时、天），但时间点与时间点之间的间隔必须是一致的。\n",
    "\n",
    "在NLP领域中，我们常常一次性处理多个时间序列，如下图所示，我们可以一次性处理多支股票的股价波动序列——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e021c0-6a30-4ecd-8ab9-fc9816819c02",
   "metadata": {},
   "source": [
    "> 三维时间序列\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/03_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0272e8-14d2-4357-9332-854fab733139",
   "metadata": {},
   "source": [
    "此时我们拥有的是一个三维矩阵，其中batch_size是样本量，也就是一共有多少个二维时间序列表单。你或许已经发现了，其实三维时间序列数据就是机器学习中定义的“多变量时间序列数据”。在多变量时间序列数据当中，时间和另一个因素共同决定唯一的特征值。在上面的例子中，每张时序二维表代表一支股票，因此在这个多变量时间序列数据中“时间”和“股票编号”共同决定了一个时间点上的值，如果在机器学习中，我们会看到这样的数据结构：\n",
    "\n",
    "|股票ID|时间|开盘价|收盘价|交易量|最高价|最低价|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|00K621|6月1日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00K621|6月2日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00K621|6月3日|xxx|xxx|xxx|xxx|xxx|\n",
    "|……|||||||\n",
    "|00E504|6月1日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00E504|6月2日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00E504|6月3日|xxx|xxx|xxx|xxx|xxx|\n",
    "|……|||||||\n",
    "|00H829|6月1日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00H829|6月2日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00H829|6月3日|xxx|xxx|xxx|xxx|xxx|\n",
    "|……|||||||\n",
    "\n",
    "当把这张表单拆成独立的三张表单，每张表单上只显示一支股票时，就是深度学习中常见的三维时间序列数据。相似的例子还可能是——不同用户在不同时间点上的行为，不同植物在不同季节时分泌的激素值、不同商家在不同时间点上的销售额等等。\n",
    "\n",
    "需要注意的是，虽然上述两种形式的时序数据是深度学习中最常见的时序数据，但时序数据被用于不同的算法时可能有不同的形态，有时候我们甚至不会拘泥于“时序”和“连续性”这些时间序列的常规属性，而是从时间数据的局部性和全局性来考虑，将时序数据变形为特定网络所需要的输入数据形态（例如，时间序列数据用于CNN，或时间序列数据用于GAN时，它的形态会不同于上述我们描述的形态）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c68d46-d179-4067-ae40-064c59b6d472",
   "metadata": {},
   "source": [
    "## 2.2 深度学习中的文字序列数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0105501-23d8-4548-b548-bc6d6acb5203",
   "metadata": {},
   "source": [
    "> 二维文字序列\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/04.png)\n",
    "\n",
    "在文字数据中，样本与样本之间的联系是语义的联系，语义的联系即是词与词之间、字与字之间的联系，因此在文字序列中每个样本是一个单词或一个字（对英文来说大部分时候是一个单词，偶尔也可以是更小的语言单位，如字母或半词），故而**在中文文字数据中，一张二维表往往是一个句子或一段话，而单个样本则表示单词或字**。\n",
    "\n",
    "此时，不能够打乱顺序的维度是vocab_size，它代表了一个句子/一段话中的字词总数量。一个句子或一段话越长，vocab_size也就会越大，因此这一维度的作用与时间序列中的time_step一致，vocab_size在许多时候也被称之为是序列长度（sequence_length）。同样，vocab_size这一维度上的顺序就是算法需要学习的语义顺序。\n",
    "\n",
    "算法是不能认知文字数据的，因此我们必须将文字数据转化为“数字”来进行表示，这个过程叫做“编码”。编码是一个复杂的过程，但是现在我们不必对其进行深究，我们只需要知道，我们可以像上面的图像中一样将一个单词编码成一个数字，也可以将单词编码成一个序列。大部分时候，我们需要学习的肯定不止一个句子，当每个句子被编码成矩阵后，就会构成高维的多特征词向量。由于在实际训练时，所有句子或段落长度都一致的可能性太小（即所有句子的vocab_size都一致的可能性太小），因此我们往往为短句子进行填充、或将长句子进行裁剪，让所有的特征词向量保持在同样的维度。\n",
    "\n",
    "- 三维文字序列\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/06_.png)\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/07_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ce99f-6d1e-4ba9-8f18-a947e872ddbd",
   "metadata": {},
   "source": [
    "### 2.2.1 分词操作\n",
    "\n",
    "原始文本数据大多是段落，但是输入到深度学习中的文字数据的样本却是词或字，因此文字数据大部分时候需要进行“分词”。分词是将连续的文本切分成一个个具有独立意义的词或词组的过程，良好的分词可以降低算法理解文本的难度，可以很好地提升模型的性能。例如，将诗句“玉露凋伤枫树林”分为[“玉露”，“调伤”，“枫树林”]三个词，可能会比将其分为[“玉”，“露”，“凋”，“伤”，“枫”，“树”，“林”]7个字更容易理解，要求每个字都自带完整的语义其实会有些困难。\n",
    "\n",
    "由于不同的语言有不同的特色，因此每种语言所使用的分词方式也大不相同。例如，英文等拉丁语系的语言天然就有空格来分割不同的单词，只要按照空格进行分词就能够自然得到很好的结果，而中文日文韩文等语言却没有空格来进行辅助，有时分词的结果可能会造成巨大的误解。例如，经典的“吃烧烤不给你带”这一句子，分割成[“吃”，“烧烤”，“不给”，“你”，“带”]和[“吃”，“烧烤”，“不”，“给你”，“带”]就会令语义有所不同，因此中文还面临着“断句”的挑战。\n",
    "\n",
    "现在对于不同语言的分词，我们都有丰富的手段可以操作：\n",
    "\n",
    "> **中文分词**：\n",
    "> > **基于词典的方法**：最经典的方法，例如最大匹配法、最小匹配法等，它们基于预先定义的字典来执行分词。在使用何种方法之前需要先构造词典。<br><br>\n",
    "> > **基于统计的方法**：例如HMM（隐马尔科夫模型）和CRF（条件随机场）。<br><br>\n",
    "> > **深度学习方法**：例如基于Bi-LSTM的分词模型。<br><br>\n",
    "> > **经典工具**——\n",
    "> >  - **jieba**：是一个Python的第三方库。支持三种分词模式：精确模式、全模式和搜索引擎模式。速度相对较快，使用简单。可以自定义添加词典。<br><br>\n",
    "> >  - **HanLP**：不仅仅是分词工具，还包括词性标注、命名实体识别等多种NLP任务。支持多种分词算法，如CRF、感知机等。同时支持繁体中文和简体中文。提供了丰富的预处理和后处理功能。<br><br>\n",
    "> >  - **THULAC（清华大学THU词法分析工具包）**：不仅支持分词，还提供词性标注功能。使用条件随机场（CRF）模型。<br><br>\n",
    "> >  - **FudanNLP（复旦大学NLP工具集）**：提供了分词、词性标注、命名实体识别等功能。使用结构化感知机模型。<br><br>\n",
    "> >  - **LTP（语言技术平台）**：由哈工大社会计算与信息检索研究中心开发。提供全套中文NLP处理工具，包括分词、词性标注、句法分析等。使用感知机模型。<br><br>\n",
    "> >  - **SNLP (Stanford NLP for Chinese)** ：斯坦福大学开发的NLP工具，支持多种语言，其中包括中文。提供分词、词性标注、命名实体识别等功能，基于都使用基于深度学习的方法。<br><br>\n",
    "> 选择哪种工具主要取决于特定任务的需求和使用场景。对于大部分应用，例如简单的文本预处理，jieba可能就足够了。但如果需要更深入的语言学特性或高准确性的处理，可能需要考虑如HanLP或LTP这样的更全面的工具。在NLP部分后续的代码实战环节，我们将展示使用各式分词工具的代码与结果。\n",
    "> \n",
    "> **英文分词**：\n",
    "> > **空白字符分词**：由于英文单词之间通常由空格分隔，所以简单的空格分词在很多情况下都很有效。<br><br>\n",
    "> > **基于规则的方法**：如NLTK、spaCy等工具提供的分词方法。<br><br>\n",
    "> > **子词分词**：如BPE或SentencePiece，它们可以将英文单词进一步切分成常见的子词或字符级别的片段。\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db502a3-f577-41f5-ae78-458c1363f88b",
   "metadata": {},
   "source": [
    "### 2.2.2 词、字与Token\n",
    "\n",
    "Token（读音tow·kn）是自然语言处理世界中的重要概念，这个概念没有官方中文译名，但我们可以根据该词语在众多论文中的语境对其进行如下定义：**Token是当前分词模式下的最小语义单元**，根据分词方式的不同，它可能是一个单词（upstair）、一个半词（up，stair）或一个字母（u,p,s,t...），也可能是一个短语（攀登高峰）、一个词语（攀登、高峰）或一个字（攀，登，高，峰）。之前我们提到，分词是将连续的文本切分成一个个**具有独立意义的词或词组**的过程，其实本质上来说分词就是分割Token的过程，因此文字数据表单中的一行行样本也就是一个个token。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/04.png)\n",
    "\n",
    "Token在自然语言处理世界中有什么意义呢？首先，它是语义的最小组成部分，同时也是大部分深度学习算法输入数据时的“单一样本”。**Token的数量代表了样本的数量，也就代表了当前文本的长度和当前算法需要处理的数据量，直接对当前算法运行需要多少资源（算力、时间、电力）产生影响**，也就会影响模型开发、模型训练、模型调用的成本。在NLP和大语言模型的世界中，OpenAI等模型开发厂商是以Token使用程度来进行计价和使用限制的，我们也以大模型训练和微调时所必须使用的token数量来衡量大模型的性能。随着大模型的发展，Token已成为NLP世界中广受认可的数据量/模型吞吐量计量单位——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84617ad-4c95-49ed-bcce-edff96cb51ba",
   "metadata": {},
   "source": [
    "<center>OpenAI规定的GPT3.5在一次对话中允许的token总量    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c4ca1-f003-486a-8344-353d586d7c3d",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa3b82-83c3-41d3-bba6-80740eb9cf3d",
   "metadata": {},
   "source": [
    "<center>OpenAI规定的pay-as-you-go（充值型）大模型计费方法和使用限制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6334c-2382-4374-aa93-570f00c1fb9c",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751e276-0678-4ebb-b608-0904568e7c6a",
   "metadata": {},
   "source": [
    "在OpenAI官网上，我们甚至能够直接找到计算当前文本Token的工具[Tokenizer](https://platform.openai.com/tokenizer)。同时，OpenAI还非常贴心地为我们准备了一篇Github文档专门用于讲解[如何计算一篇文案的Token、如何选择最省token的方式来节省费用](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)，感兴趣的大家可以去进行阅读。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ff624-eb5b-4714-a063-8ca96a942f86",
   "metadata": {},
   "source": [
    "### 2.3.3 编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa1a36-4c7c-477b-a72f-dcb1083ce87f",
   "metadata": {},
   "source": [
    "一直以来，文字序列是不能直接放入算法进行运行的，必须要要编码成数字数据才能供算法学习，因此在NLP领域中我们大概率会将文字数据进行编码。编码的方式有很多种，但无一例外的，**编码的本质是用单一数字或一串数字的组合去代表某个字/词**，在同一套规则下，同一个字会被编码为同样的序列或同样的数字，而使用一个数字还是一串数字则可以由算法工程师自行决定。目前深度学习中常见的编码方式有：\n",
    "\n",
    "- **One-hot编码**：将每个词表示为一个长向量，这个向量的维度等于词汇表的大小，其中只有一个维度的值为1，其余为0。这个1的位置对应于该词在词汇表中的位置。\n",
    "\n",
    "- **词嵌入（Word Embeddings）**：词嵌入是一种将词或短语映射到高维空间的表示方法，使得语义上相似的词在这个高维空间中彼此接近。例如，通过训练得到的词嵌入模型，我们可能发现\"king\"和\"queen\"、\"man\"和\"woman\"在向量空间中是相似的。词嵌入通常通过大量的文本数据训练得到，目的是捕捉单词之间的语义关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6cb52-5f2c-4e4b-a178-fef84999723f",
   "metadata": {},
   "source": [
    "<center>对句子分别进行embedding编码和独热编码后产生的二维表单："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7577d-f535-4330-a32b-034d51a22b42",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba99824-bdca-44fe-adf9-ce93b4b7c0b2",
   "metadata": {},
   "source": [
    "<center>语义空间示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a01ab-c55d-4014-af9c-fcba5d50ad6e",
   "metadata": {},
   "source": [
    "![](https://wires.onlinelibrary.wiley.com/cms/asset/a30c782e-f0bb-4dfb-8163-251f3b93b3a6/wcs1457-fig-0001-m.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f827b-a770-4b0e-a34f-7deae8acb93e",
   "metadata": {},
   "source": [
    "经典的词嵌入方法有：\n",
    "> Word2Vec：通过神经网络模型学习单词的向量表示，常见的有CBOW和Skip-Gram两种模型。\n",
    "> \n",
    "> GloVe（Global Vectors for Word Representation）：基于单词共现统计信息来学习单词的向量表示。\n",
    "> \n",
    "> FastText：与Word2Vec类似，但考虑了单词内部的子词信息。\n",
    "> \n",
    "> 固定编码：例如使用BERT、GPT等预训练的深度学习模型来编码文本。这些模型通常使用大量数据进行预训练，并可以为新任务进行微调。\n",
    "> \n",
    "> 基于大语言模型进行编码：在OpenAI研发的大模型生态矩阵当中，存在专用于构建语义空间的embeddings大模型。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/21.png)\n",
    "\n",
    "除了最为经典的one-hot和embedding之外，还有以下方法可以进行编码：\n",
    "\n",
    "- **TF-IDF**：基于单词在文档中的频率和在整个数据集中的反向文档频率来为单词分配权重。\n",
    "\n",
    "- **Byte Pair Encoding (BPE) / SentencePiece**：这是子词级别的编码，能够处理词汇外的单词和多种语言的文本。\n",
    "\n",
    "- **ElMo**：深度上下文化词嵌入，考虑了单词的上下文信息来生成词向量。\n",
    "\n",
    "- **Seq2Seq等序列变化模型**：如果将“编码”的概念拓展到“如何将非结构化数据（如文本）转换为结构化的数字表示”，那seq2seq等序列转化模型也可以作为编码的手段之一。seq2seq，即sequence-to-sequence，是一个在多种NLP任务中广泛应用的神经网络结构，常常被用在机器翻译、文本摘要、问答系统等需要从一个序列生成另一个序列的任务中，因此seq2seq本质与encoder-decoder非常相似，是输入序列、输出序列的模型。seq2seq模型的编码器涉及到了与文本编码很类似的过程，它将输入序列（如一个句子）转换为一个固定大小的向量。但这个向量通常是为了特定的seq2seq任务（如翻译）而生成的，并不是用于一般的文本表示。因此，虽然seq2seq的编码器确实进行了“编码”，但它和我们之前讨论的文本编码方法（如Word2Vec或TF-IDF）有些不同。\n",
    "\n",
    "在后续的课程中，我们将会单独增设编码相关的课程篇章，为大家详细梳理经典的编码方法、阅读经典编码论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b921b7f-c6e4-41c3-8dfa-caf711f1fa01",
   "metadata": {},
   "source": [
    "在了解时序和文字数据后，你能够大概想象语音数据和视频数据的状态吗？虽然限于NLP的主题与课程时间，我们无法就所有的序列数据展开详谈，但根据上面所绘制的图像，如果你层曾经学过计算机视觉、且很好地掌握了卷积神经网络，那我想你应该能够自己绘制出音频数据与视频数据的结构。现在我们已经了解了时序数据和文本数据的一般结构，**那请问循环神经网络是用于上述哪种结构的呢**？\n",
    "\n",
    "答案是，都可以！循环神经网络是深度学习中为数不多的、可以在网络架构不变的情况下、同时接受二、三维数据的网络，接下来就让我们看看循环神经网络是如何在序列数据上学习样本与样本之间的顺序的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19512743-72a4-4b76-9b8f-842a159d77f3",
   "metadata": {},
   "source": [
    "# 3 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6507192-1ecf-4e23-ad52-4964343fea0f",
   "metadata": {},
   "source": [
    "循环神经网络（Recurrent Neural Network）是自然语言处理领域的入门级深度学习算法，也是序列数据处理方法的经典代表作，它开创了“记忆”方式、让神经网络可以学习样本之间的关联、它可以处理时间、文字、音频数据，也可以执行NLP领域最为经典的情感分析、机器翻译等工作。在NLP领域，循环神经网络是GRU、LSTM以及许多经典算法的基础、更对我们理解transformer结构有巨大的帮助，因此即便在Transformer和大语言模型统治前沿算法战场的今天，我们依然需要学习RNN算法。RNN就仿佛机器学习中的逻辑回归算法一般，是打开NLP领域大门的钥匙。今天我们就一起来看看RNN的基本逻辑和实现手段。\n",
    "\n",
    "<font color=\"red\">【注意】在学习循环网络之前，请确保你已对基础的深度神经网络（DNN）有深刻的理解，确保你了解所有经典优化层、理解梯度下降过程、理解正向反向传播过程、理解链式法则、神经网络如何训练、深度神经网络如何输入输出等基础知识。<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e9094-9dbd-4c7e-855c-79842a2f2139",
   "metadata": {},
   "source": [
    "## 3.1 RNN的基本架构与数据流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb162d3-3fb5-4762-859b-f95f3d72fd5f",
   "metadata": {},
   "source": [
    "如果你去找寻网络上的各种资源，你会惊讶地发现循环神经网络有各种各样复杂的公式表示和图像表示方法。然而，**光从网络架构来说，循环神经网络与深度神经网络是完全一致的**。\n",
    "\n",
    "首先，循环神经网络由输入层、隐藏层和输出层构成，并且这三类层都是线性层。和深度神经网络中的线性层一样，**输入层的神经元个数由输入数据的特征数量决定**，隐藏层数量和隐藏层上神经元的个数都可自己设置，而**输出层的神经元数量则需要根据输出的任务目标进行设置**。以下面的数据为例，现在我们将每个单词都编码成了5个特征构成的词向量，因此输入层就会需要5个神经元，我们将该文字数据输入循环神经网络执行三分类的“情感分类”任务（三分类分别是[积极，消极，中性]），那输出层就会需要三个神经元。假设有一个隐藏层，而隐藏层上有2个神经元，一个最为简单的循环网络的网络结构如下：\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/08_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0f4f2-a6df-4c7d-adb6-e453e498283c",
   "metadata": {},
   "source": [
    "在这个结构中，激活函数的设置、神经元的连接方式等都与深度神经网络一致，因此循环神经网络在网络构建方面没有太多可以深究的内容，循环网络真正精彩的地方在于其**创造了全新的数据流**，我们来具体看一下——\n",
    "\n",
    "当我们将数据输入深度神经网络DNN时，一个神经元会一次性处理一列数据，5个神经元会涵盖整张表的数据，在一次正向传播中深度神经网络就会接触到完整的一张数据表。这种方式计算效率很高，同时矩阵计算也很简单：输入结构为（9，5），中间层输出为（9，2），最终输出结果为（9，3），整个计算过程完全只考虑每个单词的特征之间的转换（5➡️2➡️3），而完全忽略“单词与单词之间的联系”，毕竟输入9个单词，输出9个单词，并没有对单词之间的关系进行任何学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da603a87-df79-4f23-bf3b-3a1f56f25bca",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06b0ba-68ea-4985-8b1c-f6a2e7d5d147",
   "metadata": {},
   "source": [
    "但是，在循环神经网络当中就不一样了。虽然是一模一样的网络结构，但当我们将数据输入到循环神经网络时，一个神经元一次性只会处理一行（也就是一个单词）的数据，5个神经元会覆盖当前单词的5个特征，在一次正向传播中，循环神经网络只会接触到一个单词的全部信息，而不会接触到整张表。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0e832-bb47-46ef-a012-2498921bfb95",
   "metadata": {},
   "source": [
    "如果这样的话，岂不是要一行一行处理数据了？没错！虽然非常颠覆神经网络当中对效率的根本追求，但循环神经网络是一个单词、一个单词地处理文本数据，一个时间点、一个时间点处理时序数据的。具体过程如下：\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/11.png)\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/12.png)\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f032f6-c7cb-4c49-8259-7ee8417519dd",
   "metadata": {},
   "source": [
    "如果一次正向传播只处理一行数据，那对于结构为（vocab_size，input_dimension）的文字数据来说，就需要在同一个网络上进行vocab_size次正向传播。同样的，对于结构为（time_step，input_dimension）的时间序列数据来说，就需要在同一个网络上进行time_step次正向传播。所以为了便利，**vocab_size和time_step这个维度可以统称为时间步，对任意数据来说，循环神经网络都需要进行时间步次正向传播，而每个时间步上是一个单词或一个时间点的数据**。\n",
    "\n",
    "基于这样的数据流设置，循环神经网络构建了自己的**灵魂结构：循环数据流**。在多次进行正向传播的过程中，循环神经网络会将每个单词的信息向下传递给下一个单词，从而让网络在处理下一个单词时还能够“记得”上一个单词的信息。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/10.png)\n",
    "\n",
    "如下图所示，在$T_{t-1}$时间步上时，循环网络处理了一个单词，此时隐藏层上输出的中间变量$H_{t-1}$会走向两条数据流，一条数据流是继续向输出层的方向正向传播，另一条则流向了下一个时间步的隐藏层。在$T_{t}$时间步时，隐藏层会结合当前正向传播的输入层传入的$X_t$和上个时间不的隐藏层传来的中间变量$H_{t-1}$共同计算当前隐藏层的输出$H_{t}$。如此，$H_{t}$当中就包含了上一个单词的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf099f3-6fb5-4aeb-b5d7-aab0b26a4f9d",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097da0f-a013-42dc-9bba-bef458319bf7",
   "metadata": {},
   "source": [
    "使用数学公式表示如下：\n",
    "\n",
    "$$H_t = f(W_{hh}H_{t-1} + W_{xh}X_t)$$\n",
    "$$ = f(W_{hh}f(W_{hh}H_{t-2} + W_{xh}X_{t-1}) + W_{xh}X_t)$$\n",
    "$$ = f(W_{hh}f(W_{hh}f(W_{hh}H_{t-3} + W_{xh}X_{t-2}) + W_{xh}X_{t-1}) + W_{xh}X_t)$$\n",
    "$$ \\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bd3aa-0b4a-41c2-be51-b2b58dc16c63",
   "metadata": {},
   "source": [
    "使用架构图表示，则可表示如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77617633-546e-4c19-9f5d-a27b64740ed3",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515f89e-b107-4a80-8da1-859657a7678a",
   "metadata": {},
   "source": [
    "利用这种方式，只要进行vocal_size次向前传播，并且每次都将上一个时间步中隐藏层上诞生的中间变量传递给下一个时间步的隐藏层，整个网络就能在全部的正向传播完成后获得整个句子上的全部信息。在这个过程中，我们在同一个网络上不断运行正向传播，**此过程在神经网络结构上是循环，在数学逻辑上是递归，这也是循环神经网络名称的由来**。\n",
    "\n",
    "在这个过程中，H被称之为是“隐藏状态”（Hidden Representation 或 Hidden State），它特指在循环神经网络的隐藏层上诞生的中间变量，一般在代码中也表示为小写的字母h。当然，在循环神经网络中我们也是可以有多个隐藏层的，虽然我们的图像上只显示了最为简单的情况（一个输入层、一个隐藏层、一个输出层）：\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ca99a-d964-4730-8898-c55f7dcee7f7",
   "metadata": {},
   "source": [
    "## 3.2 RNN的效率问题与权值共享"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2bb86-bfa8-44c0-b811-db84e749dfb6",
   "metadata": {},
   "source": [
    "现在你已经知道循环网络的数据流和基本结构了，但我们还面临一个巨大的问题——效率。刚才我们以一张表为例讲解了循环神经网络的迭代过程，但循环网络在实际应用时可能面临batch_size张表单，如果每张表单都需要一行一行进行向前传播的话，那循环神经网络运行一次需要（batch_size * time_step）次向前传播，这样整个网络的运行效率必然是非常非常低的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc494f45-2df3-443b-8c91-f48aff251508",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/06_.png)\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/07_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e2691-48a6-41c1-83c2-ed119cb8169e",
   "metadata": {},
   "source": [
    "幸运的是，事实上这个问题并不存在。在现实中使用循环神经网络的时候，我们所使用的输入数据结构往往是三维时间或三维文字数据，也就是说数据中大概率会包括不止一张时序二维表、会包括不止一个句子或一个段落。之前我们提到过，循环神经网络要顺利运行的前提是所有的句子/时间序列被处理成同等的长度，因此**编码后每张二维表需要循环的时间步数量是相等的**，因此在实际训练的时候循环神经网络是会一次性将所有的batch_size张二维表的第一行数据都放入神经元进行处理，故而RNN并不需要对每张表单一一处理，而是对全部表单的每一行进行一一处理，所以最终循环神经网络只会进行time_step次向前传播，所有的batch是共享权重的。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494362d-1619-4585-8f2c-d7b06ef24104",
   "metadata": {},
   "source": [
    "如果将三维数据看作是一个立方体，那循环神经网络就是一次性处理位于最上层的一整个平面的数据，因此循环神经网络一次性处理的数据结构与深度神经网络一样都是二维的，只不过这个二维数据不是（vocal_size，input_dimension）结构，而是（batch_size，input_dimension）结构罢了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd89a81-b910-4e0f-b84e-71f942ae74de",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a69fe-26c3-4777-aadb-b7224468ce9c",
   "metadata": {},
   "source": [
    "讲到这里，我相信你已经非常了解循环神经网络的基本结构和巧妙之处了，在开始进行循环神经网络的实现之前，我们还需要解决最后一个问题：明确循环神经网络的输入和输出数据结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e02f9e-ecfb-4995-ad5e-d01fff24741a",
   "metadata": {},
   "source": [
    "## 3.3 RNN的输入与输出结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b22b-03e9-45a4-97b7-8f87b4fcdfc1",
   "metadata": {},
   "source": [
    "- **输入结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc54b1-e525-413b-94ca-e6da3c027e44",
   "metadata": {},
   "source": [
    "在之前的课程中我们提到过，循环神经网络是为数不多的、能够在不改变网络结构情况下同时处理二维数据和三维数据的网络，**但在PyTorch或tensorflow这样的深度学习框架的要求下，循环神经网络的输入结构一律为三维数据**。\n",
    "\n",
    "通常来说，最常见的结构就是之前我们提到过无数次的（batch_size，vocal_size，input_dimension），且循环是在vocal_size维度进行。不过，如果你曾经自学循环神经网络，或找寻过其他相关的材料，那你可能会发现，在某些材料当中，循环神经网络的输入被描述为（vocal_size，batch_size，input_dimension）结构。事实上，这两种结构是同一种结构，我们来看："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3908f9-70d5-4969-875d-a4c5e66229af",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133fb4b6-d3a6-48a4-8d5c-f3bcca620278",
   "metadata": {},
   "source": [
    "普通的结构（batch_size，vocal_size，input_dimension）如左图，此时循环神经网络会在vocal_size这一维度上循环，执行vocal_size个时间步的正向传播、即从上至下不断处理面向上方的二维表单（虚线标注处）。但立方体是可以被旋转的，当我们将立方体旋转一个角度，即需要处理的二维表单由正上方专向正前方时，我们就得到了（vocal_size，batch_size，input_dimension）的数据结构，此时循环神经网络依然是在vocal_size方向进行循环，只不过我们需要处理的表单方向由从上到下变成了从前往后。\n",
    "\n",
    "**因此不难发现，本质上这两种结构是一模一样的，但无论是哪种结构，循环神经网络都必须在时间步的方向进行循环**。对于时序数据来说，这个方向是time_step的方向，对于文字数据来说，这个方向是vocab_size的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541acef-01f6-4e4a-9f6e-fcb80bc62d62",
   "metadata": {},
   "source": [
    "- **输出结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3c138-c4d2-4d29-a20a-086a6f50e206",
   "metadata": {},
   "source": [
    "循环神经网络的输出层结构是由具体的输出任务决定的，但丰富的NLP任务让RNN输出层也变得丰富多彩。从网络结构的角度，我们可以将NLP的任务大略分成以下三类：\n",
    "\n",
    "1. 对语义/关系/文字本身进行**有标签分类/标注**的任务，包括但不限于——\n",
    "> **文本分类**：如情感分析、新闻分类、垃圾邮件检测等。<br><br>\n",
    "> **命名实体识别（NER）**：识别文本中的人名、地点、日期、组织等实体。<br><br>\n",
    "> **关系抽取**：从文本中抽取实体之间的关系。<br><br>\n",
    "> **词性标注**：为文本中的每个词指定其词性（名词、动词等）。<br><br>\n",
    "> **依存句法分析**：解析句子中词语之间的依存关系。<br><br>\n",
    "> **核心指代消解**：确定文本中的代词或名词短语指的是什么。\n",
    "\n",
    "对这类有标签的经典任务，RNN的行为与CNN、DNN相似，会一次性判断出所有样本的标签。此时，**输出层的神经元数量通常是标签的类别数量**，例如具体有多少分类、具体有多少种关系、具体有多少种词性等等。所使用的损失也是相对常规的损失函数，例如交叉熵损失、Hinge损失（Hinge Loss）、负对数似然损失（Negative Log Likelihood Loss），特别的，如果是标注任务或NER任务，我们还可能使用Dice损失、条件随机场CRF损失（Conditional Random Fields Loss）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e866bb-6576-4fab-a2c0-00f086002e54",
   "metadata": {},
   "source": [
    "2. 其他基于时序/语义的**分类/回归**任务，包括但不限于——\n",
    "> **文本匹配和相似度计算**：如自动问答评分、重复问题检测等。<br><br>\n",
    "> **时序任务**：预测趋势变化（温度变化、股价变化）、根据历史行为预测未来行为变化（特定时间点下，用户是否购买、是否留存、是否点击）等等。\n",
    "\n",
    "在这一类任务中，输出层神经元的情况是相当灵活的。例如，如果是对趋势进行预测的时序问题，在采用单步预测方法时，可能一次性输出未来所有时间点上的预测值，此时输出层上可能会有test_size个神经元；在采用多步预测方法时，一次只能预测一个值，则可能输出层上只有1个神经元。如果要一次性输出一个时间点下的多个行为变化（例如，输出下午4点的空气适度、温度、风向等指标），那神经元的数量可能与要输出的类别数量一致。整体来说，这个类别的任务在输出层面时比较简单的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face0aca-0eca-4c8a-8840-60c375fca996",
   "metadata": {},
   "source": [
    "3. 序列到序列、或对序列进行补充或回应的**生成式**任务，包括但不限于——\n",
    "> **依据指令进行文本生成**：如生成诗歌、生成故事、生成一段话，对现有的内容进行续写等等。<br><br>\n",
    "> **图-文生成、语音识别**：根据输入的语音数据、图像数据生成描述、进行语音识别等等。<br><br>\n",
    "> **问答系统或对话生成**：对用户的问题给出直接的答案，或针对用户给出的关键词生成一系列自问自答的对话等等<br><br>\n",
    "> **摘要生成**：从长文本中生成简短的摘要、写总结、提取信息。<br><br>\n",
    "> **机器翻译**：将一种语言翻译成另一种语言。\n",
    "\n",
    "尽管NLP世界的预测任务都有非常专业的流程，但总体来说生成式任务的流程比分类/回归任务要复杂得多。在生成式任务中，RNN需要一个字、一个字、或一个词一个词地进行生成，在多次生成中逐渐构建出一个完整的句子或段落（所以你可能会观察到，ChatGPT这样的产品在说话的时候是一个词一个词往外蹦），所以生成式RNN的输出层和分类任务中的输出层有很大的区别。\n",
    "\n",
    "首先，NLP算法的生成并不能“无中生有”，模型只能从它曾经见过的字/词/短语中挑选它认为当下最能在语义上自洽的字/词/短语来进行输出，所以生成的本质是“在模型曾见过的字/词/短语中，挑选出最有可能使句子语义自洽的那个字/词/短语”。在进行实际生成时，模型会对它所见过的**每个字/词/短语**都输出一个概率，预测这个字/词/短语是当前最佳输出的可能性，再从中挑选出可能性最高或较高的词进行输出。因此**本质上来说，生成模型是多分类概率模型**。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/05.png)\n",
    "\n",
    "那模型曾见过的全部字/词/短语都在哪里呢？我们之前提到，在NLP数据进入模型之前，首先要进行分词和编码，这两个步骤会为我们构建当前训练集上所有字/词/短语对应的完整词汇表。此时，词汇表的大小vocab_size就是生成模型多分类的类别数量，因此**在生成式任务中，输出层神经元数量原则上需要与词汇表的大小一致**——假设训练集经过分词后有2w个不重复的字/词/短语，那输出层上就需要2w个神经元。此时，每个神经元对应着一个可能的字符/词，这个神经元的输出代表了该位置对应的词或字符被模型预测为下一个词或字符的概率，模型再选择概率最高或较高的词进行输出。\n",
    "\n",
    "![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/627d1242313db8d13b355616_60f040c91fd9cd1e45773b93_Neural-Network-Architectures-Cover%2520(1).png)\n",
    "\n",
    "所以不难发现，**一个生成式模型想要进行灵活、丰富的文字生成，就必须先见过巨量文字数据**。同时，分词越细致，生成式模型在生成时的创造力就会越强。如果模型是基于字符级别的，那么输出层的神经元数量就是所有可能字符的数量，模型就可能基于字符构建新的词语。如果模型是基于词级别的，那么输出层的神经元数量就是词汇表中所有词的数量，模型就可能基于词语构建新的短语。但这样，模型自然对数据量、算力就会有更高的要求。\n",
    "\n",
    "当然，在实际进行生成式任务时，我们会有很多手段来结局生成式模型中的问题——例如，每次预测下一个字时都必须对全部的数十万个字/单词生成概率，那计算效率实在是会过低；同时，如果模型只能生成“曾经见过”的词/短语的话，那远远达不到具备“认知智能”的水准，因此业内也有相当多的手段用于增强语言模型的创造力和泛化能力。在后续我们讲解Transformer的生成式案例时，我们将会详细展开来聊聊这些技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe737c0-5f02-4823-803f-9b1ad25a78a7",
   "metadata": {},
   "source": [
    "## 3.4 在PyTorch中实现循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e38dd7-0205-44a0-9635-d7c3535fb9a4",
   "metadata": {},
   "source": [
    "在了解循环神经网络的基本原理之后，我们可以借助PyTorch来实现循环神经网络了。在之前的课程当中，我们已经认识了PyTorch框架的基本结构，整个PyTorch框架可以大致被分Torch和成熟AI领域两大板块，其中Torch包含各类神经网络组成元素、用于构建各类神经网络，各类AI领域中则包括Torchvision、Torchtext、Torchaudio等辅助完成图像、文字、语音方面各类任务的领域模块。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/24.png)\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80e195-8e11-4123-b731-8a0ce4bca579",
   "metadata": {},
   "source": [
    "在NLP入门的世界中，我们常常用到的是torch.nn下各类经典元素：\n",
    "\n",
    "- **词嵌入层Embedding Layers**: torch.nn.Embedding: 用于将离散的词汇ID转换为连续的词向量。注意，普通的Embedding层与Word2Vec，GloVe等方法有巨大的区别。\n",
    "\n",
    "- **用于处理序列数据的各类神经网络层**: torch.nn.RNN, torch.nn.LSTM, 和 torch.nn.GRU等。当然，对RNN来说，我们可以使用nn.Linear来替代nn.RNN，毕竟二者的网络结构是完全一样的，只要将数据流设置正确，torch.nn.RNN和torch.nn.Linear有时可以实现完全一致的效果。\n",
    "\n",
    "- **服务于Transformer架构的各类神经网络层和模型**：例如nn.Transformer、nn.TransformerEncoder、nn.TransformerDecoder、nn.TransformerEncoderLayer、nn.TransformerDecoderLayer、nn.MultiheadAttention等。\n",
    "\n",
    "- **在文本任务中常常出现的各类损失函数**：例如，在各种NLP任务场景下都可以使用的交叉熵损失torch.nn.CrossEntropyLoss、均方误差nn.MSELoss、合页损失nn.MarginRankingLoss等损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b9a72-e698-44e2-b691-cdbc26e893e5",
   "metadata": {},
   "source": [
    "在今天的课程中，我们要详细介绍的是nn.RNN类——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b7da7-47b5-491b-ad80-530e2fe04aa0",
   "metadata": {},
   "source": [
    "`class torch.nn.RNN(input_size, hidden_size, num_layers, nonlinearity, bias, batch_first, dropout, bidirectional, *args, **kwargs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9921c-6bcb-47c7-a6ff-eb08bcc414fd",
   "metadata": {},
   "source": [
    "不难发现，这是一个参数很少、相当简单的类。虽然该类的名字叫做循环神经网络，但实际上**nn.RNN是循环层**，它是在线性层的基础上改进后的层，除了线性层的基本功能（匹配权重、神经元结果加和、激活函数、向前向后传播等）外，循环层还负责执行我们在讲解RNN的原理时提到的“将上个时间步的中间变量传递给下个时间步”的功能。我们来看看这些nn.RNN的几个重要参数：\n",
    "\n",
    "- `input_size`: 输入特征的数量，也是输入层的神经元数量。\n",
    "\n",
    "- `hidden_size`: 隐藏层的神经元数量，也是隐藏状态h的特征数量。\n",
    "\n",
    "- `nonlinearity`: 激活函数，可选择 'tanh' 或 'relu'，这是RNN论文中默认的两个选项。\n",
    "\n",
    "- `batch_first`: 如果为True，输入和输出Tensor的形状为 [batch_size, seq_len, input_dimension]，否则为[seq_len, batch_size, input_dimension]。当数据是时间序列数据时，seq_len是time_step，当数据是文字序列时，seq_len就是vocab_size。注意，默认值为False，所以pytorch官方所使用的结构是[seq_len, batch_size, input_dimension]。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/22.png)\n",
    "\n",
    "同时，RNN类有两个输出，一个是`output`，另一个是`hn`——\n",
    "\n",
    "- `output`：代表所有时间步上最后一个隐藏层上输出的隐藏状态的集合。很显然，对单层神经网络来说，output代表了唯一一个隐藏层上的隐藏状态。output的形状都为 [seq_len, batch_size, hidden_size]，不受隐藏层数量的影响。\n",
    "\n",
    "- `hn`: 最后一个时间步的、所有隐藏层上的隐藏状态。形状为 [num_layers, batch_size, hidden_size]。\n",
    "\n",
    "不难发现，虽然每个时间步、每个隐藏层都会输出隐藏状态，但是RNN类却只会“选择性”地帮助我们输出部分隐藏状态，例如output在深层神经网络中只会保留最后一个隐藏层的隐藏状态，而hn则是只显示最后一个时间步的隐藏状态。通常来说当我们在实现rnn时，**所有隐藏状态都是需要向下一个时间步、以及下一个隐藏层传递的**，因此output和hn都不能代表nn.RNN层在循环中所传输的信息。\n",
    "\n",
    "当然，在很多情况下，我们确实也只会关心最后一个时间步的隐藏状态，或者最后一个隐藏层的隐藏状态。output关注全部时间步，hn关注全部隐藏层，在NLP经典任务当中——\n",
    "\n",
    "- 如果我们需要执行对**每一个时间步**进行预测的任务（比如，预测每一分钟的股价，预测明天是否会下雨，预测每个单词的情感倾向，预测每个单词的词性），此时我们就会关注每个时间步在最后一个隐藏层上的输出。此时我们要关注的是整个output。\n",
    "\n",
    "- 如果我们需要执行的是对**每张表单、每个句子**进行预测的任务时（比如对句子进行情感分类，预测某个时间段内用户的行为），我们就只会关注最后一个时间步的输出。此时我们更可能使用hn。\n",
    "\n",
    "但需要注意的是，无论是output还是hn，都只是循环层的输出，不是循环神经网络真正的输出，**因为nn.RNN层缺乏关键性结构：输出层**。一般来说，我们不会把循环层的输出直接当作RNN的输出，但这个操作也不是完全不符合规定，毕竟很多时候，我们可能会结合RNN和其他更高级的神经网络，此时循环层的输出需要作为其他神经网络的输入来使用，那循环层本身也可以被认为是构建了一个“网络”本身，但这种定义是不严谨的，通常来说我们还是需要给循环神经网络加上一个输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778728fb-43e0-4553-acfa-86189941edeb",
   "metadata": {},
   "source": [
    "### 3.4.1 单层循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888c2ad6-bbf3-412e-9a90-c8cf07a43653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c4dc69-0c64-4731-95ca-094e5fc21cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#假设一个文字数据，序列长度为3（时间步为3）\n",
    "inputs = torch.randn((3,50,10)) #seq_len（vocal_size），batch_size，input_dimension\n",
    "rnn1 = nn.RNN(input_size=10,hidden_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705b158f-0dc6-4648-a4df-de262dee1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1, hn1 = rnn1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b198e4-0c02-4047-b40c-085c4c014b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#包含了全部时间步上的隐藏状态\n",
    "#seq_len（vocal_size），batch_size，hidden_size\n",
    "outputs1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c2d0b-831c-46ec-b28e-0240f3055c8b",
   "metadata": {},
   "source": [
    "原本输入的10个特征变成了隐藏层的20个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a87b8bcb-ed0c-41f8-b632-33e571646df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn1.shape #因为只有1个隐藏层，所以输出了这个隐藏层上全部50个batch下、20个神经元上的隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e83f48e-2664-40f6-8907-3ead48da7007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5402,  0.1220, -0.4039, -0.7486, -0.1939,  0.4193,  0.1498,  0.1766,\n",
       "         0.2598,  0.4682, -0.6524,  0.2306,  0.4266, -0.0413, -0.7277, -0.2280,\n",
       "        -0.4804, -0.4672,  0.2444,  0.2189], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对outputs，我们可以索引出最后一个时间步上、第一个batch_size中的隐藏状态\n",
    "outputs1[-1,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c2f567-3a2b-4438-9da2-9dfe4c2e31d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5402,  0.1220, -0.4039, -0.7486, -0.1939,  0.4193,  0.1498,  0.1766,\n",
       "          0.2598,  0.4682, -0.6524,  0.2306,  0.4266, -0.0413, -0.7277, -0.2280,\n",
       "         -0.4804, -0.4672,  0.2444,  0.2189]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#不难发现，与h1中的第一个batch_size上的隐藏状态一样\n",
    "hn1[:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c088595-a634-434d-bfc7-6ed50ba9d900",
   "metadata": {},
   "source": [
    "既然h1可以被从outputs中索引出来，为什么nn.RNN层还要单独设置hn作为输出呢？在许多任务中，我们可能会只关注RNN的最后一个时间步输出的隐藏状态（例如，在对整个句子进行预测时，在执行生成式任务时），因此从工程出发，PyTorch设计了单独的hn输出，希望更直观地表达出最后的隐藏状态，而省略让用户自己提取的这个过程。这种设计也会更方便一些复杂的架构，如双向的RNN，多层的RNN来管理数据流，毕竟在这些复杂的RNN中，直接调用hn会比outputs更加便捷。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c840d2-7e99-4676-9db6-bc1b319750a7",
   "metadata": {},
   "source": [
    "请注意，在运行上述nn.RNN的过程中已经自动执行了3次循环（经历了三个时间步），虽然从输出结果完全看不出来，但我们可以尝试调大seq_len的数字来增加循环的次数，通过rnn层运行的时间来判断循环是否真的在进行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a243c4b-3f44-453a-b914-eba77d20de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037300586700439453\n"
     ]
    }
   ],
   "source": [
    "rnn1 = nn.RNN(input_size=10,hidden_size=20)\n",
    "\n",
    "#创建新的输入\n",
    "inputs1 = torch.randn((3,50,10))\n",
    "\n",
    "#此时seq_len（volca_size）是3，因此会循环3次\n",
    "start = time.time()\n",
    "outputs1, hn1 = rnn1(inputs1)\n",
    "spend = time.time() - start\n",
    "print(spend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511c8bcb-b0f1-4001-820a-d80c5ffb3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6146790981292725\n"
     ]
    }
   ],
   "source": [
    "#网络结构完全不变化，但是数据的seq_len由原本的3增加到3000\n",
    "inputs2 = torch.randn((3000,50,10))\n",
    "\n",
    "#此时seq_len（volca_size）是3000，因此会循环3000次\n",
    "start = time.time()\n",
    "outputs2, hn2 = rnn1(inputs2)\n",
    "spend = time.time() - start\n",
    "print(spend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376bcf6-4dca-4066-b5b6-2f3b83dce2f4",
   "metadata": {},
   "source": [
    "可见，虽然代码上看不出实际的循环过程，但nn.RNN层是真的在进行seq_len方向的循环。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607200bb-8efe-4f00-8a69-e3b8e723f572",
   "metadata": {},
   "source": [
    "### 3.4.2 深度循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b534f7-233e-41bd-8007-c18f476bb067",
   "metadata": {},
   "source": [
    "和普通的循环神经网络比起来，深度循环神经网络是指不只有一个隐藏层的神经网络。在这里，我们需要来查看nn.RNN两个关键的参数：\n",
    "\n",
    "- `num_layers`: 隐藏层的层数，不过这里的隐藏层除了向下一个隐藏层输送数据外，还需要向下一个时间步上的同层的隐藏层输送数据。\n",
    "\n",
    "- `hidden_size`: 隐藏层的神经元数量，也是隐藏状态h的特征数量。注意，无论有多少隐藏层，当前参数都只能填写一个数字，**这说明在PyTorch中，循环神经网络的每一层隐藏层的尺寸都是相同的**，在实践情况下这种设计可以满足大部分的需求。从实践的角度看，当我们有多个堆叠的RNN层时，为每一层都设定一个不同的隐藏层大小可能会让模型的设计和维护变得复杂（也很没必要）。因此，一般的做法是为所有的RNN层使用相同的隐藏单元数。这简化了模型的设计和超参数调整。当然，从RNN的原理角度来说，我们可以自由设置每一个隐藏层的神经元数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38a6f39a-58c6-4507-84dc-324813d6ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果要实现多层的循环神经网络，outputs结构会如何变化呢？\n",
    "#依然是假设一个文字数据，序列长度为3\n",
    "inputs3 = torch.randn((3,50,10)) #seq_len（vocal_size），batch_size，input_dimension\n",
    "drnn1 = nn.RNN(input_size=10,num_layers = 4, hidden_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d900bb2-1ecc-45d4-88cd-cb35e3181ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3, hn3 = drnn1(inputs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "763b4a39-a3a4-440e-93f2-1ba684a00ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 20])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3.shape #结构依然是seq_len（vocal_size），batch_size，input_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06158568-d26a-4e98-9525-af54ea14affb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 20])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn3.shape #因为有4个隐藏层，所以会输出4个隐藏层上的hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "446b4e06-3896-4d53-8b08-e0157aeddde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4194,  0.0468,  0.1088, -0.3220, -0.0857, -0.4593,  0.5587, -0.2238,\n",
       "         -0.5314, -0.6424, -0.2880,  0.5217,  0.3579,  0.5119,  0.3305, -0.1822,\n",
       "         -0.6506, -0.1889, -0.2999, -0.2126],\n",
       "        [-0.1154,  0.0176, -0.0772,  0.2337, -0.1694, -0.5166, -0.1711, -0.1378,\n",
       "          0.3393,  0.4530,  0.5087,  0.2500, -0.2603,  0.2665,  0.1589,  0.2296,\n",
       "          0.3555,  0.2313,  0.0092,  0.0066],\n",
       "        [ 0.0661,  0.3032, -0.0243,  0.0259, -0.0574, -0.2458,  0.0815, -0.2073,\n",
       "          0.0023, -0.0416, -0.3574, -0.2818,  0.0363,  0.0177, -0.1293, -0.1047,\n",
       "          0.2732,  0.1527,  0.2878, -0.1096],\n",
       "        [ 0.3548,  0.4097, -0.2139, -0.1851,  0.0168,  0.1550,  0.1378, -0.0911,\n",
       "          0.3059, -0.0197,  0.4082, -0.1166, -0.0847,  0.5665, -0.1688,  0.3729,\n",
       "          0.0983, -0.0215, -0.1026,  0.0740]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn3[:,0,:] #提取出全部隐藏层上、最后一个batch的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b12cd660-c418-4b44-8e47-3acab1008381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3548,  0.4097, -0.2139, -0.1851,  0.0168,  0.1550,  0.1378, -0.0911,\n",
       "         0.3059, -0.0197,  0.4082, -0.1166, -0.0847,  0.5665, -0.1688,  0.3729,\n",
       "         0.0983, -0.0215, -0.1026,  0.0740], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn3[-1,0,:] #提取出最后一个隐藏层上、第一个batch的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5820064-069e-48f9-bba2-ebe2bf6fee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3548,  0.4097, -0.2139, -0.1851,  0.0168,  0.1550,  0.1378, -0.0911,\n",
       "         0.3059, -0.0197,  0.4082, -0.1166, -0.0847,  0.5665, -0.1688,  0.3729,\n",
       "         0.0983, -0.0215, -0.1026,  0.0740], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#提取出最后一个时间步上，最后一个batch的输出\n",
    "#但由于outputs只展示最后一个隐藏层，所以提取出的结果与hn提取出的结果是一致的\n",
    "output3[-1,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a322a-9d2f-491c-b519-b11a42a1e56c",
   "metadata": {},
   "source": [
    "接下来就让我们使用nn.RNN来实现一个简单的深度循环神经网络，我们设置输入层为100个神经元，输出层为3个神经元（假设这是一个最为单纯的三分类任务，需要对每个句子进行情感分类），其中每个隐藏层都有256个神经元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f483549-4c43-4b6f-a841-835bccfa7fc7",
   "metadata": {},
   "source": [
    "> 对句子层面进行预测（关注最后一个时间步上的输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4724101d-d33f-4845-b7b7-e16cbc04e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class myRNN(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=256\n",
    "                 , num_layers=4, output_size=3):\n",
    "        super(myRNN, self).__init__()\n",
    "        \n",
    "        # 创建一个RNN模块，里面自然就包含了4个隐藏层\n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers)\n",
    "        \n",
    "        # 输出层是需要单独建立的\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的形状: (seq_length, batch_size, input_size)\n",
    "        \n",
    "        # 传入x\n",
    "        output, hn = self.rnn(x)\n",
    "        \n",
    "        # 我们关心的是最后一个时间步的输出，可以从output中索引\n",
    "        predict = self.fc(output[-1, :, :])\n",
    "        \n",
    "        # 也可以从hn中索引\n",
    "        #predict = self.fc(hn[-1,:,:])\n",
    "        \n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5cdc02a-4a35-4b43-bc33-2c925ef3bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myRNN(\n",
      "  (rnn): RNN(100, 256, num_layers=4)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型，不难发现，从网络结构中是看不出4个隐藏层的，只能观察参数num_layers\n",
    "model = myRNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a6f08e-379f-46d4-bd6c-4940ecbe09a0",
   "metadata": {},
   "source": [
    "假设我现在需要对每个单词/每个时间步都进行情感分类：\n",
    "\n",
    "> 对时间步层面进行预测（关注全部时间步上的输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee8fe802-d5bd-43cd-8f0a-5337b006c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class myRNN2(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=256\n",
    "                 , num_layers=4, output_size=3):\n",
    "        super(myRNN2, self).__init__()\n",
    "        \n",
    "        #需要先定义好在forward中要使用的属性\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 创建一个RNN模块，里面自然就包含了4个隐藏层\n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers)\n",
    "        \n",
    "        # 输出层是需要单独建立的\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的形状: (seq_length, batch_size, input_size)\n",
    "        # output的形状: (seq_length, batch_size, hidden_size)\n",
    "        \n",
    "        # 传入x\n",
    "        output, _ = self.rnn(x)\n",
    "        \n",
    "        # 我们关心的是全部时间步上的输出\n",
    "        output_resize = output.reshape(output.shape[0] * output.shape[1], self.hidden_size)\n",
    "        #output_resize = output.reshape(-1, self.hidden_size)\n",
    "        predict = self.fc(output_resize)\n",
    "        \n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51e0b917-cb2d-4b91-a39b-8d821b71e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myRNN2(\n",
      "  (rnn): RNN(100, 256, num_layers=4)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = myRNN2()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9cf306-1699-413f-989e-32ac005637cb",
   "metadata": {},
   "source": [
    "> 设置hn的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c8e04cb-9092-4028-a7ea-7b5ef5badb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class myRNN3(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=256\n",
    "                 , num_layers=4, output_size=3):\n",
    "        super(myRNN3, self).__init__()\n",
    "        \n",
    "        #为了传入h0，需要先定义好构建h0结构的两个属性\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的形状: (batch_size, seq_length, input_size)\n",
    "        # h0的形状: (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        #初始化h0\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # 同时输入x和hn\n",
    "        output, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # 假设我们关心的是最后一个时间步的输出\n",
    "        predict = self.fc(output[-1, :, :])\n",
    "        \n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d8980-b15b-450a-a0b6-1f355540f9c6",
   "metadata": {},
   "source": [
    "> 实现每个隐藏层上神经元数量不一致的DRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bcb4f8-cc5e-48b4-a91e-25e4a3640517",
   "metadata": {},
   "source": [
    "虽说大部分时候，循环神经网络内部的隐藏层上神经元数量都是一致的，但如果我们要求神经元数量不一致呢？例如，在全部的隐藏层中，前2个隐藏层为256个神经元，后两个隐藏层为512个神经元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c28bcb69-a52e-4b9d-a1b7-8f4c14285260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class myRNN4(nn.Module):\n",
    "    def __init__(self, input_size=100\n",
    "                 , hidden_sizes=[256, 256, 512, 512]\n",
    "                 , output_size=3):\n",
    "        super(myRNN4, self).__init__()\n",
    "        \n",
    "        # 定义4个不同的rnn层\n",
    "        self.rnn1 = nn.RNN(input_size, hidden_sizes[0])\n",
    "        self.rnn2 = nn.RNN(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.rnn3 = nn.RNN(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.rnn4 = nn.RNN(hidden_sizes[2], hidden_sizes[3])\n",
    "        \n",
    "        # 定义输出层\n",
    "        self.linear = nn.Linear(hidden_sizes[3], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始化h0，需要对4个隐藏层都初始化\n",
    "        # 但由于此时4个隐藏层是分开的，且神经元数量不同，因此需要分开初始化\n",
    "        h0 = [torch.zeros(1, x.size(0), self.hidden_size[0])\n",
    "             ,torch.zeros(1, x.size(0), self.hidden_size[1])\n",
    "             ,torch.zeros(1, x.size(0), self.hidden_size[2])\n",
    "             ,torch.zeros(1, x.size(0), self.hidden_size[3])]\n",
    "        \n",
    "        # 让输出的x不断进入下一个rnn层\n",
    "        # 注意这里我们手动实现循环，每个隐藏层之间是分开的\n",
    "        # 因此我们看得更加清晰；\n",
    "        # 我们是先完成每个时间步上的循环，再将该循环信息传递给下一个隐藏层\n",
    "        output1, _ = self.rnn1(x,h0[0])\n",
    "        output2, _ = self.rnn2(output1,h0[1])\n",
    "        output3, _ = self.rnn3(output2,h0[2])\n",
    "        output4, _ = self.rnn4(output3,h0[3])\n",
    "        \n",
    "        # 取出最后一个时间步上的结果\n",
    "        output = self.linear(output4[-1, :, :])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48192d54-9210-40c5-a1f8-eb7f6ee5e42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myRNN4(\n",
      "  (rnn1): RNN(100, 256)\n",
      "  (rnn2): RNN(256, 256)\n",
      "  (rnn3): RNN(256, 512)\n",
      "  (rnn4): RNN(512, 512)\n",
      "  (linear): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#这样就能够看出4个隐藏层了\n",
    "model = myRNN4()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4549f-8308-41aa-bcdb-6b8986381487",
   "metadata": {},
   "source": [
    "> 在nn.RNN中，各层的循环究竟是如何完成的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b28ef22e-c52b-4a9d-926a-97beac1ab45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class myRNN(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=256, num_layers=4, output_size=3):\n",
    "        super(myRNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        # 在这里，虽然我们使用一行代码完成了全部时间步、全部隐藏层上的循环\n",
    "        # 但实际上，rnn内部是“先纵再横”的规则\n",
    "        # 即先完成第一个隐藏层上的全部时间步循环\n",
    "        # 再将全部中间变量传递给第二个隐藏层\n",
    "        # 再完成下一个隐藏层上的全部时间步，以此类推这样来循环的\n",
    "        output, hn = self.rnn(x)\n",
    "        \n",
    "        predict = self.fc(output[-1, :, :])\n",
    "        \n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad7141-e36a-4e43-87ad-a0e6261d9d3c",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f6b46-4508-4329-8cac-19228a97e0e8",
   "metadata": {},
   "source": [
    "### 3.4.3 双向循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f1ea0-4209-4593-b106-92c748ca4782",
   "metadata": {},
   "source": [
    "双向循环神经网络（Bi-directional RNN，简称BiRNN）是循环神经网络（RNN）的一种扩展，专为捕获序列数据中的前后依赖关系而设计。传统的RNN由于其结构特点，往往只能捕获序列中前向的时序信息，而忽略了后向的上下文，但在文字数据处理过程中，影响一个词语语义的关键信息可能不止在这个词语的前方，也可能在这个词语的后方，而整个句子的含义可能也会因后续的信息发生变化，来看一个非常简单的例子：\n",
    "\n",
    "> **“他站在山巅，一眼望去尽是蜿蜒的河川和广袤的森林。”**在这个句子中，山巅一词很可能是指一座真正的山峰的山顶。<br><br>\n",
    "> **“他站在山巅，这个时代之中再也没有人可以与他匹敌。”**在这个句子中，山巅一词很可能只是一个比喻，而不是指真正的山峰。\n",
    "\n",
    "这样的例子还有非常多，除了前文之外，后文也是严重影响语义理解的关键，传统的RNN无法解决这个问题，因此双向网络BiRNN应运而生。\n",
    "\n",
    "BiRNN的核心思想是同时在序列的两个方向上运行两个独立的RNN。其中一个RNN按正常顺序处理输入序列，就像我们在之前的可成中讲解的那样，从第一个词语开始一个一个、从前向后将词语输入网络；而另一个RNN则从句子的最末端开始，从最后一个词语开始，一个一个、从后向前地将词语输入网络。这意味着，对于序列中的任意一个时间点，BiRNN都可以捕获其前向和后向的信息。这两个方向上的RNN输出随后通常会被联结或合并，以形成一个统一的输出表示。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/07_.png)\n",
    "\n",
    "通常来说，我们会看到双向循环网络的数据流结构如下：\n",
    "\n",
    "![](http://img.xjishu.com/img/zl/2019/6/13/2243756543093.gif)\n",
    "\n",
    "需要注意的是，虽然架构图上展示的过程似乎表示正向和反向的循环过程是同时进行的，但在实际代码实现过程中，正向和反向循环网络是按顺序进行的，没有任何并行处理。虽然从理论上说，正向和反向RNN在每个时间步都是独立的（即它们不需要相互的输出作为输入），但在大多数深度学习框架的实现中，双向RNN是按照上述顺序分步骤执行的，主要是为了代码的简洁和效率。一般来说，正向的循环结束之后，反向的循环才会开始，因此BiRNN的运行时间是更长的。\n",
    "\n",
    "在实际应用中，双向RNN与其他深度学习结构（如LSTM或GRU）的结合已经显示出了卓越的性能，特别是在序列标注、文本分类和机器翻译等任务中。但是很明显，与传统RNN相比，双向RNN的计算需求成倍增加，所以双向RNN所带来的性能提升也不是完全无代价的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dd458-c320-4d06-8c64-55dd2e02eb24",
   "metadata": {},
   "source": [
    "幸运的是，在PyTorc中，我们只需要调节一个参数就能够让普通RNN变为双向的RNN，来看参数：\n",
    "\n",
    "- `bidirectional`: 该参数控制循环的方向，如果设置为True，则当前循环网络是一个双向循环网络。此时在每个隐藏层上，都会有两个独立的RNN数据流：一个正向数据流的和一个反向的数据流。正向数据流从序列的开始到结束进行循环，而反向数据流则从序列的结束到开始进行计算。\n",
    "\n",
    "双向RNN的输出和隐藏状态的维度与单向RNN有所不同，当我们建立一个双向的RNN时，RNN层的输出output的形状会变为 (seq_len, batch_size, 2 * hidden_size)，其中2 * hidden_size包括正向和反向RNN的两组输出。对于隐藏状态hn，其形状为 (2 * num_layers, batch_size, hidden_size)，这里的2 * num_layers也是由于正向和反向的两个RNN。\n",
    "\n",
    "我们在代码中来看一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abf80a69-c419-4caa-a8bf-365563e4afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs4 = torch.randn((3,50,10)) #seq_len（vocal_size），batch_size，input_dimension\n",
    "brnn = nn.RNN(input_size=10,hidden_size=20,bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb63073d-6709-42f0-bf0d-1033910c24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs4, hn4 = brnn(inputs4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24c7eb89-4242-43ba-8162-a75386750682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 40])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs4.shape #对每个时间步、每个batch都生成了20 * 2个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4abc7b03-7d0f-4aae-a55c-77470d1772d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 20])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn4.shape #隐藏层数量翻倍，同时生成的隐藏状态数量也翻倍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7997d46-47c5-4d77-8dbb-f12053b7050c",
   "metadata": {},
   "source": [
    "现在我们已经知道RNN在代码中如何实现了，你已经可以使用循环层来构建各类深度循环神经网络。通常来说我们的下一步是对循环神经网络进行训练，但作为NLP领域的入门算法，RNN在解决实际问题时的表现很难让人满意，对RNN算法进行单独的训练并不是一个高效的做法，因此我们将不会在课程中特地安排RNN地训练流程，而是会对后续的LSTM、Transformer等算法进行更详细的训练流程讲解。在后续的篇章中，我们将详细展开聊聊RNN的缺陷和问题，让我们了解它究竟为什么无法应对现实中的复杂问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e752022-2f8b-4d0d-a34f-e32e4e47f1b4",
   "metadata": {},
   "source": [
    "## 3.5 RNN的反向传播与缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d83abb-a3c2-457a-8903-90f7c1a964ba",
   "metadata": {},
   "source": [
    "自诞生以来，循环神经网络（RNN）一直在许多序列任务中都表现出色，它的独特结构使其能够捕捉时间序列数据中样本与样本之间的依赖关系，但它也存在一些固有的、不可克服的缺陷，例如容易发生梯度消失、梯度爆炸、不擅长长期记忆等等。这些问题不仅限于理论层面，而且在实际应用中也反复出现，严重影响了模型的实际效果和训练的稳定性。对于深度学习的从业者来说，理解这些缺陷非常重要，因为它决定了RNN是否适合特定的任务，同时也影响我们对于后续改进方案（如LSTM、transformer等算法）的理解。在本部分，我们将深入探讨RNN的这些局限性，以及它们如何影响模型的性能。\n",
    "\n",
    "要讨论RNN的局限性，就必然要讨论RNN的反向传播过程，因为RNN在实际应用中的问题基本都和它异常复杂的反向传播过程有关。反向传播是神经网络优化迭代过程中的关键过程，它基于链式法则来计算损失函数$L$相对于网络权重$W$的梯度，并用这些梯度优化、迭代网络中的权重，以辅助梯度下降、Adam等优化算法的完成。<font color=\"red\">请注意，掌握基础求导公式、掌握深度神经网络的反向传播过程、链式法则等基本知识是能够顺利学习本节课的关键，如果你已经对这些信息感到陌生、或对反向传播的细节了解并不清晰，请回顾Lesson10、Lesson11的课程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2a73c-0889-4f58-bfdd-858ed192cee1",
   "metadata": {},
   "source": [
    "### 3.5.1 反向传播的数学过程\n",
    "    \n",
    "首先，由于循环层的存在，**RNN数据流存在两个方向**：第一个方向是与普通神经网络一致的【输入层-隐藏层-输出层】方向，第二个维度则是沿着时间步进行传播的【输入-t时刻隐藏层-t+1时刻隐藏层】方向。由于反向传播是正向传播的逆过程，因此循环神经网络在反向传播上也同样有两个方向：一个是从与普通神经网络一致的【输出层-隐藏层-输入层】方向，另一个则是循环网络独特的【输出-t时刻隐藏层-t+1时刻隐藏层】的方向。这种同时在两个方向上进行反向传播的方式被称之为“通过时间的反向传播”（Backpropagation Through Time，BPTT），是循环神经网络的特色之一。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410119ad-8f9c-4707-960e-abc476ae03e3",
   "metadata": {},
   "source": [
    "通过时间的反向传播（BPTT）比一般的反向传播要更为复杂，不过它也是从损失函数开始不断向各级的权重$W$求导、并利用导数来迭代权重的过程。我们来仔细看一下它的具体流程：\n",
    "\n",
    "- **明确NLP任务，明确标签输出的方式**\n",
    "\n",
    "在之前的课程中我们提到，在不同类型NLP任务会有不同的输出层结构、会有不同的标签输出方式。例如，在对词语/样本进行预测的任务中（情感分类、词性标注、时间序列等任务），RNN**会在每个时间步都输出词语对应的相应预测标签**；但是，在对句子进行预测的任务中（例如，生成式任务、seq2seq的任务、或以句子为单位进行标注、分类的任务），RNN很可能**只会在最后一个时间步输出句子相对应的预测标签**。输出标签的方式不同，反向传播的流程自然会有所区别。考虑到生成式任务的多样性和复杂性，在这里我们使用最简单的词语分类任务作为例子来为大家讲解RNN反向传播中的各种特点与问题。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccf8a8-abb2-4ebc-97ea-091aded3c8f2",
   "metadata": {},
   "source": [
    "- **正向/反向传播的数学过程**\n",
    "\n",
    "假设现在我们有一个最为简单的RNN，需要完成针对每个词语的情感分类任务。该RNN由输入层、一个隐藏层和一个输出层构成，全部层都没有截距项，总共循环$t$个时间步。该网络的输入数据为$X$，输出的预测标签为$\\hat{y}$，真实标签为$y$，激活函数为$\\sigma$，输入层与隐藏层之间的权重矩阵为$W_{xh}$，隐藏层与输出层之间的权重矩阵为$W_{hy}$，隐藏层与隐藏层之间的权重为$W_{hh}$，损失函数为$L(\\hat{y},y)$，t时刻的损失函数我们简写为$L_t$。此时，这个RNN的**正向传播过程**可以展示如下：\n",
    "\n",
    "> 时间步1，初始化$h_0$，并且初始化需要迭代的参数$W_{hh}$、${W}_{xh}$、${W}_{hy}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_1 &= \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_1 + \\mathbf{W}_{hh} \\mathbf{h}_0) \\\\ \\\\\n",
    "\\mathbf{\\hat{y}}_1 &= \\mathbf{W}_{hy} \\mathbf{h}_1 \\\\ \\\\\n",
    "L_1 &= L(\\mathbf{\\hat{y}}_1，\\mathbf{y}_1)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a203a15-0e52-4f41-a90b-3ac7ce18dad0",
   "metadata": {},
   "source": [
    "> 时间步2\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_2 &= \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_2 + \\mathbf{W}_{hh} \\color{red}{\\mathbf{h}_1}) \\\\\n",
    "& = \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_2 + \\mathbf{W}_{hh} \\color{red}{\\sigma(\\mathbf{W}_{xh} \\mathbf{X}_1 + \\mathbf{W}_{hh} \\mathbf{h}_0)}) \\\\ \\\\\n",
    "\\mathbf{\\hat{y}}_2 &= \\mathbf{W}_{hy} \\mathbf{h}_2 \\\\ \\\\\n",
    "L_2 &= L(\\mathbf{\\hat{y}}_2，\\mathbf{y}_2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f35ae-d047-42c1-887a-778767ee9197",
   "metadata": {},
   "source": [
    "> 时间步$t-1$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{t-1} &= \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\color{red}{\\mathbf{h}_{t-2}}) \\\\ \n",
    "& = \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\color{red}{\\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-2} + \\mathbf{W}_{hh} \\mathbf{h}_{t-3})}) \\\\ \\\\\n",
    "\\mathbf{\\hat{y}}_{t-1} &= \\mathbf{W}_{hy} \\mathbf{h}_{t-1} \\\\ \\\\ \n",
    "L_{t-1} &= L(\\mathbf{\\hat{y}}_{t-1}，\\mathbf{y}_{t-1})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e252d0-f1d6-432b-a312-4d6d355ec8d2",
   "metadata": {},
   "source": [
    "> 时间步$t$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{t} &= \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\color{red}{\\mathbf{h}_{t-1}}) \\\\\n",
    "& = \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_t + \\mathbf{W}_{hh} \\color{red}{\\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\mathbf{h}_{t-2})}) \\\\ \\\\ \n",
    "\\mathbf{\\hat{y}}_{t} &= \\mathbf{W}_{hy} \\mathbf{h}_{t} \\\\ \\\\ \n",
    "L_{t} &= L(\\mathbf{\\hat{y}}_{t}，\\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418181e9-6eeb-4519-99c0-37fb7ad3635e",
   "metadata": {},
   "source": [
    "不难发现，**RNN中存在至少三个权重矩阵需要迭代**：输入层与隐藏层之间的权重矩阵为$W_{xh}$，隐藏层与输出层之间的权重矩阵为$W_{hy}$，隐藏层与隐藏层之间的权重为$W_{hh}$。在循环神经网络中，我们**首先要完成全部时间步上的正向传播，才可以开始进行反向传播和参数迭代**，因此用于计算$h_1$的$W_{xh}$和$W_{hh}$与用于计算$h_t$的$W_{xh}$和$W_{hh}$是完全相同的权重矩阵。这是循环神经网络权值共享的关键，表面上看是所有表单共享一套参数，本质是所有时间步上的所有循环共享一套参数，无论走过多少时间步，我们使用的始终都是同一套$W_{hh}$、$W_{xh}$和$W_{hy}$。\n",
    "\n",
    "当完成正向传播后，我们需要在反向传播过程中对以上三个权重求解梯度、并迭代权重。反向传播是从最后的时间步开始，因此以最后的时间步t为例子，我们的反向传播过程为：\n",
    "\n",
    "> 时间步t，我们需要求解的三个梯度为：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "1）\\frac{\\partial L_{t}}{\\partial W_{hy}}\\\\ \\\\\n",
    "2.1）\\frac{\\partial L_{t}}{\\partial W_{xh}}\\\\ \\\\\n",
    "3.1）\\frac{\\partial L_{t}}{\\partial W_{hh}}\\\\ \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中，1、2.1是做用于传统深度神经网络的、做用于隐藏层和输入层、输出层之间的权重矩阵，3.1则是在循环神经网络的循环层上、作用于时间步方向的权重矩阵。根据之前的数学流程，$L_{t}$可以展开展示为：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{t} &= L(\\mathbf{\\hat{y}}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\mathbf{h}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\mathbf{h}_{t-1}), \\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "可见，$L_{t}$是以$\\hat{y}_{t}$为自变量的函数，$\\hat{y}_{t}$是以$W_{hy}$和$h_{t}$为自变量的函数，$h_{t}$又是以$W_{xh}$和$W_{hh}$为自变量的函数，因此要求解上面三个梯度，其实是需要对复合函数进行求导。根据链式法则规则，如果y = f(u)并且u = g(x)，那y直接对x求导的公式则可写成：\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "因此根据链式法则，我们有：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "1) \\ \\frac{\\partial L_{t}}{\\partial W_{hy}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial W_{hy}} \\\\ \\\\\n",
    "2.1) \\ \\frac{\\partial L_{t}}{\\partial W_{xh}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial W_{xh}} \\\\ \\\\\n",
    "3.1) \\  \\frac{\\partial L_{t}}{\\partial W_{hh}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial W_{hh}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4002b-7679-4268-bfb5-098829a0a034",
   "metadata": {},
   "source": [
    "好了，到这里为止循环神经网络的反向传播过程都与普通深度神经网络类似，但有的小伙伴可能已经注意到了，上面的公式2.1和3.1中存在一个关键问题，那就是$h_t$作为一个复合函数，不止能以$W_{xh}$和$W_{hh}$为自变量，还能以上层的隐藏状态$h_{t-1}$作为自变量，而$h_{t-1}$本身又是以$W_{xh}$和$W_{hh}$为自变量的函数：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{t} &=L(\\mathbf{W}_{hy} \\mathbf{h}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&= L(\\mathbf{W}_{hy} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\color{red}{\\mathbf{h}_{t-1}}), \\mathbf{y}_{t})  \\\\ \\\\\n",
    "&= L(\\mathbf{W}_{hy} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\color{red}{\\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\mathbf{h}_{t-2})},\\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30a559-2c16-4885-a7d7-054f8fabd28e",
   "metadata": {},
   "source": [
    "在之前我们提到过，由于循环神经网络有权值共享机制，因此用于计算$h_{t}$的$W_{xh}$和$W_{hh}$与用于计算$h_{t-1}$的$W_{xh}$和$W_{hh}$是完全相同的权重矩阵。如下面的公式所示，蓝色的部分是完全相同的矩阵，红色的部分也是完全相同的矩阵：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{t} &=L(\\mathbf{W}_{hy} \\mathbf{h}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&= L(\\mathbf{W}_{hy} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} {\\mathbf{h}_{t-1}}), \\mathbf{y}_{t})  \\\\ \\\\\n",
    "&= L(\\mathbf{W}_{hy} \\sigma(\\color{blue}{\\mathbf{W}_{xh}} \\mathbf{X}_{t} + \\color{red}{\\mathbf{W}_{hh}} \\sigma(\\color{blue}{\\mathbf{W}_{xh}} \\mathbf{X}_{t} + \\color{red}{\\mathbf{W}_{hh}} \\mathbf{h}_{t-2}),\\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9249f0-fedf-4819-8fee-2084f10f0655",
   "metadata": {},
   "source": [
    "此时你发现了吗？在求解$L_{t}$对$W_{xh}$和$W_{hh}$的导数时，不止可以求解上面所写的式子2.1和3.1，还可以继续对嵌套函数求解得到下面的梯度2.2和3.2——\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2.1) \\ \\frac{\\partial L_{t}}{\\partial W_{xh}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial W_{xh}} \\\\ \\\\\n",
    "\\color{red}{2.2)} \\ \\frac{\\partial L_{t}}{\\partial W_{xh}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\\\ \\\\\n",
    "3.1) \\  \\frac{\\partial L_{t}}{\\partial W_{hh}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial W_{hh}} \\\\ \\\\\n",
    "\\color{red}{3.2)} \\  \\frac{\\partial L_{t}}{\\partial W_{hh}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\\\ \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d79ef-b79a-4266-beab-45dd5b9a9ac4",
   "metadata": {},
   "source": [
    "甚至，我们还可以将$h_{t-1}$继续拆解为$\\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\mathbf{h}_{t-2})$，还可以将$h_{t-2}$继续拆解为$\\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-2} + \\mathbf{W}_{hh} \\mathbf{h}_{t-3})$，我们可以将嵌套函数无止尽地拆解下去，直到拆到$\\mathbf{h}_1 = \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_1 + \\mathbf{W}_{hh} \\mathbf{h}_0)$为止。在这个过程中，只要拆解足够多，我们可以从$L_{t}$求解出t个针对$W_{xh}$和$W_{hh}$的导数。因此惊人的事实是，<font color=\"red\">**在时间步t上，我们可以计算t个用于迭代$W_{xh}$和$W_{hh}$的梯度**！\n",
    "\n",
    "在整个RNN的反向传播过程中，每个时间步上可以计算的用于迭代$W_{xh}$和$W_{hh}$的梯度等于时间步的数值本身。在时间步t上，我们可以计算t个用于迭代$W_{xh}$和$W_{hh}$的梯度，在时间步t-1上，我们可以计算t-1个用于迭代$W_{xh}$和$W_{hh}$的梯度，因此有t个时间步时，我们可以计算的用与迭代$W_{xh}$和$W_{hh}$的梯度的梯度总数量为：\n",
    "\n",
    "$$t+(t−1)+(t−2)+⋯+1=\\frac{t(t-1)}{2}$$\n",
    "\n",
    "这是一个算数级数之和。那我们现在该如何使用这么多梯度来对$W_{hh}$进行迭代呢？通常来说，我们可以对这些梯度进行求和、求均值等聚合计算，并将聚合后的值用与迭代。以下是一个聚合的例子：\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta W_{hh} &= \\sum_{i=1}^{\\frac{t(t-1)}{2}}\\frac{\\partial L}{\\partial W_{hh}^{(i)}} \\\\ \\\\\n",
    "W_{hh} &= W_{hh} - \\eta \\cdot \\Delta W_{hh}\n",
    "\\end{align*}\n",
    "$$\n",
    "    \n",
    "其中，$\\eta$是迭代中的学习率，也可写做$\\alpha$等字母。以上迭代流程在每个batch会发生一次，假设现在分批之后每个batch中有100张表单，即数据格式（batch_size，seq_len，input_dimensions）中的第一个维度是100，那RNN在看完全部的100张表单后就会进行一次迭代。而总共会有多少次迭代呢？就看全数据集中有多少个batch、以及我们在训练中循环多少个epoch了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b310c6-8a0b-4941-aa15-a48a61e9d3e0",
   "metadata": {},
   "source": [
    "### 3.5.2 RNN的反向传播所带来的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16f708-9d4d-4523-9c3e-e40074685a63",
   "metadata": {},
   "source": [
    "作为NLP入门级经典算法，RNN处理序列数据的思路精彩非凡、值的借鉴，但同时它也有众多不可忽视的缺陷，而这些缺陷大多与它的复杂的反向传播过程有关，让我们来一一看看这些问题：\n",
    "\n",
    "- **极其容易发生梯度消失和梯度爆炸**\n",
    "\n",
    "梯度消失和梯度爆炸是神经网络在训练过程中很常见的问题之一，其中梯度消失是指随着迭代进行、权重的梯度变得越来越小，从而导致迭代失效的现象；梯度爆炸是指随着迭代进行、权重的梯度变得越来越大、从而导致迭代失效的现象。这种现象一般是由反向传播中的偏导数连乘引起的，但**相比起一般的深度学习算法，RNN算法更容易发生梯度消失和梯度爆炸现象**，因为RNN反向传播中的连乘过程会比DNN反向传播中的连乘过程更加极端。来看RNN反向传播过程中损失函数的表达式，在时间步t上我们有：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{t} &= L(\\mathbf{\\hat{y}}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\mathbf{h}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\mathbf{h}_{t-1}), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\mathbf{h}_{t-2})), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "& \\vdots \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\dots + \\mathbf{W}_{hh} \\sigma(\\mathbf{W}_{xh} \\mathbf{X}_{1} + \\mathbf{W}_{hh} \\mathbf{h}_{0}))...), \\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "这个公式嵌套了t层公式，共有$h_t$到$h_0$的t个自变量，且每个自变量h都与权重$W_{hh}$相乘。**假设此时我们令激活函数$\\sigma$为恒等函数$f(x) = x$，**则损失函数的公式可以改写为：\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{t} &= L(\\mathbf{\\hat{y}}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\mathbf{h}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\mathbf{h}_{t-1}), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} (\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\mathbf{h}_{t-2})), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "& \\vdots \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} (\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\dots + \\mathbf{W}_{hh} (\\mathbf{W}_{xh} \\mathbf{X}_{1} + \\mathbf{W}_{hh} \\mathbf{h}_{0}))...), \\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "在这个彻底拆解后的公式上，我们可以求解出嵌套了t层的$W_{hh}$的梯度（如公式3.t）：\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "{3.t)} \\  \\frac{\\partial L_{t}}{\\partial W_{hh}} &= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} * \\ \\  ... \\ \\ * \\frac{\\partial h_2}{\\partial h_1} * \\frac{\\partial h_1}{\\partial W_{hh}} \\\\ \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ca1d2-b7a2-41ce-9d12-aa2b28e6c2d4",
   "metadata": {},
   "source": [
    "此时在这个公式中，许多偏导数的求解就变得非常简单，例如：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\because {\\hat{y}}_{t} = W_{hy} h_{t},\\ \\ \\therefore \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} = {W}_{hy}\n",
    "\\\\ \\\\\n",
    "&\\because {h}_{t} = {W}_{xh} {X}_{t} + {W}_{hh} {h}_{t-1}, \\ \\ \\therefore \\frac{\\partial h_{t}}{\\partial h_{t-1}} = {W}_{hh} \\\\ \\\\\n",
    "&\\because {h}_{t-1} = {W}_{xh} {X}_{t-1} + {W}_{hh} {h}_{t-2}, \\ \\ \\therefore \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} = {W}_{hh} \\\\ \\\\\n",
    "& \\vdots \\\\ \\\\\n",
    "&\\because {h}_2 = {W}_{xh} {X}_{2} + {W}_{hh} {h}_{1}, \\ \\ \\therefore \\frac{\\partial h_2}{\\partial h_1} = {W}_{hh} \\\\ \\\\\n",
    "&\\because {h}_1 = {W}_{xh} {X}_{1} + {W}_{hh} {h}_{0}, \\ \\ \\therefore \\frac{\\partial h_1}{\\partial {W}_{hh}} = {h}_{0}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae86d50-8d4f-46f8-8996-c5f525f7f034",
   "metadata": {},
   "source": [
    "所以最终的梯度表达式为：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "{3.t)} \\ \\frac{\\partial L_{t}}{\\partial W_{hh}}\n",
    "&= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * \\frac{\\partial \\hat{y}_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} * \\ \\  ... \\ \\ * \\frac{\\partial h_2}{\\partial h_1} * \\frac{\\partial h_1}{\\partial W_{hh}} \\\\ \\\\\n",
    "&= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * W_{hy} * W_{hh} * W_{hh} * \\ \\  ... \\ \\ * W_{hh} * h_0 \\\\ \\\\\n",
    "&= \\frac{\\partial L_{t}}{\\partial \\hat{y}_{t}} * W_{hy} * (W_{hh})^{t-1}* h_0 \\\\ \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213145bb-2f94-421e-bbaf-fcf0f9b4d84c",
   "metadata": {},
   "source": [
    "不难发现，在这个梯度表达式中出现了$(W_{hh})^{t-1}$这样的高次项，这就是循环神经网络非常容易梯度爆炸和梯度消失的根源所在——假设$W_{hh}$是一个小于1的值，那$(W_{hh})^{t-1}$将会非常接近于0，从而导致梯度消失；假设$W_{hh}$大于1，那$(W_{hh})^{t-1}$将会接近无穷大，从而引发梯度爆炸，其中梯度消失发生的可能性又远远高于梯度爆炸。在深度神经网络中，在应用链式法则后，我们也会面临复合函数梯度连乘的问题，但由于普通神经网络中并不存在“权值共享”的现象，因此每个偏导数的表达式求解出的值大多是不一致的，在连乘的时候有的偏导数值较大、有的偏导数值较小，相比之下就不那么容易发生梯度爆炸或梯度消失问题的问题。当然，现在的公式是建立在“激活函数是恒等函数”这个假设上的，当激活函数不是恒等函数时，而是sigmoid或tanh时，梯度消失会更容易发生（毕竟激活函数会将连乘的值不断压缩到0-1之间），此时我们将会看到不同的公式，但公式中相同的“权重不断连乘形成高次方项”的问题本质都是一致的，因此如果你在其他教材或课程中看到不同的公式也不必惊慌。\n",
    "\n",
    "在RNN的训练过程中，梯度消失和梯度爆炸非常常见，所以为了应对这种现象，我们有如下的解决方案：\n",
    "\n",
    "1. **权重初始化**:使用适当的权重初始化策略可以帮助缓解梯度消失/爆炸的问题。例如，使用Xavier初始化。当然，不恰当的初始化更可能引发梯度爆炸，因为初始化的权重矩阵一般都是小于1的值，因此在使用该手段时要特别小心。\n",
    "2. **梯度截断**(Gradient Clipping):通过设定一个阈值，当梯度超过这个阈值时，将其缩小到该阈值，从而避免梯度爆炸。同样的，设定一个阈值，当梯度小于这个阈值时，将其放大到阈值，从而避免梯度消失。\n",
    "3. 在RNN中适当加入**残差链接**、Batch Normalization等技术，对深层循环神经网络可能有用。\n",
    "4. 使用**更高级的、改进后的RNN结构**，例如——\n",
    "> LSTM (Long Short-Term Memory)：LSTM是RNN的一个变种，设计上加入了三个门结构（输入门、遗忘门和输出门）来控制信息的流动，从而有效地缓解了梯度消失的问题。<br>\n",
    "> GRU (Gated Recurrent Unit)：GRU是LSTM的简化版本，它只有两个门结构（更新门和重置门）。虽然结构更简单，但在某些任务上与LSTM有着类似的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08073e92-ac62-4997-9962-2ddba3f6a494",
   "metadata": {},
   "source": [
    "- **容易遗忘，难以捕获长期依赖关系**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926f1e8-2d31-49fa-b59e-13cddaede757",
   "metadata": {},
   "source": [
    "尽管我们说循环网络在最后一个时间步时依然能够保留第一个时间步的信息，但在循环神经网络复杂的嵌套过程中，较早的时间步中的信息重要性会被大幅削弱，从而导致网络会“遗忘掉”最初的信息，而只记得最近的时间步的信息。在我们了解循环网络的嵌套和连乘过程后，遗忘问题其实很好理解，回到损失函数的表达式中：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{t} &= L(\\mathbf{\\hat{y}}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\mathbf{h}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\mathbf{h}_{t-1}), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} (\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\mathbf{h}_{t-2})), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "& \\vdots \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\mathbf{W}_{hh} (\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\dots + \\mathbf{W}_{hh} (\\mathbf{W}_{xh} \\mathbf{X}_{1} + \\mathbf{W}_{hh} \\mathbf{h}_{0}))...), \\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc33c1-cdf5-4033-a054-ea42927cb1f7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "同样是基于$\\sigma$是恒等函数的假设，我们来观察上述损失函数中的输入信息$X$。不难发现，在标签输出和损失计算过程中，每个输入信息$X$前都有权重的连乘项：\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{t} &= L(\\mathbf{\\hat{y}}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\mathbf{W}_{hy} \\mathbf{h}_{t}, \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\color{red}{\\mathbf{W}_{hy} (\\mathbf{W}_{xh}} \\mathbf{X}_{t} + \\mathbf{W}_{hh} \\mathbf{h}_{t-1}), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "&=L(\\color{red}{\\mathbf{W}_{hy}} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\color{red}{\\mathbf{W}_{hh} (\\mathbf{W}_{xh}} \\mathbf{X}_{t-1} + \\mathbf{W}_{hh} \\mathbf{h}_{t-2})), \\mathbf{y}_{t}) \\\\ \\\\\n",
    "& \\vdots \\\\ \\\\\n",
    "&=L(\\color{red}{\\mathbf{W}_{hy}} (\\mathbf{W}_{xh} \\mathbf{X}_{t} + \\color{red}{\\mathbf{W}_{hh}} (\\mathbf{W}_{xh} \\mathbf{X}_{t-1} + \\dots + \\color{red}{\\mathbf{W}_{hh} (\\mathbf{W}_{xh}} \\mathbf{X}_{1} + \\mathbf{W}_{hh} \\mathbf{h}_{0}))...), \\mathbf{y}_{t})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "你发现了吗？对于最初输入的$X_1$来说前面的连乘项为$\\mathbf{W}_{hy}(\\mathbf{W}_{hh})^{t-2}\\mathbf{W}_{xh}$，对于$X_2$来说前面的连乘项为$\\mathbf{W}_{hy}(\\mathbf{W}_{hh})^{t-3}\\mathbf{W}_{xh}$，这些高次项连乘后再乘以$X$，大部分时候会极大程度地削弱$X$本身的信息传递，即便不发生梯度消失，权重连乘后的值一般也是一个比较小的数字。因此在RNN当中，越早时间步的输入越容易被遗忘，RNN也因此不擅长处理很长的序列，而这一点其实非常难以改善。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74dec0-655c-42de-ab4a-2ea059ce96fc",
   "metadata": {},
   "source": [
    "除了前面提到的容易发生梯度消失、梯度爆炸问题，以及难以记忆长期记忆等，RNN还有许多其他难以克服的问题，包括但不限于：\n",
    "\n",
    "- **运算过程过于复杂，计算效率极其低下**\n",
    "> 相信在之前讲解RNN的反向传播时，你已经意识到RNN的运算本身有多复杂了。尽管RNN在进行循环过程中使用了权值共享的方式，但其本身执行循环、函数本身是递归的性质还是让RNN本身的计算效率较低——毕竟在RNN中，我们必须一个时间步、一个时间步地循环，因此下一个时间步的计算必须等待前一个时间步的计算完成。这意味着RNN在处理长序列时必须进行大量的顺序计算，这让RNN很难被用于多核、并行、多线程等加速技术当中。\n",
    "\n",
    "- **保存大量中间变量，运存利用率低**\n",
    "> RNN需要存储每个时间步的中间状态以进行反向传播，对长序列来说会有非常大的内存存储开销，每次进行反向传播的计算量也会很大，这使得本来就不太擅长处理长序列的RNN在长序列运算上更加乏力。\n",
    "\n",
    "- **使用全链接层，再限制参数量也会很大**\n",
    "> RNN在使用全链接层的基础上，还增加了循环的维度，导致整体参数量变得更大。\n",
    "\n",
    "- **容易过拟合，抗过拟合结构严重不足**\n",
    "> RNN参数量巨大，学习能力本身却不足，因此非常容易过拟合。在RNN的结构中，也没有任何正则化层（各类normalization层或dropout层）来帮助限制过拟合，在具有循环结构的网络中，这些层的使用也需要格外小心（之后在LSTM中我们会看到详情），因此想要控制循环网络的过拟合很不容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afe7a2-fa6a-4555-9cda-780187bf32b2",
   "metadata": {},
   "source": [
    "随着我们深入探索RNN的奥秘，可以明显地看到这一算法如何为序列数据建模提供了强大的工具。从RNN的基本架构，到其独特的循环原理，再到复杂的反向传播过程，我们已经对这一核心技术有了全面的了解。在实际操作中，通过PyTorch这一先进的深度学习框架，我们进一步巩固了对RNN的认识，也体会到了其在NLP领域的实际应用价值。\n",
    "\n",
    "RNN，尽管在处理序列数据上有其天然的优势，但正如我们所探讨的，它并不是没有挑战的。它的计算效率、容易过拟合等问题都需要我们在实际应用中注意和解决。幸运的是，深度学习的研究者和工程师们从未停止对其进行优化和改进。为了让大家更加直观地理解和应用RNN，接下来的两个手动实现RNN的例子会为大家提供一个实战的机会，希望大家可以通过这些例子进一步巩固所学，并体验到RNN的魅力。从下一章开始，我们即将探索LSTM算法，这是RNN家族中的另一位重要成员，它为我们提供了处理长序列数据中的挑战的有效策略，我们将会看到LSTM是如何解决RNN的各种问题的，请期待后续的课程！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a044d-5eed-4db6-9113-5301688a9ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
