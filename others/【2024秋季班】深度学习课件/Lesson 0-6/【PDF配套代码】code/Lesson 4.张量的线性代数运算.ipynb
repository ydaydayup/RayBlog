{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.张量的线性代数运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;也就是PyTorch中BLAS和LAPACK模块的相关运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PyTorch中并未设置单独的矩阵对象类型，因此PyTorch中，二维张量就相当于矩阵对象，并且拥有一系列线性代数相关函数和方法。      \n",
    "&emsp;&emsp;在实际机器学习和深度学习建模过程中，矩阵或者高维张量都是基本对象类型，而矩阵所涉及到的线性代数理论也是深度学习用户必备的基本数学基础。因此，本节在介绍张量的线性代数运算时，也会回顾基本的矩阵运算，及其基本线性代数的数学理论基础，以期在强化张量的线性代数运算过程中，也进一步夯实同学的线性代数数学基础。      \n",
    "&emsp;&emsp;另外，在实际的深度学习建模过程中，往往会涉及矩阵的集合，也就是三维甚至是四维张量的计算，因此在部分场景中，我们也将把二维张量计算拓展到更高维的张量计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、BLAS和LAPACK概览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;BLAS（Basic Linear Algeria Subprograms）和LAPACK（Linear Algeria Package）模块提供了完整的线性代数基本方法，由于涉及到函数种类较多，因此此处对其进行简单分类，具体包括：\n",
    "- 矩阵的形变及特殊矩阵的构造方法：包括矩阵的转置、对角矩阵的创建、单位矩阵的创建、上/下三角矩阵的创建等；\n",
    "- 矩阵的基本运算：包括矩阵乘法、向量内积、矩阵和向量的乘法等，当然，此处还包含了高维张量的基本运算，将着重探讨矩阵的基本运算拓展至三维张量中的基本方法；\n",
    "- 矩阵的线性代数运算：包括矩阵的迹、矩阵的秩、逆矩阵的求解、伴随矩阵和广义逆矩阵等；\n",
    "- 矩阵分解运算：特征分解、奇异值分解和SVD分解等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关内容如果涉及数学基础，将在讲解过程中逐步补充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、矩阵的形变及特殊矩阵构造方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵的形变方法其实也就是二维张量的形变方法，在此基础上本节将补充转置的基本方法。另外，在实际线性代数运算过程中，经常涉及一些特殊矩阵，如单位矩阵、对角矩阵等，相关创建方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>Tensor矩阵运算</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.t(t)        | t转置| \n",
    "| torch.eye(n)       | 创建包含n个分量的单位矩阵 | \n",
    "| torch.diag(t1)        | 以t1中各元素，创建对角矩阵 | \n",
    "| torch.triu(t)        | 取矩阵t中的上三角矩阵 | \n",
    "| torch.tril(t)        | 取矩阵t中的下三角矩阵 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个2*3的矩阵\n",
    "t1 = torch.arange(1, 7).reshape(2, 3).float()\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转置\n",
    "torch.t(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 矩阵的转置就是每个元素行列位置互换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 3, 0],\n",
       "        [0, 0, 0, 0, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 3, 0],\n",
       "        [0, 0, 0, 0, 0, 4],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线向上偏移一位\n",
    "torch.diag(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 2, 0, 0, 0],\n",
       "        [0, 0, 0, 3, 0, 0],\n",
       "        [0, 0, 0, 0, 4, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线向下偏移一位\n",
    "torch.diag(t, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(9).reshape(3, 3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 4, 5],\n",
       "        [0, 0, 8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取上三角矩阵\n",
    "torch.triu(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [0, 7, 8]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上三角矩阵向左下偏移一位\n",
    "torch.triu(t1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 0, 5],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上三角矩阵向右上偏移一位\n",
    "torch.triu(t1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [3, 4, 0],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下三角矩阵\n",
    "torch.tril(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、矩阵的基本运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵不同于普通的二维数组，其具备一定的线性代数含义，而这些特殊的性质，其实就主要体现在矩阵的基本运算上。课程中常见的矩阵基本运算如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>矩阵的基本运算</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.dot(t1, t2)        | 计算t1、t2张量内积 | \n",
    "| torch.mm(t1, t2)        | 矩阵乘法 | \n",
    "| torch.mv(t1, t2)        | 矩阵乘向量 | \n",
    "| torch.bmm(t1, t2)        | 批量矩阵乘法 | \n",
    "| torch.addmm(t, t1, t2)        | 矩阵相乘后相加 | \n",
    "| torch.addbmm(t, t1, t2)        | 批量矩阵相乘后相加 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dot\\vdot：点积计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在PyTorch中，dot和vdot只能作用于一维张量，且对于数值型对象，二者计算结果并没有区别，两种函数只在进行复数运算时会有区别。更多复数运算的规则，我们将在涉及复数运算的场景中再进行详细说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vdot(t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5eafa2b4bbd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 不能进行除了一维张量以外的计算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "# 不能进行除了一维张量以外的计算\n",
    "torch.dot(t1, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mm：矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;再PyTorch中，矩阵乘法其实是一个函数簇，除了矩阵乘法以外，还有批量矩阵乘法、矩阵相乘相加、批量矩阵相乘相加等等函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(1, 7).reshape(2, 3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(1, 10).reshape(3, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [16, 25, 36]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对应位置元素相乘\n",
    "t1 * t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 36, 42],\n",
       "        [66, 81, 96]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "torch.mm(t1, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘法执行过程如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](https://i.loli.net/2021/01/14/gshVBOWM4QD2TiL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mv：矩阵和向量相乘      \n",
    "&emsp;&emsp;PyTorch中提供了一类非常特殊的矩阵和向量相乘的函数，矩阵和向量相乘的过程我们可以看成是先将向量转化为列向量然后再相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met = torch.arange(1, 7).reshape(2, 3)\n",
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = torch.arange(1, 4)\n",
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际执行向量和矩阵相乘的过程中，需要矩阵的列数和向量的元素个数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(met, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.reshape(3, 1)             # 转化为列向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14],\n",
       "        [32]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(met, vec.reshape(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(met, vec.reshape(3, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理解**：mv函数本质上提供了一种二维张量和一维张量相乘的方法，在线性代数运算过程中，有很多矩阵乘向量的场景，典型的如线性回归的求解过程，通常情况下我们需要将向量转化为列向量（或者某些编程语言就默认向量都是列向量）然后进行计算，但PyTorch中单独设置了一个矩阵和向量相乘的方法，从而简化了行/列向量的理解过程和将向量转化为列向量的转化过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bmm：批量矩阵相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓批量矩阵相乘，指的是三维张量的矩阵乘法。根据此前对张量结构的理解，我们知道，三维张量就是一个包含了多个相同形状的矩阵的集合。例如，一个（3， 2， 2）的张量，本质上就是一个包含了3个2*2矩阵的张量。而三维张量的矩阵相乘，则是三维张量内部各对应位置的矩阵相乘。由于张量的运算往往涉及二维及以上，因此批量矩阵相乘也有非常多的应用场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 5,  6],\n",
       "         [ 7,  8]],\n",
       "\n",
       "        [[ 9, 10],\n",
       "         [11, 12]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.arange(1, 13).reshape(3, 2, 2)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.arange(1, 19).reshape(3, 2, 3)\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9,  12,  15],\n",
       "         [ 19,  26,  33]],\n",
       "\n",
       "        [[ 95, 106, 117],\n",
       "         [129, 144, 159]],\n",
       "\n",
       "        [[277, 296, 315],\n",
       "         [335, 358, 381]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(t3, t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point:**     \n",
    "- 三维张量包含的矩阵个数需要相同；\n",
    "- 每个内部矩阵，需要满足矩阵乘法的条件，也就是左乘矩阵的行数要等于右乘矩阵的列数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- addmm：矩阵相乘后相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addmm函数结构：addmm(input, mat1, mat2, beta=1, alpha=1)       \n",
    "输出结果：beta * input + alpha * (mat1 * mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 36, 42],\n",
       "        [66, 81, 96]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(t1, t2)                    # 矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 37, 44],\n",
       "        [66, 82, 98]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addmm(t, t1, t2)              # 先乘法后相加   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[300, 360, 420],\n",
       "        [660, 810, 960]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addmm(t, t1, t2, beta = 0, alpha = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- addbmm：批量矩阵相乘后相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和addmm类似，都是先乘后加，并且可以设置权重。不同的是addbmm是批量矩阵相乘，并且，在相加的过程中也是矩阵相加，而非向量加矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(6).reshape(2, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 5,  6],\n",
       "         [ 7,  8]],\n",
       "\n",
       "        [[ 9, 10],\n",
       "         [11, 12]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9,  12,  15],\n",
       "         [ 19,  26,  33]],\n",
       "\n",
       "        [[ 95, 106, 117],\n",
       "         [129, 144, 159]],\n",
       "\n",
       "        [[277, 296, 315],\n",
       "         [335, 358, 381]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(t3, t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[381, 415, 449],\n",
       "        [486, 532, 578]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addbmm(t, t3, t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：**addbmm会在原来三维张量基础之上，对其内部矩阵进行求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、矩阵的线性代数运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果说矩阵的基本运算是矩阵基本性质，那么矩阵的线性代数运算，则是我们利用矩阵数据类型在求解实际问题过程中经常涉及到的线性代数方法，具体相关函数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>矩阵的线性代数运算</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.trace(A)       | 矩阵的迹 |\n",
    "| matrix_rank(A)       | 矩阵的秩 |\n",
    "| torch.det(A)         | 计算矩阵A的行列式 |  \n",
    "| torch.inverse(A)        | 矩阵求逆 | \n",
    "| torch.lstsq(A,B)        | 最小二乘法 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，由于线性代数所涉及的数学基础知识较多，从实际应用的角度出发，我们将有所侧重的介绍实际应用过程中需要掌握的相关内容，并通过本节末尾的实际案例，来加深线性代数相关内容的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.矩阵的迹（trace）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵的迹的运算相对简单，就是矩阵对角线元素之和，在PyTorch中，可以使用trace函数进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()  \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，对于矩阵的迹来说，计算过程不需要是方阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(1, 7).reshape(2, 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.矩阵的秩(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵的秩（rank），是指矩阵中行或列的极大线性无关数，且矩阵中行、列极大无关数总是相同的，任何矩阵的秩都是唯一值，满秩指的是方阵（行数和列数相同的矩阵）中行数、列数和秩相同，满秩矩阵有线性唯一解等重要特性，而其他矩阵也能通过求解秩来降维，同时，秩也是奇异值分解等运算中涉及到的重要概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matrix_rank计算矩阵的秩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 5).reshape(2, 2).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2], [2, 4]]).float()\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于矩阵B来说，第一列和第二列明显线性相关，最大线性无关组只有1组，因此矩阵的秩计算结果为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.矩阵的行列式(det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓行列式，我们可以简单将其理解为矩阵的一个基本性质或者属性，通过行列式的计算，我们能够知道矩阵是否可逆，从而可以进一步求解矩阵所对应的线性方程。当然，更加专业的解释，行列式的作为一个基本数学工具，实际上就是矩阵进行线性变换的伸缩因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任何一个n维方正，行列式计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/14/AkeTpgOrctHoIiq.jpg\" alt=\"7\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更为简单的情况，如果对于一个2*2的矩阵，行列式的计算就是主对角线元素之积减去另外两个元素之积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()     # 秩的计算要求浮点型张量\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A的行列式计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/15/wMvxGTu7a2VCzE9.jpg\" alt=\"6\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于行列式的计算，要求二维张量必须是方正，也就是行列数必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(1, 7).reshape(2, 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A must be batches of square matrices, but they are 3 by 2 matrices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-beff1455abd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: A must be batches of square matrices, but they are 3 by 2 matrices"
     ]
    }
   ],
   "source": [
    "torch.det(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.线性方程组的矩阵表达形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在正式进入到更进一步矩阵运算的讨论之前，我们需要对矩阵建立一个更加形象化的理解。通常来说，我们会把高维空间中的一个个数看成是向量，而由这些向量组成的数组看成是一个矩阵。例如：（1，2），（3，4）是二维空间中的两个点，矩阵A就代表这两个点所组成的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 5).reshape(2, 2).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25f0f6eb488>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制点图查看两个点的位置\n",
    "plt.plot(A[:,0], A[:, 1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果更进一步，我们希望在二维空间中找到一条直线，来拟合这两个点，也就是所谓的构建一个线性回归模型，我们可以设置线性回归方程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $ y = ax + b $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带入（1，2）和（3，4）两个点之后，我们还可以进一步将表达式改写成矩阵表示形式，改写过程如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/14/UEPNYc9OjGn3J5b.jpg\" alt=\"8\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而用矩阵表示线性方程组，则是矩阵的另一种常用用途，接下来，我们就可以通过上述矩阵方程组来求解系数向量x。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先一个基本思路是，如果有个和A矩阵相关的另一个矩阵，假设为$A^{-1}$，可以使得二者相乘之后等于1，也就是$A * A^{-1} = 1$，那么在方程组左右两边同时左乘该矩阵，等式右边的计算结果$A^{-1} * B$就将是x系数向量的取值。而此处的$A^{-1}$就是所谓的A的逆矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆矩阵定义：    \n",
    "<center> $ 如果存在两个矩阵A、B，并在矩阵乘法运算下，A * B = E（单位矩阵），则我们称A、B互为逆矩阵$  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述线性方程组求解场景中，我们已经初步看到了逆矩阵的用途，而一般来说，我们往往会通过伴随矩阵来进行逆矩阵的求解。由于伴随矩阵本身并无其他核心用途，且PyTorch中也未给出伴随矩阵的计算函数（目前），因此我们直接调用inverse函数来进行逆矩阵的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 当然，并非所有矩阵都有逆矩阵，对于一个矩阵来说，首先必须是方正，其次矩阵的秩不能为零，满足两个条件才能求解逆矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inverse函数：求解逆矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，根据上述矩阵表达式，从新定义A和B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [3., 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 1], [3, 1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([2.0, 4])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用inverse函数进行逆矩阵求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5000,  0.5000],\n",
       "        [ 1.5000, -0.5000]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inverse(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单试探逆矩阵的基本特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -5.9605e-08],\n",
       "        [-1.1921e-07,  1.0000e+00]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.inverse(A), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -5.9605e-08],\n",
       "        [-1.1921e-07,  1.0000e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A, torch.inverse(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后在方程组左右两边同时左乘$A^{-1}$，求解x      \n",
    "<center> $A^{-1} * A * x= A^{-1} * B $ </center>      \n",
    "<center>$ E * x = A^{-1} * B $  </center>     \n",
    "<center>$ x = A^{-1} * B$ </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(torch.inverse(A), B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终得到线性方程为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$ y = x + 1$ </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，上述计算过程只是一个简化的线性方程组求解系数的过程，同时也是一个简单的一元线性方程拟合数据的过程，关于常用求解线性方程组系数的最小二乘法，可以先阅读本节末尾的选读内容，更多线性回归相关内容，我们将在下周进行详细讲解。     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、矩阵的分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵的分解也是矩阵运算中的常规计算，矩阵分解也有很多种类，常见的例如QR分解、LU分解、特征分解、SVD分解等等等等，虽然大多数情况下，矩阵分解都是在形式上将矩阵拆分成几种特殊矩阵的乘积，但本质上，矩阵的分解是去探索矩阵更深层次的一些属性。本节将主要围绕特征分解和SVD分解展开讲解，更多矩阵分解的运算，我们将在后续课程中逐渐进行介绍。值得一提的是，此前的逆矩阵，其实也可以将其看成是一种矩阵分解的方式，分解之后的等式如下：      \n",
    "<center> $A = A * A^{-1} * A $ </center>      \n",
    "而大多数情况下，矩阵分解都是分解成形如下述形式      \n",
    "<center> $ A = VUD$ </center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.特征分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征分解中，矩阵分解形式为：       \n",
    "<center> $ A = Q\\Lambda Q^{-1}$ </center>         \n",
    "其中，Q和$Q^{-1}$互为逆矩阵，并且Q的列就是A的特征值所对应的特征向量，而$\\Lambda$为矩阵A的特征值按照降序排列组成的对角矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.eig函数：特征分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 10).reshape(3, 3).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[ 1.6117e+01,  0.0000e+00],\n",
       "        [-1.1168e+00,  0.0000e+00],\n",
       "        [-1.2253e-07,  0.0000e+00]]),\n",
       "eigenvectors=tensor([[-0.2320, -0.7858,  0.4082],\n",
       "        [-0.5253, -0.0868, -0.8165],\n",
       "        [-0.8187,  0.6123,  0.4082]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(A, eigenvectors=True)                 # 注，此处需要输入参数为True才会返回矩阵的特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果中，eigenvalues表示特征值向量，即A矩阵分解后的Λ矩阵的对角线元素值，并按照又大到小依次排列，eigenvectors表示A矩阵分解后的Q矩阵，此处需要理解特征值，所谓特征值，可简单理解为对应列在矩阵中的信息权重，如果该列能够简单线性变换来表示其他列，则说明该列信息权重较大，反之则较小。特征值概念和秩的概念有点类似，但不完全相同，矩阵的秩表示矩阵列向量的最大线性无关数，而特征值的大小则表示某列向量能多大程度解读矩阵列向量的变异度，即所包含信息量，秩和特征值关系可用下面这个例子来进行解读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([1, 2, 2, 4]).reshape(2, 2).float()\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[0., 0.],\n",
       "        [5., 0.]]),\n",
       "eigenvectors=tensor([]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(B)          # 返回结果中只有一个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.],\n",
       "        [3., 6., 9.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.tensor([[1, 2, 3], [2, 4, 6], [3, 6, 9]]).float()\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[ 1.4000e+01,  0.0000e+00],\n",
       "        [-1.6447e-07,  0.0000e+00],\n",
       "        [ 2.8710e-07,  0.0000e+00]]),\n",
       "eigenvectors=tensor([]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(C)                # 只有一个特征的有效值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征值一般用于表示矩阵对应线性方程组解空间以及数据降维，当然，由于特征分解只能作用于方阵，而大多数实际情况下矩阵行列数未必相等，此时要进行类似的操作就需要采用和特征值分解思想类似的奇异值分解（SVD）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.奇异值分解（SVD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;奇异值分解（SVD）来源于代数学中的矩阵分解问题，对于一个方阵来说，我们可以利用矩阵特征值和特征向量的特殊性质（矩阵点乘特征向量等于特征值数乘特征向量），通过求特征值与特征向量来达到矩阵分解的效果     \n",
    "<center> $ A = Q\\Lambda Q^{-1}$ </center>            \n",
    "这里，Q是由特征向量组成的矩阵，而Λ是特征值降序排列构成的一个对角矩阵（对角线上每个值是一个特征值，按降序排列，其他值为0），特征值的数值表示对应的特征的重要性。      \n",
    "在很多情况下，最大的一小部分特征值的和即可以约等于所有特征值的和，而通过矩阵分解的降维就是通过在Q、Λ 中删去那些比较小的特征值及其对应的特征向量，使用一小部分的特征值和特征向量来描述整个矩阵，从而达到降维的效果。      \n",
    "但是，实际问题中大多数矩阵是以奇异矩阵形式，而不是方阵的形式出现的，奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为m×n的奇异矩阵A分解成三个部分 :      \n",
    "<center> $ A = U\\sum V^{T}$ </center>         \n",
    "其中U、V是两个正交矩阵，其中的每一行（每一列）分别被称为左奇异向量和右奇异向量，他们和∑中对角线上的奇异值相对应，通常情况下我们只需要保留前k个奇异向量和奇异值即可，其中U是m×k矩阵，V是n×k矩阵，∑是k×k的方阵，从而达到减少存储空间的效果，即      \n",
    "<center> $ A_{m*n} = U_{m*m}\\sum_{m*n}V^{T}_{n*n}\\approx U_{m*k}\\sum_{k*k}V^{T}_{k*n}$ </center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svd奇异值分解函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.],\n",
       "        [3., 6., 9.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[-0.2673, -0.8018, -0.5345],\n",
       "        [-0.5345, -0.3382,  0.7745],\n",
       "        [-0.8018,  0.4927, -0.3382]]),\n",
       "S=tensor([14.0000,  0.0000,  0.0000]),\n",
       "V=tensor([[-0.2673,  0.0000,  0.9636],\n",
       "        [-0.5345, -0.8321, -0.1482],\n",
       "        [-0.8018,  0.5547, -0.2224]]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.svd(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "CU, CS, CV = torch.svd(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证SVD分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.mm(CU, torch.diag(CS)), CV.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够看出，上述输出完整还原了C矩阵，此时我们可根据svd输出结果对C进行降维，此时C可只保留第一列（后面的奇异值过小），即k=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2673],\n",
       "        [-0.5345],\n",
       "        [-0.8018]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1 = CU[:, 0].reshape(3, 1)         # U的第一列\n",
    "U1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.0000)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1 = CS[0]                           # C的第一个值\n",
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2673, -0.5345, -0.8018]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = CV[:, 0].reshape(1, 3)           # V的第一行\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm((U1 * C1), V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时输出的Cd矩阵已经和原矩阵C高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本节选读内容**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 另外，我们需要知道的是，除了利用逆矩阵求解线性方程组系数外，比较通用的方法是使用最小二乘法进行求解："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.lstsq：最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最小二乘法是最通用的线性方程拟合求解工具，我们可以利用最小二乘法的直接计算拟合直线的系数最优解。当然，本节仅介绍最小二乘法的函数调用，下节在介绍目标函数和优化手段时，还将进一步介绍最小二乘法的数学原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.lstsq(\n",
       "solution=tensor([[1.0000],\n",
       "        [1.0000]]),\n",
       "QR=tensor([[-3.1623, -1.2649],\n",
       "        [ 0.7208, -0.6325]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.lstsq(B.reshape(2, 1), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, q = torch.lstsq(B.reshape(2, 1), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1623, -1.2649],\n",
       "        [ 0.7208, -0.6325]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，最小二乘法返回了两个结果，分别是x的系数和QR分解后的QR矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solve函数与LU分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.solve(\n",
       "solution=tensor([[1.],\n",
       "        [1.]]),\n",
       "LU=tensor([[3.0000, 1.0000],\n",
       "        [0.3333, 0.6667]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.solve(B.reshape(2, 1), A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LU分解函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.0000, 1.0000],\n",
       "         [0.3333, 0.6667]]),\n",
       " tensor([2, 2], dtype=torch.int32))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.lu(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
