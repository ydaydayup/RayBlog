---  
title: 【机器学习笔记】 混淆矩阵 Confusion Matrix
date: 2024-08-23 
authors: Ray  
tags: [ 真正例,True Positives,假正例,FP False Positives,真负例,TN True Negatives,假负例,FN False Negatives ]  
keywords: [ 真正例,True Positives,假正例,FP False Positives,真负例,TN True Negatives,假负例,FN False Negatives ]  
description: 混淆矩阵（Confusion Matrix）是评估分类模型性能的一个重要工具，它展示了模型预测结果与实际标签之间的关系。在混淆矩阵中，通常会有四个基本元素：真正例（TP, True Positives）、假正例（FP, False Positives）、真负例（TN, True Negatives）和假负例（FN, False Negatives）。基于这些元素，我们可以计算出多个分类指标，下面我将详细解释这些指标
#image: https://img.kuizuo.cn/202312270328599.png  
sticky: 2  
---  

## **Summary**
混淆矩阵（Confusion Matrix）是评估分类模型性能的一个重要工具，它展示了模型预测结果与实际标签之间的关系。在混淆矩阵中，通常会有四个基本元素：真正例（TP, True Positives）、假正例（FP, False Positives）、真负例（TN, True Negatives）和假负例（FN, False Negatives）。基于这些元素，我们可以计算出多个分类指标
详细内容请阅读
https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#cite_note-11

<!-- truncate -->
## 正文

混淆矩阵（Confusion Matrix）是评估分类模型性能的一个重要工具，它展示了模型预测结果与实际标签之间的关系。在混淆矩阵中，通常会有四个基本元素：真正例（TP, True Positives）、假正例（FP, False Positives）、真负例（TN, True Negatives）和假负例（FN, False Negatives）。基于这些元素，我们可以计算出多个分类指标，下面我将详细解释这些指标：

1. **Accuracy（精确率）**：
  - **定义**：准确率是所有正确预测（真正例和真负例）与所有预测（真正例、假正例、真负例、假负例）的比率。
  - **公式**：$$  {Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}  $$
  - **作用**：提供了模型整体性能的一个概览，即模型正确预测的比例。

2. **Precision（正确率或准确率或查准率）**：
  - **定义**：正确率是真正例与所有被模型预测为正例（真正例和假正例）的比率。
  - **公式**：$${Precision} = \frac{TP}{TP + FP}  $$
  - **作用**：衡量了模型预测为正例的样本中，实际为正例的比例。在正例标签较为重要的情况下，正确率是一个关键指标。

3. **Recall（召回率）**：
  - **定义**：召回率是真正例与所有实际为正例（真正例和假负例）的比率。
  - **公式**：$$  \text{Recall} = \frac{TP}{TP + FN} $$
  - **作用**：衡量了模型能够正确识别出的实际正例的比例。在需要尽可能多地识别出所有正例的情况下，召回率尤为重要。

> 召回率高说明，获取正例(目标类别，感兴趣的类别)的能力高

4. **Specificity（特异性）**：
  - **定义**：特异性是真负例与所有实际为负例（真负例和假负例）的比率。
  - **公式**：$$  \text{Specificity} = \frac{TN}{TN + FP} $$
  - **作用**：衡量了模型正确识别出的实际负例的比例。在负例标签较为重要的情况下，特异性是一个关键指标。

5. **Sensitivity（灵敏度）**：
  - **定义**：灵敏度是召回率的另一种说法，即模型识别出实际正例的能力。
  - **公式**：与召回率相同，$$ \text{Sensitivity} = \frac{TP}{TP + FN} $$
  - **作用**：与召回率相同，强调模型对正例的识别能力。

**区别**：
- **Accuracy** 提供了一个整体的模型性能指标，但它可能掩盖了模型在特定类别上的性能。
- **Precision** 和 **Recall** 通常成对出现，它们衡量模型在不同方面的性能：Precision 关注预测为正例的准确性，而 Recall 关注模型识别所有正例的能力。
- **Specificity** 与 **Recall** 类似，但它关注的是负例的识别能力。
- **Sensitivity** 与 **Recall** 相同，只是从不同的角度描述了相同的概念。

**作用**：
- 在不同的应用场景中，这些指标的重要性可能不同。例如，在医疗诊断中，Recall（灵敏度）可能比 Precision 更重要，因为我们希望尽可能多地识别出所有可能的病例。而在垃圾邮件过滤中，Precision 可能更重要，因为用户更关心的是邮件列表中不要有太多误判的非垃圾邮件。


在混淆矩阵中，"真"、"假"、"正例"、"负例"这四个字眼是用来描述模型预测结果与实际标签之间的关系的。以下是它们各自的含义：

1. **真正例（True Positives, TP）**：
   - **定义**：模型正确预测为正例的样本数量。
   - **解释**：在实际中是正例，模型也预测为正例。

2. **假正例（False Positives, FP）**：
   - **定义**：模型错误预测为正例的样本数量。
   - **解释**：在实际中是负例，但模型预测为正例。

3. **真负例（True Negatives, TN）**：
   - **定义**：模型正确预测为负例的样本数量。
   - **解释**：在实际中是负例，模型也预测为负例。

4. **假负例（False Negatives, FN）**：
   - **定义**：模型错误预测为负例的样本数量。
   - **解释**：在实际中是正例，但模型预测为负例。

这些术语在二分类问题中非常常见，**其中"正例"通常指的是我们感兴趣的类别（比如疾病的存在、邮件是垃圾邮件等），而"负例"则是我们不感兴趣的类别（比如疾病不存在、邮件不是垃圾邮件等）**。模型的目标是尽可能准确地区分这两类样本。

- **真正例**和**真负例**是模型预测正确的情况。
- **假正例**和**假负例**是模型预测错误的情况。

在实际应用中，根据问题的不同，我们可能会更关注某些错误类型而不是其他类型。例如，在医疗检测中，我们可能更关心减少**假负例**（即不漏诊），因为漏诊可能导致严重后果。而在信贷审批中，我们可能更关心减少**假正例**（即不批准不符合条件的贷款申请），因为这可能导致财务损失。

## "真"、"假"、"正例"、"负例"的含义

> 真：正确的
> 假：错误的
> 正例： 通常指的是我们感兴趣的类别
